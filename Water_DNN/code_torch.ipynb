{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split, Dataset\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "features = pd.read_csv('./features_notnormal.csv', header=None).values\n",
    "target= pd.read_csv('./target_notnormal.csv', header=None).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)  # Convert features to tensor\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)  # Convert labels to tensor\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Return the data point and its label\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "# dataset = CustomDataset(xyz_train, ener_train)\n",
    "dataset = CustomDataset(features, target)\n",
    "\n",
    "# Define the split ratio (80% train, 20% test)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "batch_size = 64 \n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=test_size, shuffle=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AE model\n",
    "import torch.nn.init as init\n",
    "\n",
    "class DNN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(DNN, self).__init__()\n",
    "\n",
    "        self.regressor = nn.Sequential(\n",
    "            nn.Linear(input_dim, 8),\n",
    "            nn.Sigmoid(),\n",
    "            nn.Linear(8, 1) \n",
    "        )\n",
    "        \n",
    "        # self.apply(self.init_weights)\n",
    "\n",
    "    # def init_weights(self, m):\n",
    "    #     if isinstance(m, nn.Linear):  \n",
    "    #         # init.constant_(m.weight, 1e-2)\n",
    "    #         init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "    #         # init.normal_(m.weight, mean=0.0, std=0.02)\n",
    "    #         if m.bias is not None:\n",
    "    #             init.zeros_(m.bias)  # Set biases to zero\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.regressor(x)\n",
    "        \n",
    "\n",
    "def dnn_loss(y_pred, y_true):\n",
    "    return nn.MSELoss(reduction='mean')(y_pred, y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2000, train Loss: 1.0982\n",
      "Epoch 1/2000, test Loss: 1.2325\n",
      "Epoch 2/2000, train Loss: 1.0146\n",
      "Epoch 2/2000, test Loss: 1.1615\n",
      "Epoch 3/2000, train Loss: 0.9669\n",
      "Epoch 3/2000, test Loss: 1.1147\n",
      "Epoch 4/2000, train Loss: 0.9379\n",
      "Epoch 4/2000, test Loss: 1.0875\n",
      "Epoch 5/2000, train Loss: 0.9213\n",
      "Epoch 5/2000, test Loss: 1.0679\n",
      "Epoch 6/2000, train Loss: 0.9096\n",
      "Epoch 6/2000, test Loss: 1.0522\n",
      "Epoch 7/2000, train Loss: 0.8977\n",
      "Epoch 7/2000, test Loss: 1.0390\n",
      "Epoch 8/2000, train Loss: 0.8868\n",
      "Epoch 8/2000, test Loss: 1.0249\n",
      "Epoch 9/2000, train Loss: 0.8749\n",
      "Epoch 9/2000, test Loss: 1.0107\n",
      "Epoch 10/2000, train Loss: 0.8635\n",
      "Epoch 10/2000, test Loss: 0.9967\n",
      "Epoch 11/2000, train Loss: 0.8507\n",
      "Epoch 11/2000, test Loss: 0.9813\n",
      "Epoch 12/2000, train Loss: 0.8383\n",
      "Epoch 12/2000, test Loss: 0.9655\n",
      "Epoch 13/2000, train Loss: 0.8251\n",
      "Epoch 13/2000, test Loss: 0.9493\n",
      "Epoch 14/2000, train Loss: 0.8118\n",
      "Epoch 14/2000, test Loss: 0.9332\n",
      "Epoch 15/2000, train Loss: 0.7982\n",
      "Epoch 15/2000, test Loss: 0.9177\n",
      "Epoch 16/2000, train Loss: 0.7850\n",
      "Epoch 16/2000, test Loss: 0.9019\n",
      "Epoch 17/2000, train Loss: 0.7718\n",
      "Epoch 17/2000, test Loss: 0.8864\n",
      "Epoch 18/2000, train Loss: 0.7594\n",
      "Epoch 18/2000, test Loss: 0.8702\n",
      "Epoch 19/2000, train Loss: 0.7465\n",
      "Epoch 19/2000, test Loss: 0.8562\n",
      "Epoch 20/2000, train Loss: 0.7345\n",
      "Epoch 20/2000, test Loss: 0.8430\n",
      "Epoch 21/2000, train Loss: 0.7240\n",
      "Epoch 21/2000, test Loss: 0.8289\n",
      "Epoch 22/2000, train Loss: 0.7132\n",
      "Epoch 22/2000, test Loss: 0.8178\n",
      "Epoch 23/2000, train Loss: 0.7039\n",
      "Epoch 23/2000, test Loss: 0.8059\n",
      "Epoch 24/2000, train Loss: 0.6951\n",
      "Epoch 24/2000, test Loss: 0.7957\n",
      "Epoch 25/2000, train Loss: 0.6872\n",
      "Epoch 25/2000, test Loss: 0.7864\n",
      "Epoch 26/2000, train Loss: 0.6801\n",
      "Epoch 26/2000, test Loss: 0.7782\n",
      "Epoch 27/2000, train Loss: 0.6737\n",
      "Epoch 27/2000, test Loss: 0.7710\n",
      "Epoch 28/2000, train Loss: 0.6685\n",
      "Epoch 28/2000, test Loss: 0.7639\n",
      "Epoch 29/2000, train Loss: 0.6634\n",
      "Epoch 29/2000, test Loss: 0.7577\n",
      "Epoch 30/2000, train Loss: 0.6590\n",
      "Epoch 30/2000, test Loss: 0.7527\n",
      "Epoch 31/2000, train Loss: 0.6549\n",
      "Epoch 31/2000, test Loss: 0.7482\n",
      "Epoch 32/2000, train Loss: 0.6513\n",
      "Epoch 32/2000, test Loss: 0.7441\n",
      "Epoch 33/2000, train Loss: 0.6485\n",
      "Epoch 33/2000, test Loss: 0.7396\n",
      "Epoch 34/2000, train Loss: 0.6457\n",
      "Epoch 34/2000, test Loss: 0.7362\n",
      "Epoch 35/2000, train Loss: 0.6431\n",
      "Epoch 35/2000, test Loss: 0.7331\n",
      "Epoch 36/2000, train Loss: 0.6405\n",
      "Epoch 36/2000, test Loss: 0.7298\n",
      "Epoch 37/2000, train Loss: 0.6385\n",
      "Epoch 37/2000, test Loss: 0.7271\n",
      "Epoch 38/2000, train Loss: 0.6363\n",
      "Epoch 38/2000, test Loss: 0.7241\n",
      "Epoch 39/2000, train Loss: 0.6346\n",
      "Epoch 39/2000, test Loss: 0.7218\n",
      "Epoch 40/2000, train Loss: 0.6327\n",
      "Epoch 40/2000, test Loss: 0.7191\n",
      "Epoch 41/2000, train Loss: 0.6309\n",
      "Epoch 41/2000, test Loss: 0.7172\n",
      "Epoch 42/2000, train Loss: 0.6296\n",
      "Epoch 42/2000, test Loss: 0.7146\n",
      "Epoch 43/2000, train Loss: 0.6280\n",
      "Epoch 43/2000, test Loss: 0.7126\n",
      "Epoch 44/2000, train Loss: 0.6261\n",
      "Epoch 44/2000, test Loss: 0.7101\n",
      "Epoch 45/2000, train Loss: 0.6246\n",
      "Epoch 45/2000, test Loss: 0.7077\n",
      "Epoch 46/2000, train Loss: 0.6231\n",
      "Epoch 46/2000, test Loss: 0.7058\n",
      "Epoch 47/2000, train Loss: 0.6216\n",
      "Epoch 47/2000, test Loss: 0.7037\n",
      "Epoch 48/2000, train Loss: 0.6202\n",
      "Epoch 48/2000, test Loss: 0.7017\n",
      "Epoch 49/2000, train Loss: 0.6189\n",
      "Epoch 49/2000, test Loss: 0.6997\n",
      "Epoch 50/2000, train Loss: 0.6171\n",
      "Epoch 50/2000, test Loss: 0.6978\n",
      "Epoch 51/2000, train Loss: 0.6158\n",
      "Epoch 51/2000, test Loss: 0.6959\n",
      "Epoch 52/2000, train Loss: 0.6144\n",
      "Epoch 52/2000, test Loss: 0.6934\n",
      "Epoch 53/2000, train Loss: 0.6131\n",
      "Epoch 53/2000, test Loss: 0.6915\n",
      "Epoch 54/2000, train Loss: 0.6113\n",
      "Epoch 54/2000, test Loss: 0.6895\n",
      "Epoch 55/2000, train Loss: 0.6101\n",
      "Epoch 55/2000, test Loss: 0.6876\n",
      "Epoch 56/2000, train Loss: 0.6088\n",
      "Epoch 56/2000, test Loss: 0.6856\n",
      "Epoch 57/2000, train Loss: 0.6071\n",
      "Epoch 57/2000, test Loss: 0.6835\n",
      "Epoch 58/2000, train Loss: 0.6058\n",
      "Epoch 58/2000, test Loss: 0.6820\n",
      "Epoch 59/2000, train Loss: 0.6042\n",
      "Epoch 59/2000, test Loss: 0.6797\n",
      "Epoch 60/2000, train Loss: 0.6030\n",
      "Epoch 60/2000, test Loss: 0.6778\n",
      "Epoch 61/2000, train Loss: 0.6016\n",
      "Epoch 61/2000, test Loss: 0.6756\n",
      "Epoch 62/2000, train Loss: 0.6008\n",
      "Epoch 62/2000, test Loss: 0.6741\n",
      "Epoch 63/2000, train Loss: 0.5984\n",
      "Epoch 63/2000, test Loss: 0.6717\n",
      "Epoch 64/2000, train Loss: 0.5971\n",
      "Epoch 64/2000, test Loss: 0.6697\n",
      "Epoch 65/2000, train Loss: 0.5957\n",
      "Epoch 65/2000, test Loss: 0.6677\n",
      "Epoch 66/2000, train Loss: 0.5944\n",
      "Epoch 66/2000, test Loss: 0.6658\n",
      "Epoch 67/2000, train Loss: 0.5929\n",
      "Epoch 67/2000, test Loss: 0.6637\n",
      "Epoch 68/2000, train Loss: 0.5914\n",
      "Epoch 68/2000, test Loss: 0.6617\n",
      "Epoch 69/2000, train Loss: 0.5903\n",
      "Epoch 69/2000, test Loss: 0.6595\n",
      "Epoch 70/2000, train Loss: 0.5884\n",
      "Epoch 70/2000, test Loss: 0.6575\n",
      "Epoch 71/2000, train Loss: 0.5869\n",
      "Epoch 71/2000, test Loss: 0.6555\n",
      "Epoch 72/2000, train Loss: 0.5852\n",
      "Epoch 72/2000, test Loss: 0.6533\n",
      "Epoch 73/2000, train Loss: 0.5837\n",
      "Epoch 73/2000, test Loss: 0.6511\n",
      "Epoch 74/2000, train Loss: 0.5824\n",
      "Epoch 74/2000, test Loss: 0.6492\n",
      "Epoch 75/2000, train Loss: 0.5809\n",
      "Epoch 75/2000, test Loss: 0.6470\n",
      "Epoch 76/2000, train Loss: 0.5790\n",
      "Epoch 76/2000, test Loss: 0.6447\n",
      "Epoch 77/2000, train Loss: 0.5774\n",
      "Epoch 77/2000, test Loss: 0.6426\n",
      "Epoch 78/2000, train Loss: 0.5756\n",
      "Epoch 78/2000, test Loss: 0.6407\n",
      "Epoch 79/2000, train Loss: 0.5741\n",
      "Epoch 79/2000, test Loss: 0.6384\n",
      "Epoch 80/2000, train Loss: 0.5725\n",
      "Epoch 80/2000, test Loss: 0.6360\n",
      "Epoch 81/2000, train Loss: 0.5706\n",
      "Epoch 81/2000, test Loss: 0.6341\n",
      "Epoch 82/2000, train Loss: 0.5688\n",
      "Epoch 82/2000, test Loss: 0.6317\n",
      "Epoch 83/2000, train Loss: 0.5669\n",
      "Epoch 83/2000, test Loss: 0.6294\n",
      "Epoch 84/2000, train Loss: 0.5653\n",
      "Epoch 84/2000, test Loss: 0.6269\n",
      "Epoch 85/2000, train Loss: 0.5635\n",
      "Epoch 85/2000, test Loss: 0.6248\n",
      "Epoch 86/2000, train Loss: 0.5614\n",
      "Epoch 86/2000, test Loss: 0.6224\n",
      "Epoch 87/2000, train Loss: 0.5593\n",
      "Epoch 87/2000, test Loss: 0.6203\n",
      "Epoch 88/2000, train Loss: 0.5573\n",
      "Epoch 88/2000, test Loss: 0.6175\n",
      "Epoch 89/2000, train Loss: 0.5551\n",
      "Epoch 89/2000, test Loss: 0.6148\n",
      "Epoch 90/2000, train Loss: 0.5531\n",
      "Epoch 90/2000, test Loss: 0.6124\n",
      "Epoch 91/2000, train Loss: 0.5507\n",
      "Epoch 91/2000, test Loss: 0.6099\n",
      "Epoch 92/2000, train Loss: 0.5484\n",
      "Epoch 92/2000, test Loss: 0.6073\n",
      "Epoch 93/2000, train Loss: 0.5461\n",
      "Epoch 93/2000, test Loss: 0.6048\n",
      "Epoch 94/2000, train Loss: 0.5438\n",
      "Epoch 94/2000, test Loss: 0.6021\n",
      "Epoch 95/2000, train Loss: 0.5412\n",
      "Epoch 95/2000, test Loss: 0.5993\n",
      "Epoch 96/2000, train Loss: 0.5387\n",
      "Epoch 96/2000, test Loss: 0.5963\n",
      "Epoch 97/2000, train Loss: 0.5363\n",
      "Epoch 97/2000, test Loss: 0.5933\n",
      "Epoch 98/2000, train Loss: 0.5335\n",
      "Epoch 98/2000, test Loss: 0.5905\n",
      "Epoch 99/2000, train Loss: 0.5305\n",
      "Epoch 99/2000, test Loss: 0.5878\n",
      "Epoch 100/2000, train Loss: 0.5280\n",
      "Epoch 100/2000, test Loss: 0.5844\n",
      "Epoch 101/2000, train Loss: 0.5258\n",
      "Epoch 101/2000, test Loss: 0.5817\n",
      "Epoch 102/2000, train Loss: 0.5214\n",
      "Epoch 102/2000, test Loss: 0.5786\n",
      "Epoch 103/2000, train Loss: 0.5185\n",
      "Epoch 103/2000, test Loss: 0.5748\n",
      "Epoch 104/2000, train Loss: 0.5153\n",
      "Epoch 104/2000, test Loss: 0.5716\n",
      "Epoch 105/2000, train Loss: 0.5119\n",
      "Epoch 105/2000, test Loss: 0.5678\n",
      "Epoch 106/2000, train Loss: 0.5087\n",
      "Epoch 106/2000, test Loss: 0.5645\n",
      "Epoch 107/2000, train Loss: 0.5050\n",
      "Epoch 107/2000, test Loss: 0.5608\n",
      "Epoch 108/2000, train Loss: 0.5013\n",
      "Epoch 108/2000, test Loss: 0.5566\n",
      "Epoch 109/2000, train Loss: 0.4974\n",
      "Epoch 109/2000, test Loss: 0.5533\n",
      "Epoch 110/2000, train Loss: 0.4935\n",
      "Epoch 110/2000, test Loss: 0.5495\n",
      "Epoch 111/2000, train Loss: 0.4894\n",
      "Epoch 111/2000, test Loss: 0.5455\n",
      "Epoch 112/2000, train Loss: 0.4854\n",
      "Epoch 112/2000, test Loss: 0.5414\n",
      "Epoch 113/2000, train Loss: 0.4812\n",
      "Epoch 113/2000, test Loss: 0.5370\n",
      "Epoch 114/2000, train Loss: 0.4771\n",
      "Epoch 114/2000, test Loss: 0.5328\n",
      "Epoch 115/2000, train Loss: 0.4724\n",
      "Epoch 115/2000, test Loss: 0.5283\n",
      "Epoch 116/2000, train Loss: 0.4677\n",
      "Epoch 116/2000, test Loss: 0.5239\n",
      "Epoch 117/2000, train Loss: 0.4634\n",
      "Epoch 117/2000, test Loss: 0.5191\n",
      "Epoch 118/2000, train Loss: 0.4583\n",
      "Epoch 118/2000, test Loss: 0.5145\n",
      "Epoch 119/2000, train Loss: 0.4535\n",
      "Epoch 119/2000, test Loss: 0.5099\n",
      "Epoch 120/2000, train Loss: 0.4485\n",
      "Epoch 120/2000, test Loss: 0.5049\n",
      "Epoch 121/2000, train Loss: 0.4436\n",
      "Epoch 121/2000, test Loss: 0.5002\n",
      "Epoch 122/2000, train Loss: 0.4388\n",
      "Epoch 122/2000, test Loss: 0.4949\n",
      "Epoch 123/2000, train Loss: 0.4335\n",
      "Epoch 123/2000, test Loss: 0.4900\n",
      "Epoch 124/2000, train Loss: 0.4283\n",
      "Epoch 124/2000, test Loss: 0.4850\n",
      "Epoch 125/2000, train Loss: 0.4228\n",
      "Epoch 125/2000, test Loss: 0.4797\n",
      "Epoch 126/2000, train Loss: 0.4174\n",
      "Epoch 126/2000, test Loss: 0.4745\n",
      "Epoch 127/2000, train Loss: 0.4120\n",
      "Epoch 127/2000, test Loss: 0.4690\n",
      "Epoch 128/2000, train Loss: 0.4065\n",
      "Epoch 128/2000, test Loss: 0.4636\n",
      "Epoch 129/2000, train Loss: 0.4009\n",
      "Epoch 129/2000, test Loss: 0.4580\n",
      "Epoch 130/2000, train Loss: 0.3954\n",
      "Epoch 130/2000, test Loss: 0.4527\n",
      "Epoch 131/2000, train Loss: 0.3899\n",
      "Epoch 131/2000, test Loss: 0.4472\n",
      "Epoch 132/2000, train Loss: 0.3840\n",
      "Epoch 132/2000, test Loss: 0.4420\n",
      "Epoch 133/2000, train Loss: 0.3786\n",
      "Epoch 133/2000, test Loss: 0.4366\n",
      "Epoch 134/2000, train Loss: 0.3730\n",
      "Epoch 134/2000, test Loss: 0.4308\n",
      "Epoch 135/2000, train Loss: 0.3674\n",
      "Epoch 135/2000, test Loss: 0.4254\n",
      "Epoch 136/2000, train Loss: 0.3621\n",
      "Epoch 136/2000, test Loss: 0.4198\n",
      "Epoch 137/2000, train Loss: 0.3562\n",
      "Epoch 137/2000, test Loss: 0.4143\n",
      "Epoch 138/2000, train Loss: 0.3507\n",
      "Epoch 138/2000, test Loss: 0.4093\n",
      "Epoch 139/2000, train Loss: 0.3452\n",
      "Epoch 139/2000, test Loss: 0.4037\n",
      "Epoch 140/2000, train Loss: 0.3396\n",
      "Epoch 140/2000, test Loss: 0.3982\n",
      "Epoch 141/2000, train Loss: 0.3344\n",
      "Epoch 141/2000, test Loss: 0.3930\n",
      "Epoch 142/2000, train Loss: 0.3295\n",
      "Epoch 142/2000, test Loss: 0.3879\n",
      "Epoch 143/2000, train Loss: 0.3237\n",
      "Epoch 143/2000, test Loss: 0.3824\n",
      "Epoch 144/2000, train Loss: 0.3184\n",
      "Epoch 144/2000, test Loss: 0.3770\n",
      "Epoch 145/2000, train Loss: 0.3132\n",
      "Epoch 145/2000, test Loss: 0.3722\n",
      "Epoch 146/2000, train Loss: 0.3082\n",
      "Epoch 146/2000, test Loss: 0.3667\n",
      "Epoch 147/2000, train Loss: 0.3031\n",
      "Epoch 147/2000, test Loss: 0.3616\n",
      "Epoch 148/2000, train Loss: 0.2982\n",
      "Epoch 148/2000, test Loss: 0.3569\n",
      "Epoch 149/2000, train Loss: 0.2931\n",
      "Epoch 149/2000, test Loss: 0.3522\n",
      "Epoch 150/2000, train Loss: 0.2885\n",
      "Epoch 150/2000, test Loss: 0.3476\n",
      "Epoch 151/2000, train Loss: 0.2841\n",
      "Epoch 151/2000, test Loss: 0.3426\n",
      "Epoch 152/2000, train Loss: 0.2791\n",
      "Epoch 152/2000, test Loss: 0.3376\n",
      "Epoch 153/2000, train Loss: 0.2746\n",
      "Epoch 153/2000, test Loss: 0.3332\n",
      "Epoch 154/2000, train Loss: 0.2701\n",
      "Epoch 154/2000, test Loss: 0.3288\n",
      "Epoch 155/2000, train Loss: 0.2656\n",
      "Epoch 155/2000, test Loss: 0.3246\n",
      "Epoch 156/2000, train Loss: 0.2616\n",
      "Epoch 156/2000, test Loss: 0.3202\n",
      "Epoch 157/2000, train Loss: 0.2575\n",
      "Epoch 157/2000, test Loss: 0.3159\n",
      "Epoch 158/2000, train Loss: 0.2530\n",
      "Epoch 158/2000, test Loss: 0.3117\n",
      "Epoch 159/2000, train Loss: 0.2493\n",
      "Epoch 159/2000, test Loss: 0.3074\n",
      "Epoch 160/2000, train Loss: 0.2455\n",
      "Epoch 160/2000, test Loss: 0.3035\n",
      "Epoch 161/2000, train Loss: 0.2415\n",
      "Epoch 161/2000, test Loss: 0.2997\n",
      "Epoch 162/2000, train Loss: 0.2378\n",
      "Epoch 162/2000, test Loss: 0.2962\n",
      "Epoch 163/2000, train Loss: 0.2341\n",
      "Epoch 163/2000, test Loss: 0.2921\n",
      "Epoch 164/2000, train Loss: 0.2306\n",
      "Epoch 164/2000, test Loss: 0.2884\n",
      "Epoch 165/2000, train Loss: 0.2270\n",
      "Epoch 165/2000, test Loss: 0.2848\n",
      "Epoch 166/2000, train Loss: 0.2235\n",
      "Epoch 166/2000, test Loss: 0.2815\n",
      "Epoch 167/2000, train Loss: 0.2203\n",
      "Epoch 167/2000, test Loss: 0.2780\n",
      "Epoch 168/2000, train Loss: 0.2170\n",
      "Epoch 168/2000, test Loss: 0.2747\n",
      "Epoch 169/2000, train Loss: 0.2137\n",
      "Epoch 169/2000, test Loss: 0.2711\n",
      "Epoch 170/2000, train Loss: 0.2107\n",
      "Epoch 170/2000, test Loss: 0.2681\n",
      "Epoch 171/2000, train Loss: 0.2077\n",
      "Epoch 171/2000, test Loss: 0.2648\n",
      "Epoch 172/2000, train Loss: 0.2046\n",
      "Epoch 172/2000, test Loss: 0.2615\n",
      "Epoch 173/2000, train Loss: 0.2018\n",
      "Epoch 173/2000, test Loss: 0.2585\n",
      "Epoch 174/2000, train Loss: 0.1988\n",
      "Epoch 174/2000, test Loss: 0.2553\n",
      "Epoch 175/2000, train Loss: 0.1960\n",
      "Epoch 175/2000, test Loss: 0.2528\n",
      "Epoch 176/2000, train Loss: 0.1931\n",
      "Epoch 176/2000, test Loss: 0.2494\n",
      "Epoch 177/2000, train Loss: 0.1904\n",
      "Epoch 177/2000, test Loss: 0.2469\n",
      "Epoch 178/2000, train Loss: 0.1877\n",
      "Epoch 178/2000, test Loss: 0.2438\n",
      "Epoch 179/2000, train Loss: 0.1851\n",
      "Epoch 179/2000, test Loss: 0.2410\n",
      "Epoch 180/2000, train Loss: 0.1826\n",
      "Epoch 180/2000, test Loss: 0.2384\n",
      "Epoch 181/2000, train Loss: 0.1800\n",
      "Epoch 181/2000, test Loss: 0.2360\n",
      "Epoch 182/2000, train Loss: 0.1773\n",
      "Epoch 182/2000, test Loss: 0.2330\n",
      "Epoch 183/2000, train Loss: 0.1748\n",
      "Epoch 183/2000, test Loss: 0.2302\n",
      "Epoch 184/2000, train Loss: 0.1724\n",
      "Epoch 184/2000, test Loss: 0.2279\n",
      "Epoch 185/2000, train Loss: 0.1700\n",
      "Epoch 185/2000, test Loss: 0.2252\n",
      "Epoch 186/2000, train Loss: 0.1676\n",
      "Epoch 186/2000, test Loss: 0.2225\n",
      "Epoch 187/2000, train Loss: 0.1653\n",
      "Epoch 187/2000, test Loss: 0.2200\n",
      "Epoch 188/2000, train Loss: 0.1629\n",
      "Epoch 188/2000, test Loss: 0.2174\n",
      "Epoch 189/2000, train Loss: 0.1607\n",
      "Epoch 189/2000, test Loss: 0.2150\n",
      "Epoch 190/2000, train Loss: 0.1583\n",
      "Epoch 190/2000, test Loss: 0.2123\n",
      "Epoch 191/2000, train Loss: 0.1561\n",
      "Epoch 191/2000, test Loss: 0.2101\n",
      "Epoch 192/2000, train Loss: 0.1538\n",
      "Epoch 192/2000, test Loss: 0.2075\n",
      "Epoch 193/2000, train Loss: 0.1517\n",
      "Epoch 193/2000, test Loss: 0.2050\n",
      "Epoch 194/2000, train Loss: 0.1493\n",
      "Epoch 194/2000, test Loss: 0.2027\n",
      "Epoch 195/2000, train Loss: 0.1474\n",
      "Epoch 195/2000, test Loss: 0.2003\n",
      "Epoch 196/2000, train Loss: 0.1450\n",
      "Epoch 196/2000, test Loss: 0.1981\n",
      "Epoch 197/2000, train Loss: 0.1429\n",
      "Epoch 197/2000, test Loss: 0.1954\n",
      "Epoch 198/2000, train Loss: 0.1408\n",
      "Epoch 198/2000, test Loss: 0.1930\n",
      "Epoch 199/2000, train Loss: 0.1386\n",
      "Epoch 199/2000, test Loss: 0.1910\n",
      "Epoch 200/2000, train Loss: 0.1366\n",
      "Epoch 200/2000, test Loss: 0.1884\n",
      "Epoch 201/2000, train Loss: 0.1345\n",
      "Epoch 201/2000, test Loss: 0.1864\n",
      "Epoch 202/2000, train Loss: 0.1325\n",
      "Epoch 202/2000, test Loss: 0.1838\n",
      "Epoch 203/2000, train Loss: 0.1305\n",
      "Epoch 203/2000, test Loss: 0.1816\n",
      "Epoch 204/2000, train Loss: 0.1286\n",
      "Epoch 204/2000, test Loss: 0.1798\n",
      "Epoch 205/2000, train Loss: 0.1266\n",
      "Epoch 205/2000, test Loss: 0.1772\n",
      "Epoch 206/2000, train Loss: 0.1244\n",
      "Epoch 206/2000, test Loss: 0.1750\n",
      "Epoch 207/2000, train Loss: 0.1226\n",
      "Epoch 207/2000, test Loss: 0.1727\n",
      "Epoch 208/2000, train Loss: 0.1206\n",
      "Epoch 208/2000, test Loss: 0.1706\n",
      "Epoch 209/2000, train Loss: 0.1189\n",
      "Epoch 209/2000, test Loss: 0.1684\n",
      "Epoch 210/2000, train Loss: 0.1168\n",
      "Epoch 210/2000, test Loss: 0.1665\n",
      "Epoch 211/2000, train Loss: 0.1150\n",
      "Epoch 211/2000, test Loss: 0.1645\n",
      "Epoch 212/2000, train Loss: 0.1132\n",
      "Epoch 212/2000, test Loss: 0.1623\n",
      "Epoch 213/2000, train Loss: 0.1114\n",
      "Epoch 213/2000, test Loss: 0.1602\n",
      "Epoch 214/2000, train Loss: 0.1097\n",
      "Epoch 214/2000, test Loss: 0.1583\n",
      "Epoch 215/2000, train Loss: 0.1081\n",
      "Epoch 215/2000, test Loss: 0.1566\n",
      "Epoch 216/2000, train Loss: 0.1063\n",
      "Epoch 216/2000, test Loss: 0.1543\n",
      "Epoch 217/2000, train Loss: 0.1045\n",
      "Epoch 217/2000, test Loss: 0.1524\n",
      "Epoch 218/2000, train Loss: 0.1029\n",
      "Epoch 218/2000, test Loss: 0.1507\n",
      "Epoch 219/2000, train Loss: 0.1013\n",
      "Epoch 219/2000, test Loss: 0.1490\n",
      "Epoch 220/2000, train Loss: 0.0998\n",
      "Epoch 220/2000, test Loss: 0.1467\n",
      "Epoch 221/2000, train Loss: 0.0981\n",
      "Epoch 221/2000, test Loss: 0.1450\n",
      "Epoch 222/2000, train Loss: 0.0967\n",
      "Epoch 222/2000, test Loss: 0.1433\n",
      "Epoch 223/2000, train Loss: 0.0952\n",
      "Epoch 223/2000, test Loss: 0.1414\n",
      "Epoch 224/2000, train Loss: 0.0937\n",
      "Epoch 224/2000, test Loss: 0.1400\n",
      "Epoch 225/2000, train Loss: 0.0922\n",
      "Epoch 225/2000, test Loss: 0.1385\n",
      "Epoch 226/2000, train Loss: 0.0909\n",
      "Epoch 226/2000, test Loss: 0.1364\n",
      "Epoch 227/2000, train Loss: 0.0895\n",
      "Epoch 227/2000, test Loss: 0.1347\n",
      "Epoch 228/2000, train Loss: 0.0881\n",
      "Epoch 228/2000, test Loss: 0.1333\n",
      "Epoch 229/2000, train Loss: 0.0868\n",
      "Epoch 229/2000, test Loss: 0.1316\n",
      "Epoch 230/2000, train Loss: 0.0856\n",
      "Epoch 230/2000, test Loss: 0.1304\n",
      "Epoch 231/2000, train Loss: 0.0844\n",
      "Epoch 231/2000, test Loss: 0.1287\n",
      "Epoch 232/2000, train Loss: 0.0831\n",
      "Epoch 232/2000, test Loss: 0.1271\n",
      "Epoch 233/2000, train Loss: 0.0819\n",
      "Epoch 233/2000, test Loss: 0.1257\n",
      "Epoch 234/2000, train Loss: 0.0808\n",
      "Epoch 234/2000, test Loss: 0.1242\n",
      "Epoch 235/2000, train Loss: 0.0797\n",
      "Epoch 235/2000, test Loss: 0.1232\n",
      "Epoch 236/2000, train Loss: 0.0785\n",
      "Epoch 236/2000, test Loss: 0.1216\n",
      "Epoch 237/2000, train Loss: 0.0777\n",
      "Epoch 237/2000, test Loss: 0.1203\n",
      "Epoch 238/2000, train Loss: 0.0763\n",
      "Epoch 238/2000, test Loss: 0.1188\n",
      "Epoch 239/2000, train Loss: 0.0753\n",
      "Epoch 239/2000, test Loss: 0.1173\n",
      "Epoch 240/2000, train Loss: 0.0744\n",
      "Epoch 240/2000, test Loss: 0.1162\n",
      "Epoch 241/2000, train Loss: 0.0733\n",
      "Epoch 241/2000, test Loss: 0.1150\n",
      "Epoch 242/2000, train Loss: 0.0723\n",
      "Epoch 242/2000, test Loss: 0.1141\n",
      "Epoch 243/2000, train Loss: 0.0714\n",
      "Epoch 243/2000, test Loss: 0.1129\n",
      "Epoch 244/2000, train Loss: 0.0705\n",
      "Epoch 244/2000, test Loss: 0.1117\n",
      "Epoch 245/2000, train Loss: 0.0696\n",
      "Epoch 245/2000, test Loss: 0.1104\n",
      "Epoch 246/2000, train Loss: 0.0687\n",
      "Epoch 246/2000, test Loss: 0.1090\n",
      "Epoch 247/2000, train Loss: 0.0679\n",
      "Epoch 247/2000, test Loss: 0.1076\n",
      "Epoch 248/2000, train Loss: 0.0670\n",
      "Epoch 248/2000, test Loss: 0.1067\n",
      "Epoch 249/2000, train Loss: 0.0663\n",
      "Epoch 249/2000, test Loss: 0.1054\n",
      "Epoch 250/2000, train Loss: 0.0653\n",
      "Epoch 250/2000, test Loss: 0.1047\n",
      "Epoch 251/2000, train Loss: 0.0646\n",
      "Epoch 251/2000, test Loss: 0.1032\n",
      "Epoch 252/2000, train Loss: 0.0638\n",
      "Epoch 252/2000, test Loss: 0.1026\n",
      "Epoch 253/2000, train Loss: 0.0631\n",
      "Epoch 253/2000, test Loss: 0.1009\n",
      "Epoch 254/2000, train Loss: 0.0622\n",
      "Epoch 254/2000, test Loss: 0.1001\n",
      "Epoch 255/2000, train Loss: 0.0615\n",
      "Epoch 255/2000, test Loss: 0.0990\n",
      "Epoch 256/2000, train Loss: 0.0608\n",
      "Epoch 256/2000, test Loss: 0.0980\n",
      "Epoch 257/2000, train Loss: 0.0601\n",
      "Epoch 257/2000, test Loss: 0.0969\n",
      "Epoch 258/2000, train Loss: 0.0595\n",
      "Epoch 258/2000, test Loss: 0.0962\n",
      "Epoch 259/2000, train Loss: 0.0588\n",
      "Epoch 259/2000, test Loss: 0.0950\n",
      "Epoch 260/2000, train Loss: 0.0582\n",
      "Epoch 260/2000, test Loss: 0.0939\n",
      "Epoch 261/2000, train Loss: 0.0575\n",
      "Epoch 261/2000, test Loss: 0.0930\n",
      "Epoch 262/2000, train Loss: 0.0568\n",
      "Epoch 262/2000, test Loss: 0.0920\n",
      "Epoch 263/2000, train Loss: 0.0562\n",
      "Epoch 263/2000, test Loss: 0.0910\n",
      "Epoch 264/2000, train Loss: 0.0556\n",
      "Epoch 264/2000, test Loss: 0.0903\n",
      "Epoch 265/2000, train Loss: 0.0550\n",
      "Epoch 265/2000, test Loss: 0.0891\n",
      "Epoch 266/2000, train Loss: 0.0544\n",
      "Epoch 266/2000, test Loss: 0.0883\n",
      "Epoch 267/2000, train Loss: 0.0538\n",
      "Epoch 267/2000, test Loss: 0.0875\n",
      "Epoch 268/2000, train Loss: 0.0532\n",
      "Epoch 268/2000, test Loss: 0.0865\n",
      "Epoch 269/2000, train Loss: 0.0528\n",
      "Epoch 269/2000, test Loss: 0.0855\n",
      "Epoch 270/2000, train Loss: 0.0522\n",
      "Epoch 270/2000, test Loss: 0.0847\n",
      "Epoch 271/2000, train Loss: 0.0516\n",
      "Epoch 271/2000, test Loss: 0.0839\n",
      "Epoch 272/2000, train Loss: 0.0511\n",
      "Epoch 272/2000, test Loss: 0.0831\n",
      "Epoch 273/2000, train Loss: 0.0506\n",
      "Epoch 273/2000, test Loss: 0.0821\n",
      "Epoch 274/2000, train Loss: 0.0502\n",
      "Epoch 274/2000, test Loss: 0.0814\n",
      "Epoch 275/2000, train Loss: 0.0495\n",
      "Epoch 275/2000, test Loss: 0.0805\n",
      "Epoch 276/2000, train Loss: 0.0490\n",
      "Epoch 276/2000, test Loss: 0.0797\n",
      "Epoch 277/2000, train Loss: 0.0485\n",
      "Epoch 277/2000, test Loss: 0.0789\n",
      "Epoch 278/2000, train Loss: 0.0481\n",
      "Epoch 278/2000, test Loss: 0.0779\n",
      "Epoch 279/2000, train Loss: 0.0476\n",
      "Epoch 279/2000, test Loss: 0.0773\n",
      "Epoch 280/2000, train Loss: 0.0471\n",
      "Epoch 280/2000, test Loss: 0.0764\n",
      "Epoch 281/2000, train Loss: 0.0468\n",
      "Epoch 281/2000, test Loss: 0.0757\n",
      "Epoch 282/2000, train Loss: 0.0462\n",
      "Epoch 282/2000, test Loss: 0.0748\n",
      "Epoch 283/2000, train Loss: 0.0458\n",
      "Epoch 283/2000, test Loss: 0.0743\n",
      "Epoch 284/2000, train Loss: 0.0454\n",
      "Epoch 284/2000, test Loss: 0.0734\n",
      "Epoch 285/2000, train Loss: 0.0450\n",
      "Epoch 285/2000, test Loss: 0.0728\n",
      "Epoch 286/2000, train Loss: 0.0445\n",
      "Epoch 286/2000, test Loss: 0.0721\n",
      "Epoch 287/2000, train Loss: 0.0442\n",
      "Epoch 287/2000, test Loss: 0.0716\n",
      "Epoch 288/2000, train Loss: 0.0437\n",
      "Epoch 288/2000, test Loss: 0.0705\n",
      "Epoch 289/2000, train Loss: 0.0434\n",
      "Epoch 289/2000, test Loss: 0.0701\n",
      "Epoch 290/2000, train Loss: 0.0429\n",
      "Epoch 290/2000, test Loss: 0.0691\n",
      "Epoch 291/2000, train Loss: 0.0425\n",
      "Epoch 291/2000, test Loss: 0.0685\n",
      "Epoch 292/2000, train Loss: 0.0421\n",
      "Epoch 292/2000, test Loss: 0.0677\n",
      "Epoch 293/2000, train Loss: 0.0418\n",
      "Epoch 293/2000, test Loss: 0.0672\n",
      "Epoch 294/2000, train Loss: 0.0414\n",
      "Epoch 294/2000, test Loss: 0.0664\n",
      "Epoch 295/2000, train Loss: 0.0410\n",
      "Epoch 295/2000, test Loss: 0.0663\n",
      "Epoch 296/2000, train Loss: 0.0407\n",
      "Epoch 296/2000, test Loss: 0.0653\n",
      "Epoch 297/2000, train Loss: 0.0402\n",
      "Epoch 297/2000, test Loss: 0.0647\n",
      "Epoch 298/2000, train Loss: 0.0399\n",
      "Epoch 298/2000, test Loss: 0.0642\n",
      "Epoch 299/2000, train Loss: 0.0395\n",
      "Epoch 299/2000, test Loss: 0.0632\n",
      "Epoch 300/2000, train Loss: 0.0393\n",
      "Epoch 300/2000, test Loss: 0.0630\n",
      "Epoch 301/2000, train Loss: 0.0389\n",
      "Epoch 301/2000, test Loss: 0.0624\n",
      "Epoch 302/2000, train Loss: 0.0385\n",
      "Epoch 302/2000, test Loss: 0.0614\n",
      "Epoch 303/2000, train Loss: 0.0382\n",
      "Epoch 303/2000, test Loss: 0.0611\n",
      "Epoch 304/2000, train Loss: 0.0379\n",
      "Epoch 304/2000, test Loss: 0.0604\n",
      "Epoch 305/2000, train Loss: 0.0375\n",
      "Epoch 305/2000, test Loss: 0.0603\n",
      "Epoch 306/2000, train Loss: 0.0373\n",
      "Epoch 306/2000, test Loss: 0.0593\n",
      "Epoch 307/2000, train Loss: 0.0369\n",
      "Epoch 307/2000, test Loss: 0.0590\n",
      "Epoch 308/2000, train Loss: 0.0366\n",
      "Epoch 308/2000, test Loss: 0.0586\n",
      "Epoch 309/2000, train Loss: 0.0363\n",
      "Epoch 309/2000, test Loss: 0.0578\n",
      "Epoch 310/2000, train Loss: 0.0359\n",
      "Epoch 310/2000, test Loss: 0.0572\n",
      "Epoch 311/2000, train Loss: 0.0357\n",
      "Epoch 311/2000, test Loss: 0.0569\n",
      "Epoch 312/2000, train Loss: 0.0354\n",
      "Epoch 312/2000, test Loss: 0.0565\n",
      "Epoch 313/2000, train Loss: 0.0351\n",
      "Epoch 313/2000, test Loss: 0.0558\n",
      "Epoch 314/2000, train Loss: 0.0348\n",
      "Epoch 314/2000, test Loss: 0.0554\n",
      "Epoch 315/2000, train Loss: 0.0345\n",
      "Epoch 315/2000, test Loss: 0.0546\n",
      "Epoch 316/2000, train Loss: 0.0343\n",
      "Epoch 316/2000, test Loss: 0.0544\n",
      "Epoch 317/2000, train Loss: 0.0340\n",
      "Epoch 317/2000, test Loss: 0.0537\n",
      "Epoch 318/2000, train Loss: 0.0337\n",
      "Epoch 318/2000, test Loss: 0.0532\n",
      "Epoch 319/2000, train Loss: 0.0334\n",
      "Epoch 319/2000, test Loss: 0.0530\n",
      "Epoch 320/2000, train Loss: 0.0332\n",
      "Epoch 320/2000, test Loss: 0.0525\n",
      "Epoch 321/2000, train Loss: 0.0329\n",
      "Epoch 321/2000, test Loss: 0.0518\n",
      "Epoch 322/2000, train Loss: 0.0327\n",
      "Epoch 322/2000, test Loss: 0.0518\n",
      "Epoch 323/2000, train Loss: 0.0324\n",
      "Epoch 323/2000, test Loss: 0.0511\n",
      "Epoch 324/2000, train Loss: 0.0321\n",
      "Epoch 324/2000, test Loss: 0.0506\n",
      "Epoch 325/2000, train Loss: 0.0318\n",
      "Epoch 325/2000, test Loss: 0.0500\n",
      "Epoch 326/2000, train Loss: 0.0316\n",
      "Epoch 326/2000, test Loss: 0.0497\n",
      "Epoch 327/2000, train Loss: 0.0313\n",
      "Epoch 327/2000, test Loss: 0.0494\n",
      "Epoch 328/2000, train Loss: 0.0311\n",
      "Epoch 328/2000, test Loss: 0.0490\n",
      "Epoch 329/2000, train Loss: 0.0308\n",
      "Epoch 329/2000, test Loss: 0.0485\n",
      "Epoch 330/2000, train Loss: 0.0306\n",
      "Epoch 330/2000, test Loss: 0.0480\n",
      "Epoch 331/2000, train Loss: 0.0304\n",
      "Epoch 331/2000, test Loss: 0.0476\n",
      "Epoch 332/2000, train Loss: 0.0301\n",
      "Epoch 332/2000, test Loss: 0.0474\n",
      "Epoch 333/2000, train Loss: 0.0300\n",
      "Epoch 333/2000, test Loss: 0.0468\n",
      "Epoch 334/2000, train Loss: 0.0297\n",
      "Epoch 334/2000, test Loss: 0.0465\n",
      "Epoch 335/2000, train Loss: 0.0294\n",
      "Epoch 335/2000, test Loss: 0.0462\n",
      "Epoch 336/2000, train Loss: 0.0292\n",
      "Epoch 336/2000, test Loss: 0.0457\n",
      "Epoch 337/2000, train Loss: 0.0291\n",
      "Epoch 337/2000, test Loss: 0.0456\n",
      "Epoch 338/2000, train Loss: 0.0287\n",
      "Epoch 338/2000, test Loss: 0.0450\n",
      "Epoch 339/2000, train Loss: 0.0286\n",
      "Epoch 339/2000, test Loss: 0.0447\n",
      "Epoch 340/2000, train Loss: 0.0283\n",
      "Epoch 340/2000, test Loss: 0.0442\n",
      "Epoch 341/2000, train Loss: 0.0282\n",
      "Epoch 341/2000, test Loss: 0.0441\n",
      "Epoch 342/2000, train Loss: 0.0279\n",
      "Epoch 342/2000, test Loss: 0.0437\n",
      "Epoch 343/2000, train Loss: 0.0276\n",
      "Epoch 343/2000, test Loss: 0.0434\n",
      "Epoch 344/2000, train Loss: 0.0274\n",
      "Epoch 344/2000, test Loss: 0.0427\n",
      "Epoch 345/2000, train Loss: 0.0272\n",
      "Epoch 345/2000, test Loss: 0.0427\n",
      "Epoch 346/2000, train Loss: 0.0271\n",
      "Epoch 346/2000, test Loss: 0.0422\n",
      "Epoch 347/2000, train Loss: 0.0268\n",
      "Epoch 347/2000, test Loss: 0.0418\n",
      "Epoch 348/2000, train Loss: 0.0266\n",
      "Epoch 348/2000, test Loss: 0.0414\n",
      "Epoch 349/2000, train Loss: 0.0265\n",
      "Epoch 349/2000, test Loss: 0.0410\n",
      "Epoch 350/2000, train Loss: 0.0262\n",
      "Epoch 350/2000, test Loss: 0.0406\n",
      "Epoch 351/2000, train Loss: 0.0260\n",
      "Epoch 351/2000, test Loss: 0.0404\n",
      "Epoch 352/2000, train Loss: 0.0258\n",
      "Epoch 352/2000, test Loss: 0.0400\n",
      "Epoch 353/2000, train Loss: 0.0255\n",
      "Epoch 353/2000, test Loss: 0.0399\n",
      "Epoch 354/2000, train Loss: 0.0253\n",
      "Epoch 354/2000, test Loss: 0.0395\n",
      "Epoch 355/2000, train Loss: 0.0252\n",
      "Epoch 355/2000, test Loss: 0.0392\n",
      "Epoch 356/2000, train Loss: 0.0250\n",
      "Epoch 356/2000, test Loss: 0.0388\n",
      "Epoch 357/2000, train Loss: 0.0248\n",
      "Epoch 357/2000, test Loss: 0.0386\n",
      "Epoch 358/2000, train Loss: 0.0246\n",
      "Epoch 358/2000, test Loss: 0.0382\n",
      "Epoch 359/2000, train Loss: 0.0244\n",
      "Epoch 359/2000, test Loss: 0.0378\n",
      "Epoch 360/2000, train Loss: 0.0243\n",
      "Epoch 360/2000, test Loss: 0.0378\n",
      "Epoch 361/2000, train Loss: 0.0241\n",
      "Epoch 361/2000, test Loss: 0.0372\n",
      "Epoch 362/2000, train Loss: 0.0239\n",
      "Epoch 362/2000, test Loss: 0.0369\n",
      "Epoch 363/2000, train Loss: 0.0237\n",
      "Epoch 363/2000, test Loss: 0.0366\n",
      "Epoch 364/2000, train Loss: 0.0235\n",
      "Epoch 364/2000, test Loss: 0.0363\n",
      "Epoch 365/2000, train Loss: 0.0233\n",
      "Epoch 365/2000, test Loss: 0.0362\n",
      "Epoch 366/2000, train Loss: 0.0231\n",
      "Epoch 366/2000, test Loss: 0.0358\n",
      "Epoch 367/2000, train Loss: 0.0230\n",
      "Epoch 367/2000, test Loss: 0.0355\n",
      "Epoch 368/2000, train Loss: 0.0229\n",
      "Epoch 368/2000, test Loss: 0.0352\n",
      "Epoch 369/2000, train Loss: 0.0226\n",
      "Epoch 369/2000, test Loss: 0.0350\n",
      "Epoch 370/2000, train Loss: 0.0224\n",
      "Epoch 370/2000, test Loss: 0.0346\n",
      "Epoch 371/2000, train Loss: 0.0223\n",
      "Epoch 371/2000, test Loss: 0.0344\n",
      "Epoch 372/2000, train Loss: 0.0221\n",
      "Epoch 372/2000, test Loss: 0.0340\n",
      "Epoch 373/2000, train Loss: 0.0219\n",
      "Epoch 373/2000, test Loss: 0.0336\n",
      "Epoch 374/2000, train Loss: 0.0218\n",
      "Epoch 374/2000, test Loss: 0.0336\n",
      "Epoch 375/2000, train Loss: 0.0216\n",
      "Epoch 375/2000, test Loss: 0.0333\n",
      "Epoch 376/2000, train Loss: 0.0214\n",
      "Epoch 376/2000, test Loss: 0.0330\n",
      "Epoch 377/2000, train Loss: 0.0212\n",
      "Epoch 377/2000, test Loss: 0.0327\n",
      "Epoch 378/2000, train Loss: 0.0211\n",
      "Epoch 378/2000, test Loss: 0.0323\n",
      "Epoch 379/2000, train Loss: 0.0210\n",
      "Epoch 379/2000, test Loss: 0.0321\n",
      "Epoch 380/2000, train Loss: 0.0208\n",
      "Epoch 380/2000, test Loss: 0.0321\n",
      "Epoch 381/2000, train Loss: 0.0207\n",
      "Epoch 381/2000, test Loss: 0.0318\n",
      "Epoch 382/2000, train Loss: 0.0205\n",
      "Epoch 382/2000, test Loss: 0.0315\n",
      "Epoch 383/2000, train Loss: 0.0203\n",
      "Epoch 383/2000, test Loss: 0.0312\n",
      "Epoch 384/2000, train Loss: 0.0202\n",
      "Epoch 384/2000, test Loss: 0.0309\n",
      "Epoch 385/2000, train Loss: 0.0200\n",
      "Epoch 385/2000, test Loss: 0.0306\n",
      "Epoch 386/2000, train Loss: 0.0199\n",
      "Epoch 386/2000, test Loss: 0.0305\n",
      "Epoch 387/2000, train Loss: 0.0198\n",
      "Epoch 387/2000, test Loss: 0.0302\n",
      "Epoch 388/2000, train Loss: 0.0196\n",
      "Epoch 388/2000, test Loss: 0.0302\n",
      "Epoch 389/2000, train Loss: 0.0194\n",
      "Epoch 389/2000, test Loss: 0.0297\n",
      "Epoch 390/2000, train Loss: 0.0193\n",
      "Epoch 390/2000, test Loss: 0.0295\n",
      "Epoch 391/2000, train Loss: 0.0191\n",
      "Epoch 391/2000, test Loss: 0.0294\n",
      "Epoch 392/2000, train Loss: 0.0190\n",
      "Epoch 392/2000, test Loss: 0.0293\n",
      "Epoch 393/2000, train Loss: 0.0189\n",
      "Epoch 393/2000, test Loss: 0.0289\n",
      "Epoch 394/2000, train Loss: 0.0188\n",
      "Epoch 394/2000, test Loss: 0.0288\n",
      "Epoch 395/2000, train Loss: 0.0186\n",
      "Epoch 395/2000, test Loss: 0.0285\n",
      "Epoch 396/2000, train Loss: 0.0185\n",
      "Epoch 396/2000, test Loss: 0.0285\n",
      "Epoch 397/2000, train Loss: 0.0184\n",
      "Epoch 397/2000, test Loss: 0.0280\n",
      "Epoch 398/2000, train Loss: 0.0182\n",
      "Epoch 398/2000, test Loss: 0.0282\n",
      "Epoch 399/2000, train Loss: 0.0182\n",
      "Epoch 399/2000, test Loss: 0.0274\n",
      "Epoch 400/2000, train Loss: 0.0180\n",
      "Epoch 400/2000, test Loss: 0.0278\n",
      "Epoch 401/2000, train Loss: 0.0179\n",
      "Epoch 401/2000, test Loss: 0.0273\n",
      "Epoch 402/2000, train Loss: 0.0177\n",
      "Epoch 402/2000, test Loss: 0.0272\n",
      "Epoch 403/2000, train Loss: 0.0177\n",
      "Epoch 403/2000, test Loss: 0.0269\n",
      "Epoch 404/2000, train Loss: 0.0175\n",
      "Epoch 404/2000, test Loss: 0.0267\n",
      "Epoch 405/2000, train Loss: 0.0175\n",
      "Epoch 405/2000, test Loss: 0.0267\n",
      "Epoch 406/2000, train Loss: 0.0173\n",
      "Epoch 406/2000, test Loss: 0.0264\n",
      "Epoch 407/2000, train Loss: 0.0172\n",
      "Epoch 407/2000, test Loss: 0.0264\n",
      "Epoch 408/2000, train Loss: 0.0171\n",
      "Epoch 408/2000, test Loss: 0.0260\n",
      "Epoch 409/2000, train Loss: 0.0170\n",
      "Epoch 409/2000, test Loss: 0.0259\n",
      "Epoch 410/2000, train Loss: 0.0168\n",
      "Epoch 410/2000, test Loss: 0.0257\n",
      "Epoch 411/2000, train Loss: 0.0167\n",
      "Epoch 411/2000, test Loss: 0.0257\n",
      "Epoch 412/2000, train Loss: 0.0167\n",
      "Epoch 412/2000, test Loss: 0.0252\n",
      "Epoch 413/2000, train Loss: 0.0165\n",
      "Epoch 413/2000, test Loss: 0.0252\n",
      "Epoch 414/2000, train Loss: 0.0165\n",
      "Epoch 414/2000, test Loss: 0.0251\n",
      "Epoch 415/2000, train Loss: 0.0163\n",
      "Epoch 415/2000, test Loss: 0.0249\n",
      "Epoch 416/2000, train Loss: 0.0162\n",
      "Epoch 416/2000, test Loss: 0.0248\n",
      "Epoch 417/2000, train Loss: 0.0161\n",
      "Epoch 417/2000, test Loss: 0.0245\n",
      "Epoch 418/2000, train Loss: 0.0161\n",
      "Epoch 418/2000, test Loss: 0.0245\n",
      "Epoch 419/2000, train Loss: 0.0159\n",
      "Epoch 419/2000, test Loss: 0.0243\n",
      "Epoch 420/2000, train Loss: 0.0158\n",
      "Epoch 420/2000, test Loss: 0.0242\n",
      "Epoch 421/2000, train Loss: 0.0157\n",
      "Epoch 421/2000, test Loss: 0.0242\n",
      "Epoch 422/2000, train Loss: 0.0157\n",
      "Epoch 422/2000, test Loss: 0.0238\n",
      "Epoch 423/2000, train Loss: 0.0156\n",
      "Epoch 423/2000, test Loss: 0.0237\n",
      "Epoch 424/2000, train Loss: 0.0154\n",
      "Epoch 424/2000, test Loss: 0.0235\n",
      "Epoch 425/2000, train Loss: 0.0153\n",
      "Epoch 425/2000, test Loss: 0.0236\n",
      "Epoch 426/2000, train Loss: 0.0152\n",
      "Epoch 426/2000, test Loss: 0.0234\n",
      "Epoch 427/2000, train Loss: 0.0152\n",
      "Epoch 427/2000, test Loss: 0.0235\n",
      "Epoch 428/2000, train Loss: 0.0152\n",
      "Epoch 428/2000, test Loss: 0.0232\n",
      "Epoch 429/2000, train Loss: 0.0151\n",
      "Epoch 429/2000, test Loss: 0.0228\n",
      "Epoch 430/2000, train Loss: 0.0149\n",
      "Epoch 430/2000, test Loss: 0.0229\n",
      "Epoch 431/2000, train Loss: 0.0148\n",
      "Epoch 431/2000, test Loss: 0.0226\n",
      "Epoch 432/2000, train Loss: 0.0147\n",
      "Epoch 432/2000, test Loss: 0.0226\n",
      "Epoch 433/2000, train Loss: 0.0147\n",
      "Epoch 433/2000, test Loss: 0.0224\n",
      "Epoch 434/2000, train Loss: 0.0146\n",
      "Epoch 434/2000, test Loss: 0.0224\n",
      "Epoch 435/2000, train Loss: 0.0145\n",
      "Epoch 435/2000, test Loss: 0.0223\n",
      "Epoch 436/2000, train Loss: 0.0144\n",
      "Epoch 436/2000, test Loss: 0.0220\n",
      "Epoch 437/2000, train Loss: 0.0144\n",
      "Epoch 437/2000, test Loss: 0.0221\n",
      "Epoch 438/2000, train Loss: 0.0143\n",
      "Epoch 438/2000, test Loss: 0.0219\n",
      "Epoch 439/2000, train Loss: 0.0142\n",
      "Epoch 439/2000, test Loss: 0.0218\n",
      "Epoch 440/2000, train Loss: 0.0141\n",
      "Epoch 440/2000, test Loss: 0.0215\n",
      "Epoch 441/2000, train Loss: 0.0141\n",
      "Epoch 441/2000, test Loss: 0.0216\n",
      "Epoch 442/2000, train Loss: 0.0139\n",
      "Epoch 442/2000, test Loss: 0.0214\n",
      "Epoch 443/2000, train Loss: 0.0139\n",
      "Epoch 443/2000, test Loss: 0.0214\n",
      "Epoch 444/2000, train Loss: 0.0139\n",
      "Epoch 444/2000, test Loss: 0.0215\n",
      "Epoch 445/2000, train Loss: 0.0137\n",
      "Epoch 445/2000, test Loss: 0.0209\n",
      "Epoch 446/2000, train Loss: 0.0136\n",
      "Epoch 446/2000, test Loss: 0.0211\n",
      "Epoch 447/2000, train Loss: 0.0136\n",
      "Epoch 447/2000, test Loss: 0.0209\n",
      "Epoch 448/2000, train Loss: 0.0136\n",
      "Epoch 448/2000, test Loss: 0.0209\n",
      "Epoch 449/2000, train Loss: 0.0135\n",
      "Epoch 449/2000, test Loss: 0.0207\n",
      "Epoch 450/2000, train Loss: 0.0134\n",
      "Epoch 450/2000, test Loss: 0.0208\n",
      "Epoch 451/2000, train Loss: 0.0133\n",
      "Epoch 451/2000, test Loss: 0.0204\n",
      "Epoch 452/2000, train Loss: 0.0132\n",
      "Epoch 452/2000, test Loss: 0.0204\n",
      "Epoch 453/2000, train Loss: 0.0131\n",
      "Epoch 453/2000, test Loss: 0.0204\n",
      "Epoch 454/2000, train Loss: 0.0131\n",
      "Epoch 454/2000, test Loss: 0.0202\n",
      "Epoch 455/2000, train Loss: 0.0130\n",
      "Epoch 455/2000, test Loss: 0.0201\n",
      "Epoch 456/2000, train Loss: 0.0129\n",
      "Epoch 456/2000, test Loss: 0.0201\n",
      "Epoch 457/2000, train Loss: 0.0129\n",
      "Epoch 457/2000, test Loss: 0.0200\n",
      "Epoch 458/2000, train Loss: 0.0128\n",
      "Epoch 458/2000, test Loss: 0.0197\n",
      "Epoch 459/2000, train Loss: 0.0127\n",
      "Epoch 459/2000, test Loss: 0.0198\n",
      "Epoch 460/2000, train Loss: 0.0126\n",
      "Epoch 460/2000, test Loss: 0.0197\n",
      "Epoch 461/2000, train Loss: 0.0126\n",
      "Epoch 461/2000, test Loss: 0.0194\n",
      "Epoch 462/2000, train Loss: 0.0125\n",
      "Epoch 462/2000, test Loss: 0.0193\n",
      "Epoch 463/2000, train Loss: 0.0125\n",
      "Epoch 463/2000, test Loss: 0.0194\n",
      "Epoch 464/2000, train Loss: 0.0125\n",
      "Epoch 464/2000, test Loss: 0.0193\n",
      "Epoch 465/2000, train Loss: 0.0124\n",
      "Epoch 465/2000, test Loss: 0.0194\n",
      "Epoch 466/2000, train Loss: 0.0123\n",
      "Epoch 466/2000, test Loss: 0.0189\n",
      "Epoch 467/2000, train Loss: 0.0123\n",
      "Epoch 467/2000, test Loss: 0.0193\n",
      "Epoch 468/2000, train Loss: 0.0123\n",
      "Epoch 468/2000, test Loss: 0.0188\n",
      "Epoch 469/2000, train Loss: 0.0121\n",
      "Epoch 469/2000, test Loss: 0.0189\n",
      "Epoch 470/2000, train Loss: 0.0121\n",
      "Epoch 470/2000, test Loss: 0.0190\n",
      "Epoch 471/2000, train Loss: 0.0120\n",
      "Epoch 471/2000, test Loss: 0.0187\n",
      "Epoch 472/2000, train Loss: 0.0119\n",
      "Epoch 472/2000, test Loss: 0.0186\n",
      "Epoch 473/2000, train Loss: 0.0119\n",
      "Epoch 473/2000, test Loss: 0.0183\n",
      "Epoch 474/2000, train Loss: 0.0118\n",
      "Epoch 474/2000, test Loss: 0.0185\n",
      "Epoch 475/2000, train Loss: 0.0118\n",
      "Epoch 475/2000, test Loss: 0.0183\n",
      "Epoch 476/2000, train Loss: 0.0117\n",
      "Epoch 476/2000, test Loss: 0.0182\n",
      "Epoch 477/2000, train Loss: 0.0116\n",
      "Epoch 477/2000, test Loss: 0.0183\n",
      "Epoch 478/2000, train Loss: 0.0116\n",
      "Epoch 478/2000, test Loss: 0.0181\n",
      "Epoch 479/2000, train Loss: 0.0116\n",
      "Epoch 479/2000, test Loss: 0.0180\n",
      "Epoch 480/2000, train Loss: 0.0115\n",
      "Epoch 480/2000, test Loss: 0.0183\n",
      "Epoch 481/2000, train Loss: 0.0114\n",
      "Epoch 481/2000, test Loss: 0.0177\n",
      "Epoch 482/2000, train Loss: 0.0114\n",
      "Epoch 482/2000, test Loss: 0.0180\n",
      "Epoch 483/2000, train Loss: 0.0114\n",
      "Epoch 483/2000, test Loss: 0.0178\n",
      "Epoch 484/2000, train Loss: 0.0112\n",
      "Epoch 484/2000, test Loss: 0.0176\n",
      "Epoch 485/2000, train Loss: 0.0113\n",
      "Epoch 485/2000, test Loss: 0.0175\n",
      "Epoch 486/2000, train Loss: 0.0112\n",
      "Epoch 486/2000, test Loss: 0.0176\n",
      "Epoch 487/2000, train Loss: 0.0112\n",
      "Epoch 487/2000, test Loss: 0.0176\n",
      "Epoch 488/2000, train Loss: 0.0112\n",
      "Epoch 488/2000, test Loss: 0.0175\n",
      "Epoch 489/2000, train Loss: 0.0110\n",
      "Epoch 489/2000, test Loss: 0.0174\n",
      "Epoch 490/2000, train Loss: 0.0109\n",
      "Epoch 490/2000, test Loss: 0.0172\n",
      "Epoch 491/2000, train Loss: 0.0110\n",
      "Epoch 491/2000, test Loss: 0.0171\n",
      "Epoch 492/2000, train Loss: 0.0109\n",
      "Epoch 492/2000, test Loss: 0.0172\n",
      "Epoch 493/2000, train Loss: 0.0108\n",
      "Epoch 493/2000, test Loss: 0.0172\n",
      "Epoch 494/2000, train Loss: 0.0108\n",
      "Epoch 494/2000, test Loss: 0.0169\n",
      "Epoch 495/2000, train Loss: 0.0107\n",
      "Epoch 495/2000, test Loss: 0.0169\n",
      "Epoch 496/2000, train Loss: 0.0106\n",
      "Epoch 496/2000, test Loss: 0.0168\n",
      "Epoch 497/2000, train Loss: 0.0106\n",
      "Epoch 497/2000, test Loss: 0.0170\n",
      "Epoch 498/2000, train Loss: 0.0106\n",
      "Epoch 498/2000, test Loss: 0.0167\n",
      "Epoch 499/2000, train Loss: 0.0105\n",
      "Epoch 499/2000, test Loss: 0.0167\n",
      "Epoch 500/2000, train Loss: 0.0105\n",
      "Epoch 500/2000, test Loss: 0.0166\n",
      "Epoch 501/2000, train Loss: 0.0105\n",
      "Epoch 501/2000, test Loss: 0.0168\n",
      "Epoch 502/2000, train Loss: 0.0104\n",
      "Epoch 502/2000, test Loss: 0.0164\n",
      "Epoch 503/2000, train Loss: 0.0104\n",
      "Epoch 503/2000, test Loss: 0.0162\n",
      "Epoch 504/2000, train Loss: 0.0103\n",
      "Epoch 504/2000, test Loss: 0.0169\n",
      "Epoch 505/2000, train Loss: 0.0103\n",
      "Epoch 505/2000, test Loss: 0.0162\n",
      "Epoch 506/2000, train Loss: 0.0102\n",
      "Epoch 506/2000, test Loss: 0.0162\n",
      "Epoch 507/2000, train Loss: 0.0101\n",
      "Epoch 507/2000, test Loss: 0.0162\n",
      "Epoch 508/2000, train Loss: 0.0101\n",
      "Epoch 508/2000, test Loss: 0.0162\n",
      "Epoch 509/2000, train Loss: 0.0101\n",
      "Epoch 509/2000, test Loss: 0.0161\n",
      "Epoch 510/2000, train Loss: 0.0101\n",
      "Epoch 510/2000, test Loss: 0.0162\n",
      "Epoch 511/2000, train Loss: 0.0100\n",
      "Epoch 511/2000, test Loss: 0.0159\n",
      "Epoch 512/2000, train Loss: 0.0100\n",
      "Epoch 512/2000, test Loss: 0.0160\n",
      "Epoch 513/2000, train Loss: 0.0099\n",
      "Epoch 513/2000, test Loss: 0.0157\n",
      "Epoch 514/2000, train Loss: 0.0098\n",
      "Epoch 514/2000, test Loss: 0.0159\n",
      "Epoch 515/2000, train Loss: 0.0098\n",
      "Epoch 515/2000, test Loss: 0.0157\n",
      "Epoch 516/2000, train Loss: 0.0098\n",
      "Epoch 516/2000, test Loss: 0.0157\n",
      "Epoch 517/2000, train Loss: 0.0097\n",
      "Epoch 517/2000, test Loss: 0.0157\n",
      "Epoch 518/2000, train Loss: 0.0097\n",
      "Epoch 518/2000, test Loss: 0.0154\n",
      "Epoch 519/2000, train Loss: 0.0096\n",
      "Epoch 519/2000, test Loss: 0.0157\n",
      "Epoch 520/2000, train Loss: 0.0096\n",
      "Epoch 520/2000, test Loss: 0.0154\n",
      "Epoch 521/2000, train Loss: 0.0096\n",
      "Epoch 521/2000, test Loss: 0.0153\n",
      "Epoch 522/2000, train Loss: 0.0095\n",
      "Epoch 522/2000, test Loss: 0.0153\n",
      "Epoch 523/2000, train Loss: 0.0095\n",
      "Epoch 523/2000, test Loss: 0.0156\n",
      "Epoch 524/2000, train Loss: 0.0095\n",
      "Epoch 524/2000, test Loss: 0.0151\n",
      "Epoch 525/2000, train Loss: 0.0094\n",
      "Epoch 525/2000, test Loss: 0.0154\n",
      "Epoch 526/2000, train Loss: 0.0093\n",
      "Epoch 526/2000, test Loss: 0.0149\n",
      "Epoch 527/2000, train Loss: 0.0094\n",
      "Epoch 527/2000, test Loss: 0.0151\n",
      "Epoch 528/2000, train Loss: 0.0093\n",
      "Epoch 528/2000, test Loss: 0.0152\n",
      "Epoch 529/2000, train Loss: 0.0093\n",
      "Epoch 529/2000, test Loss: 0.0149\n",
      "Epoch 530/2000, train Loss: 0.0093\n",
      "Epoch 530/2000, test Loss: 0.0149\n",
      "Epoch 531/2000, train Loss: 0.0092\n",
      "Epoch 531/2000, test Loss: 0.0148\n",
      "Epoch 532/2000, train Loss: 0.0092\n",
      "Epoch 532/2000, test Loss: 0.0151\n",
      "Epoch 533/2000, train Loss: 0.0091\n",
      "Epoch 533/2000, test Loss: 0.0148\n",
      "Epoch 534/2000, train Loss: 0.0091\n",
      "Epoch 534/2000, test Loss: 0.0147\n",
      "Epoch 535/2000, train Loss: 0.0091\n",
      "Epoch 535/2000, test Loss: 0.0145\n",
      "Epoch 536/2000, train Loss: 0.0090\n",
      "Epoch 536/2000, test Loss: 0.0148\n",
      "Epoch 537/2000, train Loss: 0.0090\n",
      "Epoch 537/2000, test Loss: 0.0145\n",
      "Epoch 538/2000, train Loss: 0.0089\n",
      "Epoch 538/2000, test Loss: 0.0146\n",
      "Epoch 539/2000, train Loss: 0.0089\n",
      "Epoch 539/2000, test Loss: 0.0145\n",
      "Epoch 540/2000, train Loss: 0.0089\n",
      "Epoch 540/2000, test Loss: 0.0145\n",
      "Epoch 541/2000, train Loss: 0.0089\n",
      "Epoch 541/2000, test Loss: 0.0144\n",
      "Epoch 542/2000, train Loss: 0.0088\n",
      "Epoch 542/2000, test Loss: 0.0145\n",
      "Epoch 543/2000, train Loss: 0.0087\n",
      "Epoch 543/2000, test Loss: 0.0143\n",
      "Epoch 544/2000, train Loss: 0.0087\n",
      "Epoch 544/2000, test Loss: 0.0141\n",
      "Epoch 545/2000, train Loss: 0.0087\n",
      "Epoch 545/2000, test Loss: 0.0142\n",
      "Epoch 546/2000, train Loss: 0.0087\n",
      "Epoch 546/2000, test Loss: 0.0144\n",
      "Epoch 547/2000, train Loss: 0.0086\n",
      "Epoch 547/2000, test Loss: 0.0141\n",
      "Epoch 548/2000, train Loss: 0.0086\n",
      "Epoch 548/2000, test Loss: 0.0140\n",
      "Epoch 549/2000, train Loss: 0.0085\n",
      "Epoch 549/2000, test Loss: 0.0140\n",
      "Epoch 550/2000, train Loss: 0.0086\n",
      "Epoch 550/2000, test Loss: 0.0138\n",
      "Epoch 551/2000, train Loss: 0.0085\n",
      "Epoch 551/2000, test Loss: 0.0142\n",
      "Epoch 552/2000, train Loss: 0.0085\n",
      "Epoch 552/2000, test Loss: 0.0139\n",
      "Epoch 553/2000, train Loss: 0.0084\n",
      "Epoch 553/2000, test Loss: 0.0137\n",
      "Epoch 554/2000, train Loss: 0.0085\n",
      "Epoch 554/2000, test Loss: 0.0137\n",
      "Epoch 555/2000, train Loss: 0.0083\n",
      "Epoch 555/2000, test Loss: 0.0139\n",
      "Epoch 556/2000, train Loss: 0.0083\n",
      "Epoch 556/2000, test Loss: 0.0139\n",
      "Epoch 557/2000, train Loss: 0.0083\n",
      "Epoch 557/2000, test Loss: 0.0139\n",
      "Epoch 558/2000, train Loss: 0.0083\n",
      "Epoch 558/2000, test Loss: 0.0135\n",
      "Epoch 559/2000, train Loss: 0.0082\n",
      "Epoch 559/2000, test Loss: 0.0135\n",
      "Epoch 560/2000, train Loss: 0.0082\n",
      "Epoch 560/2000, test Loss: 0.0137\n",
      "Epoch 561/2000, train Loss: 0.0082\n",
      "Epoch 561/2000, test Loss: 0.0139\n",
      "Epoch 562/2000, train Loss: 0.0081\n",
      "Epoch 562/2000, test Loss: 0.0134\n",
      "Epoch 563/2000, train Loss: 0.0082\n",
      "Epoch 563/2000, test Loss: 0.0133\n",
      "Epoch 564/2000, train Loss: 0.0081\n",
      "Epoch 564/2000, test Loss: 0.0133\n",
      "Epoch 565/2000, train Loss: 0.0080\n",
      "Epoch 565/2000, test Loss: 0.0133\n",
      "Epoch 566/2000, train Loss: 0.0080\n",
      "Epoch 566/2000, test Loss: 0.0134\n",
      "Epoch 567/2000, train Loss: 0.0080\n",
      "Epoch 567/2000, test Loss: 0.0134\n",
      "Epoch 568/2000, train Loss: 0.0080\n",
      "Epoch 568/2000, test Loss: 0.0131\n",
      "Epoch 569/2000, train Loss: 0.0080\n",
      "Epoch 569/2000, test Loss: 0.0131\n",
      "Epoch 570/2000, train Loss: 0.0079\n",
      "Epoch 570/2000, test Loss: 0.0132\n",
      "Epoch 571/2000, train Loss: 0.0079\n",
      "Epoch 571/2000, test Loss: 0.0132\n",
      "Epoch 572/2000, train Loss: 0.0079\n",
      "Epoch 572/2000, test Loss: 0.0130\n",
      "Epoch 573/2000, train Loss: 0.0078\n",
      "Epoch 573/2000, test Loss: 0.0131\n",
      "Epoch 574/2000, train Loss: 0.0078\n",
      "Epoch 574/2000, test Loss: 0.0132\n",
      "Epoch 575/2000, train Loss: 0.0078\n",
      "Epoch 575/2000, test Loss: 0.0128\n",
      "Epoch 576/2000, train Loss: 0.0078\n",
      "Epoch 576/2000, test Loss: 0.0128\n",
      "Epoch 577/2000, train Loss: 0.0077\n",
      "Epoch 577/2000, test Loss: 0.0130\n",
      "Epoch 578/2000, train Loss: 0.0078\n",
      "Epoch 578/2000, test Loss: 0.0131\n",
      "Epoch 579/2000, train Loss: 0.0076\n",
      "Epoch 579/2000, test Loss: 0.0128\n",
      "Epoch 580/2000, train Loss: 0.0076\n",
      "Epoch 580/2000, test Loss: 0.0127\n",
      "Epoch 581/2000, train Loss: 0.0076\n",
      "Epoch 581/2000, test Loss: 0.0128\n",
      "Epoch 582/2000, train Loss: 0.0076\n",
      "Epoch 582/2000, test Loss: 0.0124\n",
      "Epoch 583/2000, train Loss: 0.0076\n",
      "Epoch 583/2000, test Loss: 0.0126\n",
      "Epoch 584/2000, train Loss: 0.0076\n",
      "Epoch 584/2000, test Loss: 0.0126\n",
      "Epoch 585/2000, train Loss: 0.0075\n",
      "Epoch 585/2000, test Loss: 0.0126\n",
      "Epoch 586/2000, train Loss: 0.0074\n",
      "Epoch 586/2000, test Loss: 0.0125\n",
      "Epoch 587/2000, train Loss: 0.0074\n",
      "Epoch 587/2000, test Loss: 0.0125\n",
      "Epoch 588/2000, train Loss: 0.0074\n",
      "Epoch 588/2000, test Loss: 0.0125\n",
      "Epoch 589/2000, train Loss: 0.0074\n",
      "Epoch 589/2000, test Loss: 0.0124\n",
      "Epoch 590/2000, train Loss: 0.0075\n",
      "Epoch 590/2000, test Loss: 0.0126\n",
      "Epoch 591/2000, train Loss: 0.0074\n",
      "Epoch 591/2000, test Loss: 0.0123\n",
      "Epoch 592/2000, train Loss: 0.0074\n",
      "Epoch 592/2000, test Loss: 0.0122\n",
      "Epoch 593/2000, train Loss: 0.0073\n",
      "Epoch 593/2000, test Loss: 0.0123\n",
      "Epoch 594/2000, train Loss: 0.0073\n",
      "Epoch 594/2000, test Loss: 0.0122\n",
      "Epoch 595/2000, train Loss: 0.0073\n",
      "Epoch 595/2000, test Loss: 0.0122\n",
      "Epoch 596/2000, train Loss: 0.0072\n",
      "Epoch 596/2000, test Loss: 0.0122\n",
      "Epoch 597/2000, train Loss: 0.0072\n",
      "Epoch 597/2000, test Loss: 0.0120\n",
      "Epoch 598/2000, train Loss: 0.0072\n",
      "Epoch 598/2000, test Loss: 0.0122\n",
      "Epoch 599/2000, train Loss: 0.0072\n",
      "Epoch 599/2000, test Loss: 0.0123\n",
      "Epoch 600/2000, train Loss: 0.0072\n",
      "Epoch 600/2000, test Loss: 0.0120\n",
      "Epoch 601/2000, train Loss: 0.0071\n",
      "Epoch 601/2000, test Loss: 0.0119\n",
      "Epoch 602/2000, train Loss: 0.0071\n",
      "Epoch 602/2000, test Loss: 0.0120\n",
      "Epoch 603/2000, train Loss: 0.0071\n",
      "Epoch 603/2000, test Loss: 0.0122\n",
      "Epoch 604/2000, train Loss: 0.0071\n",
      "Epoch 604/2000, test Loss: 0.0120\n",
      "Epoch 605/2000, train Loss: 0.0071\n",
      "Epoch 605/2000, test Loss: 0.0118\n",
      "Epoch 606/2000, train Loss: 0.0071\n",
      "Epoch 606/2000, test Loss: 0.0117\n",
      "Epoch 607/2000, train Loss: 0.0071\n",
      "Epoch 607/2000, test Loss: 0.0124\n",
      "Epoch 608/2000, train Loss: 0.0070\n",
      "Epoch 608/2000, test Loss: 0.0119\n",
      "Epoch 609/2000, train Loss: 0.0070\n",
      "Epoch 609/2000, test Loss: 0.0117\n",
      "Epoch 610/2000, train Loss: 0.0069\n",
      "Epoch 610/2000, test Loss: 0.0117\n",
      "Epoch 611/2000, train Loss: 0.0069\n",
      "Epoch 611/2000, test Loss: 0.0116\n",
      "Epoch 612/2000, train Loss: 0.0069\n",
      "Epoch 612/2000, test Loss: 0.0120\n",
      "Epoch 613/2000, train Loss: 0.0069\n",
      "Epoch 613/2000, test Loss: 0.0116\n",
      "Epoch 614/2000, train Loss: 0.0069\n",
      "Epoch 614/2000, test Loss: 0.0116\n",
      "Epoch 615/2000, train Loss: 0.0068\n",
      "Epoch 615/2000, test Loss: 0.0115\n",
      "Epoch 616/2000, train Loss: 0.0068\n",
      "Epoch 616/2000, test Loss: 0.0114\n",
      "Epoch 617/2000, train Loss: 0.0068\n",
      "Epoch 617/2000, test Loss: 0.0116\n",
      "Epoch 618/2000, train Loss: 0.0067\n",
      "Epoch 618/2000, test Loss: 0.0115\n",
      "Epoch 619/2000, train Loss: 0.0068\n",
      "Epoch 619/2000, test Loss: 0.0115\n",
      "Epoch 620/2000, train Loss: 0.0068\n",
      "Epoch 620/2000, test Loss: 0.0115\n",
      "Epoch 621/2000, train Loss: 0.0067\n",
      "Epoch 621/2000, test Loss: 0.0114\n",
      "Epoch 622/2000, train Loss: 0.0067\n",
      "Epoch 622/2000, test Loss: 0.0114\n",
      "Epoch 623/2000, train Loss: 0.0067\n",
      "Epoch 623/2000, test Loss: 0.0112\n",
      "Epoch 624/2000, train Loss: 0.0067\n",
      "Epoch 624/2000, test Loss: 0.0114\n",
      "Epoch 625/2000, train Loss: 0.0066\n",
      "Epoch 625/2000, test Loss: 0.0112\n",
      "Epoch 626/2000, train Loss: 0.0066\n",
      "Epoch 626/2000, test Loss: 0.0114\n",
      "Epoch 627/2000, train Loss: 0.0066\n",
      "Epoch 627/2000, test Loss: 0.0113\n",
      "Epoch 628/2000, train Loss: 0.0066\n",
      "Epoch 628/2000, test Loss: 0.0112\n",
      "Epoch 629/2000, train Loss: 0.0065\n",
      "Epoch 629/2000, test Loss: 0.0113\n",
      "Epoch 630/2000, train Loss: 0.0066\n",
      "Epoch 630/2000, test Loss: 0.0111\n",
      "Epoch 631/2000, train Loss: 0.0065\n",
      "Epoch 631/2000, test Loss: 0.0114\n",
      "Epoch 632/2000, train Loss: 0.0065\n",
      "Epoch 632/2000, test Loss: 0.0111\n",
      "Epoch 633/2000, train Loss: 0.0064\n",
      "Epoch 633/2000, test Loss: 0.0110\n",
      "Epoch 634/2000, train Loss: 0.0064\n",
      "Epoch 634/2000, test Loss: 0.0111\n",
      "Epoch 635/2000, train Loss: 0.0064\n",
      "Epoch 635/2000, test Loss: 0.0110\n",
      "Epoch 636/2000, train Loss: 0.0065\n",
      "Epoch 636/2000, test Loss: 0.0109\n",
      "Epoch 637/2000, train Loss: 0.0064\n",
      "Epoch 637/2000, test Loss: 0.0111\n",
      "Epoch 638/2000, train Loss: 0.0064\n",
      "Epoch 638/2000, test Loss: 0.0108\n",
      "Epoch 639/2000, train Loss: 0.0063\n",
      "Epoch 639/2000, test Loss: 0.0108\n",
      "Epoch 640/2000, train Loss: 0.0064\n",
      "Epoch 640/2000, test Loss: 0.0107\n",
      "Epoch 641/2000, train Loss: 0.0063\n",
      "Epoch 641/2000, test Loss: 0.0108\n",
      "Epoch 642/2000, train Loss: 0.0063\n",
      "Epoch 642/2000, test Loss: 0.0107\n",
      "Epoch 643/2000, train Loss: 0.0063\n",
      "Epoch 643/2000, test Loss: 0.0108\n",
      "Epoch 644/2000, train Loss: 0.0064\n",
      "Epoch 644/2000, test Loss: 0.0108\n",
      "Epoch 645/2000, train Loss: 0.0063\n",
      "Epoch 645/2000, test Loss: 0.0112\n",
      "Epoch 646/2000, train Loss: 0.0063\n",
      "Epoch 646/2000, test Loss: 0.0107\n",
      "Epoch 647/2000, train Loss: 0.0063\n",
      "Epoch 647/2000, test Loss: 0.0105\n",
      "Epoch 648/2000, train Loss: 0.0063\n",
      "Epoch 648/2000, test Loss: 0.0107\n",
      "Epoch 649/2000, train Loss: 0.0062\n",
      "Epoch 649/2000, test Loss: 0.0105\n",
      "Epoch 650/2000, train Loss: 0.0061\n",
      "Epoch 650/2000, test Loss: 0.0105\n",
      "Epoch 651/2000, train Loss: 0.0061\n",
      "Epoch 651/2000, test Loss: 0.0107\n",
      "Epoch 652/2000, train Loss: 0.0061\n",
      "Epoch 652/2000, test Loss: 0.0104\n",
      "Epoch 653/2000, train Loss: 0.0061\n",
      "Epoch 653/2000, test Loss: 0.0107\n",
      "Epoch 654/2000, train Loss: 0.0062\n",
      "Epoch 654/2000, test Loss: 0.0105\n",
      "Epoch 655/2000, train Loss: 0.0061\n",
      "Epoch 655/2000, test Loss: 0.0103\n",
      "Epoch 656/2000, train Loss: 0.0061\n",
      "Epoch 656/2000, test Loss: 0.0104\n",
      "Epoch 657/2000, train Loss: 0.0061\n",
      "Epoch 657/2000, test Loss: 0.0105\n",
      "Epoch 658/2000, train Loss: 0.0060\n",
      "Epoch 658/2000, test Loss: 0.0103\n",
      "Epoch 659/2000, train Loss: 0.0060\n",
      "Epoch 659/2000, test Loss: 0.0106\n",
      "Epoch 660/2000, train Loss: 0.0061\n",
      "Epoch 660/2000, test Loss: 0.0103\n",
      "Epoch 661/2000, train Loss: 0.0061\n",
      "Epoch 661/2000, test Loss: 0.0104\n",
      "Epoch 662/2000, train Loss: 0.0060\n",
      "Epoch 662/2000, test Loss: 0.0103\n",
      "Epoch 663/2000, train Loss: 0.0060\n",
      "Epoch 663/2000, test Loss: 0.0102\n",
      "Epoch 664/2000, train Loss: 0.0060\n",
      "Epoch 664/2000, test Loss: 0.0103\n",
      "Epoch 665/2000, train Loss: 0.0060\n",
      "Epoch 665/2000, test Loss: 0.0103\n",
      "Epoch 666/2000, train Loss: 0.0060\n",
      "Epoch 666/2000, test Loss: 0.0102\n",
      "Epoch 667/2000, train Loss: 0.0059\n",
      "Epoch 667/2000, test Loss: 0.0102\n",
      "Epoch 668/2000, train Loss: 0.0059\n",
      "Epoch 668/2000, test Loss: 0.0102\n",
      "Epoch 669/2000, train Loss: 0.0059\n",
      "Epoch 669/2000, test Loss: 0.0100\n",
      "Epoch 670/2000, train Loss: 0.0059\n",
      "Epoch 670/2000, test Loss: 0.0101\n",
      "Epoch 671/2000, train Loss: 0.0059\n",
      "Epoch 671/2000, test Loss: 0.0103\n",
      "Epoch 672/2000, train Loss: 0.0058\n",
      "Epoch 672/2000, test Loss: 0.0100\n",
      "Epoch 673/2000, train Loss: 0.0058\n",
      "Epoch 673/2000, test Loss: 0.0100\n",
      "Epoch 674/2000, train Loss: 0.0058\n",
      "Epoch 674/2000, test Loss: 0.0099\n",
      "Epoch 675/2000, train Loss: 0.0058\n",
      "Epoch 675/2000, test Loss: 0.0099\n",
      "Epoch 676/2000, train Loss: 0.0058\n",
      "Epoch 676/2000, test Loss: 0.0099\n",
      "Epoch 677/2000, train Loss: 0.0058\n",
      "Epoch 677/2000, test Loss: 0.0100\n",
      "Epoch 678/2000, train Loss: 0.0058\n",
      "Epoch 678/2000, test Loss: 0.0099\n",
      "Epoch 679/2000, train Loss: 0.0057\n",
      "Epoch 679/2000, test Loss: 0.0099\n",
      "Epoch 680/2000, train Loss: 0.0057\n",
      "Epoch 680/2000, test Loss: 0.0099\n",
      "Epoch 681/2000, train Loss: 0.0057\n",
      "Epoch 681/2000, test Loss: 0.0098\n",
      "Epoch 682/2000, train Loss: 0.0057\n",
      "Epoch 682/2000, test Loss: 0.0099\n",
      "Epoch 683/2000, train Loss: 0.0057\n",
      "Epoch 683/2000, test Loss: 0.0097\n",
      "Epoch 684/2000, train Loss: 0.0057\n",
      "Epoch 684/2000, test Loss: 0.0100\n",
      "Epoch 685/2000, train Loss: 0.0057\n",
      "Epoch 685/2000, test Loss: 0.0099\n",
      "Epoch 686/2000, train Loss: 0.0057\n",
      "Epoch 686/2000, test Loss: 0.0098\n",
      "Epoch 687/2000, train Loss: 0.0056\n",
      "Epoch 687/2000, test Loss: 0.0096\n",
      "Epoch 688/2000, train Loss: 0.0057\n",
      "Epoch 688/2000, test Loss: 0.0096\n",
      "Epoch 689/2000, train Loss: 0.0056\n",
      "Epoch 689/2000, test Loss: 0.0097\n",
      "Epoch 690/2000, train Loss: 0.0057\n",
      "Epoch 690/2000, test Loss: 0.0097\n",
      "Epoch 691/2000, train Loss: 0.0056\n",
      "Epoch 691/2000, test Loss: 0.0094\n",
      "Epoch 692/2000, train Loss: 0.0056\n",
      "Epoch 692/2000, test Loss: 0.0095\n",
      "Epoch 693/2000, train Loss: 0.0056\n",
      "Epoch 693/2000, test Loss: 0.0095\n",
      "Epoch 694/2000, train Loss: 0.0056\n",
      "Epoch 694/2000, test Loss: 0.0095\n",
      "Epoch 695/2000, train Loss: 0.0056\n",
      "Epoch 695/2000, test Loss: 0.0095\n",
      "Epoch 696/2000, train Loss: 0.0056\n",
      "Epoch 696/2000, test Loss: 0.0094\n",
      "Epoch 697/2000, train Loss: 0.0055\n",
      "Epoch 697/2000, test Loss: 0.0093\n",
      "Epoch 698/2000, train Loss: 0.0055\n",
      "Epoch 698/2000, test Loss: 0.0094\n",
      "Epoch 699/2000, train Loss: 0.0055\n",
      "Epoch 699/2000, test Loss: 0.0093\n",
      "Epoch 700/2000, train Loss: 0.0055\n",
      "Epoch 700/2000, test Loss: 0.0097\n",
      "Epoch 701/2000, train Loss: 0.0055\n",
      "Epoch 701/2000, test Loss: 0.0094\n",
      "Epoch 702/2000, train Loss: 0.0055\n",
      "Epoch 702/2000, test Loss: 0.0092\n",
      "Epoch 703/2000, train Loss: 0.0054\n",
      "Epoch 703/2000, test Loss: 0.0093\n",
      "Epoch 704/2000, train Loss: 0.0055\n",
      "Epoch 704/2000, test Loss: 0.0094\n",
      "Epoch 705/2000, train Loss: 0.0055\n",
      "Epoch 705/2000, test Loss: 0.0094\n",
      "Epoch 706/2000, train Loss: 0.0054\n",
      "Epoch 706/2000, test Loss: 0.0093\n",
      "Epoch 707/2000, train Loss: 0.0054\n",
      "Epoch 707/2000, test Loss: 0.0092\n",
      "Epoch 708/2000, train Loss: 0.0054\n",
      "Epoch 708/2000, test Loss: 0.0091\n",
      "Epoch 709/2000, train Loss: 0.0054\n",
      "Epoch 709/2000, test Loss: 0.0091\n",
      "Epoch 710/2000, train Loss: 0.0054\n",
      "Epoch 710/2000, test Loss: 0.0092\n",
      "Epoch 711/2000, train Loss: 0.0053\n",
      "Epoch 711/2000, test Loss: 0.0091\n",
      "Epoch 712/2000, train Loss: 0.0053\n",
      "Epoch 712/2000, test Loss: 0.0091\n",
      "Epoch 713/2000, train Loss: 0.0053\n",
      "Epoch 713/2000, test Loss: 0.0090\n",
      "Epoch 714/2000, train Loss: 0.0053\n",
      "Epoch 714/2000, test Loss: 0.0090\n",
      "Epoch 715/2000, train Loss: 0.0053\n",
      "Epoch 715/2000, test Loss: 0.0092\n",
      "Epoch 716/2000, train Loss: 0.0053\n",
      "Epoch 716/2000, test Loss: 0.0091\n",
      "Epoch 717/2000, train Loss: 0.0053\n",
      "Epoch 717/2000, test Loss: 0.0089\n",
      "Epoch 718/2000, train Loss: 0.0052\n",
      "Epoch 718/2000, test Loss: 0.0090\n",
      "Epoch 719/2000, train Loss: 0.0053\n",
      "Epoch 719/2000, test Loss: 0.0088\n",
      "Epoch 720/2000, train Loss: 0.0053\n",
      "Epoch 720/2000, test Loss: 0.0088\n",
      "Epoch 721/2000, train Loss: 0.0053\n",
      "Epoch 721/2000, test Loss: 0.0088\n",
      "Epoch 722/2000, train Loss: 0.0053\n",
      "Epoch 722/2000, test Loss: 0.0089\n",
      "Epoch 723/2000, train Loss: 0.0052\n",
      "Epoch 723/2000, test Loss: 0.0090\n",
      "Epoch 724/2000, train Loss: 0.0052\n",
      "Epoch 724/2000, test Loss: 0.0090\n",
      "Epoch 725/2000, train Loss: 0.0052\n",
      "Epoch 725/2000, test Loss: 0.0090\n",
      "Epoch 726/2000, train Loss: 0.0052\n",
      "Epoch 726/2000, test Loss: 0.0087\n",
      "Epoch 727/2000, train Loss: 0.0052\n",
      "Epoch 727/2000, test Loss: 0.0087\n",
      "Epoch 728/2000, train Loss: 0.0052\n",
      "Epoch 728/2000, test Loss: 0.0087\n",
      "Epoch 729/2000, train Loss: 0.0052\n",
      "Epoch 729/2000, test Loss: 0.0089\n",
      "Epoch 730/2000, train Loss: 0.0052\n",
      "Epoch 730/2000, test Loss: 0.0085\n",
      "Epoch 731/2000, train Loss: 0.0051\n",
      "Epoch 731/2000, test Loss: 0.0089\n",
      "Epoch 732/2000, train Loss: 0.0051\n",
      "Epoch 732/2000, test Loss: 0.0088\n",
      "Epoch 733/2000, train Loss: 0.0051\n",
      "Epoch 733/2000, test Loss: 0.0090\n",
      "Epoch 734/2000, train Loss: 0.0052\n",
      "Epoch 734/2000, test Loss: 0.0086\n",
      "Epoch 735/2000, train Loss: 0.0052\n",
      "Epoch 735/2000, test Loss: 0.0086\n",
      "Epoch 736/2000, train Loss: 0.0051\n",
      "Epoch 736/2000, test Loss: 0.0087\n",
      "Epoch 737/2000, train Loss: 0.0051\n",
      "Epoch 737/2000, test Loss: 0.0085\n",
      "Epoch 738/2000, train Loss: 0.0051\n",
      "Epoch 738/2000, test Loss: 0.0085\n",
      "Epoch 739/2000, train Loss: 0.0051\n",
      "Epoch 739/2000, test Loss: 0.0087\n",
      "Epoch 740/2000, train Loss: 0.0051\n",
      "Epoch 740/2000, test Loss: 0.0086\n",
      "Epoch 741/2000, train Loss: 0.0050\n",
      "Epoch 741/2000, test Loss: 0.0084\n",
      "Epoch 742/2000, train Loss: 0.0050\n",
      "Epoch 742/2000, test Loss: 0.0084\n",
      "Epoch 743/2000, train Loss: 0.0051\n",
      "Epoch 743/2000, test Loss: 0.0083\n",
      "Epoch 744/2000, train Loss: 0.0050\n",
      "Epoch 744/2000, test Loss: 0.0086\n",
      "Epoch 745/2000, train Loss: 0.0050\n",
      "Epoch 745/2000, test Loss: 0.0084\n",
      "Epoch 746/2000, train Loss: 0.0050\n",
      "Epoch 746/2000, test Loss: 0.0084\n",
      "Epoch 747/2000, train Loss: 0.0050\n",
      "Epoch 747/2000, test Loss: 0.0084\n",
      "Epoch 748/2000, train Loss: 0.0050\n",
      "Epoch 748/2000, test Loss: 0.0083\n",
      "Epoch 749/2000, train Loss: 0.0049\n",
      "Epoch 749/2000, test Loss: 0.0084\n",
      "Epoch 750/2000, train Loss: 0.0050\n",
      "Epoch 750/2000, test Loss: 0.0085\n",
      "Epoch 751/2000, train Loss: 0.0050\n",
      "Epoch 751/2000, test Loss: 0.0083\n",
      "Epoch 752/2000, train Loss: 0.0049\n",
      "Epoch 752/2000, test Loss: 0.0081\n",
      "Epoch 753/2000, train Loss: 0.0049\n",
      "Epoch 753/2000, test Loss: 0.0083\n",
      "Epoch 754/2000, train Loss: 0.0049\n",
      "Epoch 754/2000, test Loss: 0.0081\n",
      "Epoch 755/2000, train Loss: 0.0051\n",
      "Epoch 755/2000, test Loss: 0.0081\n",
      "Epoch 756/2000, train Loss: 0.0049\n",
      "Epoch 756/2000, test Loss: 0.0082\n",
      "Epoch 757/2000, train Loss: 0.0049\n",
      "Epoch 757/2000, test Loss: 0.0084\n",
      "Epoch 758/2000, train Loss: 0.0049\n",
      "Epoch 758/2000, test Loss: 0.0082\n",
      "Epoch 759/2000, train Loss: 0.0049\n",
      "Epoch 759/2000, test Loss: 0.0082\n",
      "Epoch 760/2000, train Loss: 0.0049\n",
      "Epoch 760/2000, test Loss: 0.0082\n",
      "Epoch 761/2000, train Loss: 0.0049\n",
      "Epoch 761/2000, test Loss: 0.0081\n",
      "Epoch 762/2000, train Loss: 0.0049\n",
      "Epoch 762/2000, test Loss: 0.0080\n",
      "Epoch 763/2000, train Loss: 0.0049\n",
      "Epoch 763/2000, test Loss: 0.0083\n",
      "Epoch 764/2000, train Loss: 0.0049\n",
      "Epoch 764/2000, test Loss: 0.0082\n",
      "Epoch 765/2000, train Loss: 0.0048\n",
      "Epoch 765/2000, test Loss: 0.0082\n",
      "Epoch 766/2000, train Loss: 0.0049\n",
      "Epoch 766/2000, test Loss: 0.0083\n",
      "Epoch 767/2000, train Loss: 0.0049\n",
      "Epoch 767/2000, test Loss: 0.0080\n",
      "Epoch 768/2000, train Loss: 0.0048\n",
      "Epoch 768/2000, test Loss: 0.0081\n",
      "Epoch 769/2000, train Loss: 0.0048\n",
      "Epoch 769/2000, test Loss: 0.0080\n",
      "Epoch 770/2000, train Loss: 0.0048\n",
      "Epoch 770/2000, test Loss: 0.0079\n",
      "Epoch 771/2000, train Loss: 0.0048\n",
      "Epoch 771/2000, test Loss: 0.0078\n",
      "Epoch 772/2000, train Loss: 0.0048\n",
      "Epoch 772/2000, test Loss: 0.0077\n",
      "Epoch 773/2000, train Loss: 0.0047\n",
      "Epoch 773/2000, test Loss: 0.0080\n",
      "Epoch 774/2000, train Loss: 0.0047\n",
      "Epoch 774/2000, test Loss: 0.0080\n",
      "Epoch 775/2000, train Loss: 0.0047\n",
      "Epoch 775/2000, test Loss: 0.0078\n",
      "Epoch 776/2000, train Loss: 0.0047\n",
      "Epoch 776/2000, test Loss: 0.0078\n",
      "Epoch 777/2000, train Loss: 0.0047\n",
      "Epoch 777/2000, test Loss: 0.0078\n",
      "Epoch 778/2000, train Loss: 0.0047\n",
      "Epoch 778/2000, test Loss: 0.0078\n",
      "Epoch 779/2000, train Loss: 0.0047\n",
      "Epoch 779/2000, test Loss: 0.0080\n",
      "Epoch 780/2000, train Loss: 0.0047\n",
      "Epoch 780/2000, test Loss: 0.0079\n",
      "Epoch 781/2000, train Loss: 0.0047\n",
      "Epoch 781/2000, test Loss: 0.0077\n",
      "Epoch 782/2000, train Loss: 0.0047\n",
      "Epoch 782/2000, test Loss: 0.0079\n",
      "Epoch 783/2000, train Loss: 0.0047\n",
      "Epoch 783/2000, test Loss: 0.0078\n",
      "Epoch 784/2000, train Loss: 0.0047\n",
      "Epoch 784/2000, test Loss: 0.0077\n",
      "Epoch 785/2000, train Loss: 0.0046\n",
      "Epoch 785/2000, test Loss: 0.0076\n",
      "Epoch 786/2000, train Loss: 0.0047\n",
      "Epoch 786/2000, test Loss: 0.0078\n",
      "Epoch 787/2000, train Loss: 0.0047\n",
      "Epoch 787/2000, test Loss: 0.0076\n",
      "Epoch 788/2000, train Loss: 0.0046\n",
      "Epoch 788/2000, test Loss: 0.0077\n",
      "Epoch 789/2000, train Loss: 0.0046\n",
      "Epoch 789/2000, test Loss: 0.0075\n",
      "Epoch 790/2000, train Loss: 0.0046\n",
      "Epoch 790/2000, test Loss: 0.0075\n",
      "Epoch 791/2000, train Loss: 0.0046\n",
      "Epoch 791/2000, test Loss: 0.0075\n",
      "Epoch 792/2000, train Loss: 0.0046\n",
      "Epoch 792/2000, test Loss: 0.0075\n",
      "Epoch 793/2000, train Loss: 0.0046\n",
      "Epoch 793/2000, test Loss: 0.0075\n",
      "Epoch 794/2000, train Loss: 0.0046\n",
      "Epoch 794/2000, test Loss: 0.0075\n",
      "Epoch 795/2000, train Loss: 0.0046\n",
      "Epoch 795/2000, test Loss: 0.0076\n",
      "Epoch 796/2000, train Loss: 0.0045\n",
      "Epoch 796/2000, test Loss: 0.0075\n",
      "Epoch 797/2000, train Loss: 0.0046\n",
      "Epoch 797/2000, test Loss: 0.0075\n",
      "Epoch 798/2000, train Loss: 0.0045\n",
      "Epoch 798/2000, test Loss: 0.0074\n",
      "Epoch 799/2000, train Loss: 0.0046\n",
      "Epoch 799/2000, test Loss: 0.0075\n",
      "Epoch 800/2000, train Loss: 0.0045\n",
      "Epoch 800/2000, test Loss: 0.0074\n",
      "Epoch 801/2000, train Loss: 0.0045\n",
      "Epoch 801/2000, test Loss: 0.0074\n",
      "Epoch 802/2000, train Loss: 0.0045\n",
      "Epoch 802/2000, test Loss: 0.0073\n",
      "Epoch 803/2000, train Loss: 0.0045\n",
      "Epoch 803/2000, test Loss: 0.0073\n",
      "Epoch 804/2000, train Loss: 0.0046\n",
      "Epoch 804/2000, test Loss: 0.0073\n",
      "Epoch 805/2000, train Loss: 0.0045\n",
      "Epoch 805/2000, test Loss: 0.0072\n",
      "Epoch 806/2000, train Loss: 0.0045\n",
      "Epoch 806/2000, test Loss: 0.0074\n",
      "Epoch 807/2000, train Loss: 0.0045\n",
      "Epoch 807/2000, test Loss: 0.0073\n",
      "Epoch 808/2000, train Loss: 0.0045\n",
      "Epoch 808/2000, test Loss: 0.0073\n",
      "Epoch 809/2000, train Loss: 0.0045\n",
      "Epoch 809/2000, test Loss: 0.0072\n",
      "Epoch 810/2000, train Loss: 0.0045\n",
      "Epoch 810/2000, test Loss: 0.0074\n",
      "Epoch 811/2000, train Loss: 0.0044\n",
      "Epoch 811/2000, test Loss: 0.0073\n",
      "Epoch 812/2000, train Loss: 0.0044\n",
      "Epoch 812/2000, test Loss: 0.0072\n",
      "Epoch 813/2000, train Loss: 0.0044\n",
      "Epoch 813/2000, test Loss: 0.0072\n",
      "Epoch 814/2000, train Loss: 0.0045\n",
      "Epoch 814/2000, test Loss: 0.0072\n",
      "Epoch 815/2000, train Loss: 0.0044\n",
      "Epoch 815/2000, test Loss: 0.0071\n",
      "Epoch 816/2000, train Loss: 0.0045\n",
      "Epoch 816/2000, test Loss: 0.0071\n",
      "Epoch 817/2000, train Loss: 0.0044\n",
      "Epoch 817/2000, test Loss: 0.0072\n",
      "Epoch 818/2000, train Loss: 0.0044\n",
      "Epoch 818/2000, test Loss: 0.0073\n",
      "Epoch 819/2000, train Loss: 0.0044\n",
      "Epoch 819/2000, test Loss: 0.0071\n",
      "Epoch 820/2000, train Loss: 0.0044\n",
      "Epoch 820/2000, test Loss: 0.0071\n",
      "Epoch 821/2000, train Loss: 0.0044\n",
      "Epoch 821/2000, test Loss: 0.0072\n",
      "Epoch 822/2000, train Loss: 0.0043\n",
      "Epoch 822/2000, test Loss: 0.0070\n",
      "Epoch 823/2000, train Loss: 0.0044\n",
      "Epoch 823/2000, test Loss: 0.0070\n",
      "Epoch 824/2000, train Loss: 0.0044\n",
      "Epoch 824/2000, test Loss: 0.0070\n",
      "Epoch 825/2000, train Loss: 0.0044\n",
      "Epoch 825/2000, test Loss: 0.0070\n",
      "Epoch 826/2000, train Loss: 0.0044\n",
      "Epoch 826/2000, test Loss: 0.0070\n",
      "Epoch 827/2000, train Loss: 0.0043\n",
      "Epoch 827/2000, test Loss: 0.0072\n",
      "Epoch 828/2000, train Loss: 0.0044\n",
      "Epoch 828/2000, test Loss: 0.0071\n",
      "Epoch 829/2000, train Loss: 0.0043\n",
      "Epoch 829/2000, test Loss: 0.0070\n",
      "Epoch 830/2000, train Loss: 0.0043\n",
      "Epoch 830/2000, test Loss: 0.0070\n",
      "Epoch 831/2000, train Loss: 0.0043\n",
      "Epoch 831/2000, test Loss: 0.0071\n",
      "Epoch 832/2000, train Loss: 0.0043\n",
      "Epoch 832/2000, test Loss: 0.0069\n",
      "Epoch 833/2000, train Loss: 0.0043\n",
      "Epoch 833/2000, test Loss: 0.0069\n",
      "Epoch 834/2000, train Loss: 0.0043\n",
      "Epoch 834/2000, test Loss: 0.0069\n",
      "Epoch 835/2000, train Loss: 0.0043\n",
      "Epoch 835/2000, test Loss: 0.0070\n",
      "Epoch 836/2000, train Loss: 0.0043\n",
      "Epoch 836/2000, test Loss: 0.0072\n",
      "Epoch 837/2000, train Loss: 0.0042\n",
      "Epoch 837/2000, test Loss: 0.0068\n",
      "Epoch 838/2000, train Loss: 0.0043\n",
      "Epoch 838/2000, test Loss: 0.0068\n",
      "Epoch 839/2000, train Loss: 0.0043\n",
      "Epoch 839/2000, test Loss: 0.0069\n",
      "Epoch 840/2000, train Loss: 0.0042\n",
      "Epoch 840/2000, test Loss: 0.0069\n",
      "Epoch 841/2000, train Loss: 0.0042\n",
      "Epoch 841/2000, test Loss: 0.0068\n",
      "Epoch 842/2000, train Loss: 0.0042\n",
      "Epoch 842/2000, test Loss: 0.0068\n",
      "Epoch 843/2000, train Loss: 0.0042\n",
      "Epoch 843/2000, test Loss: 0.0070\n",
      "Epoch 844/2000, train Loss: 0.0043\n",
      "Epoch 844/2000, test Loss: 0.0070\n",
      "Epoch 845/2000, train Loss: 0.0042\n",
      "Epoch 845/2000, test Loss: 0.0069\n",
      "Epoch 846/2000, train Loss: 0.0042\n",
      "Epoch 846/2000, test Loss: 0.0068\n",
      "Epoch 847/2000, train Loss: 0.0042\n",
      "Epoch 847/2000, test Loss: 0.0067\n",
      "Epoch 848/2000, train Loss: 0.0042\n",
      "Epoch 848/2000, test Loss: 0.0068\n",
      "Epoch 849/2000, train Loss: 0.0042\n",
      "Epoch 849/2000, test Loss: 0.0067\n",
      "Epoch 850/2000, train Loss: 0.0041\n",
      "Epoch 850/2000, test Loss: 0.0068\n",
      "Epoch 851/2000, train Loss: 0.0041\n",
      "Epoch 851/2000, test Loss: 0.0067\n",
      "Epoch 852/2000, train Loss: 0.0042\n",
      "Epoch 852/2000, test Loss: 0.0069\n",
      "Epoch 853/2000, train Loss: 0.0042\n",
      "Epoch 853/2000, test Loss: 0.0069\n",
      "Epoch 854/2000, train Loss: 0.0041\n",
      "Epoch 854/2000, test Loss: 0.0067\n",
      "Epoch 855/2000, train Loss: 0.0041\n",
      "Epoch 855/2000, test Loss: 0.0066\n",
      "Epoch 856/2000, train Loss: 0.0041\n",
      "Epoch 856/2000, test Loss: 0.0066\n",
      "Epoch 857/2000, train Loss: 0.0042\n",
      "Epoch 857/2000, test Loss: 0.0067\n",
      "Epoch 858/2000, train Loss: 0.0041\n",
      "Epoch 858/2000, test Loss: 0.0066\n",
      "Epoch 859/2000, train Loss: 0.0041\n",
      "Epoch 859/2000, test Loss: 0.0067\n",
      "Epoch 860/2000, train Loss: 0.0041\n",
      "Epoch 860/2000, test Loss: 0.0067\n",
      "Epoch 861/2000, train Loss: 0.0041\n",
      "Epoch 861/2000, test Loss: 0.0066\n",
      "Epoch 862/2000, train Loss: 0.0041\n",
      "Epoch 862/2000, test Loss: 0.0066\n",
      "Epoch 863/2000, train Loss: 0.0041\n",
      "Epoch 863/2000, test Loss: 0.0065\n",
      "Epoch 864/2000, train Loss: 0.0041\n",
      "Epoch 864/2000, test Loss: 0.0065\n",
      "Epoch 865/2000, train Loss: 0.0041\n",
      "Epoch 865/2000, test Loss: 0.0066\n",
      "Epoch 866/2000, train Loss: 0.0040\n",
      "Epoch 866/2000, test Loss: 0.0065\n",
      "Epoch 867/2000, train Loss: 0.0041\n",
      "Epoch 867/2000, test Loss: 0.0065\n",
      "Epoch 868/2000, train Loss: 0.0041\n",
      "Epoch 868/2000, test Loss: 0.0065\n",
      "Epoch 869/2000, train Loss: 0.0040\n",
      "Epoch 869/2000, test Loss: 0.0064\n",
      "Epoch 870/2000, train Loss: 0.0040\n",
      "Epoch 870/2000, test Loss: 0.0066\n",
      "Epoch 871/2000, train Loss: 0.0041\n",
      "Epoch 871/2000, test Loss: 0.0066\n",
      "Epoch 872/2000, train Loss: 0.0041\n",
      "Epoch 872/2000, test Loss: 0.0065\n",
      "Epoch 873/2000, train Loss: 0.0040\n",
      "Epoch 873/2000, test Loss: 0.0068\n",
      "Epoch 874/2000, train Loss: 0.0040\n",
      "Epoch 874/2000, test Loss: 0.0064\n",
      "Epoch 875/2000, train Loss: 0.0040\n",
      "Epoch 875/2000, test Loss: 0.0064\n",
      "Epoch 876/2000, train Loss: 0.0040\n",
      "Epoch 876/2000, test Loss: 0.0065\n",
      "Epoch 877/2000, train Loss: 0.0039\n",
      "Epoch 877/2000, test Loss: 0.0064\n",
      "Epoch 878/2000, train Loss: 0.0040\n",
      "Epoch 878/2000, test Loss: 0.0066\n",
      "Epoch 879/2000, train Loss: 0.0040\n",
      "Epoch 879/2000, test Loss: 0.0064\n",
      "Epoch 880/2000, train Loss: 0.0040\n",
      "Epoch 880/2000, test Loss: 0.0064\n",
      "Epoch 881/2000, train Loss: 0.0039\n",
      "Epoch 881/2000, test Loss: 0.0064\n",
      "Epoch 882/2000, train Loss: 0.0039\n",
      "Epoch 882/2000, test Loss: 0.0065\n",
      "Epoch 883/2000, train Loss: 0.0040\n",
      "Epoch 883/2000, test Loss: 0.0065\n",
      "Epoch 884/2000, train Loss: 0.0040\n",
      "Epoch 884/2000, test Loss: 0.0063\n",
      "Epoch 885/2000, train Loss: 0.0039\n",
      "Epoch 885/2000, test Loss: 0.0063\n",
      "Epoch 886/2000, train Loss: 0.0039\n",
      "Epoch 886/2000, test Loss: 0.0063\n",
      "Epoch 887/2000, train Loss: 0.0039\n",
      "Epoch 887/2000, test Loss: 0.0064\n",
      "Epoch 888/2000, train Loss: 0.0039\n",
      "Epoch 888/2000, test Loss: 0.0064\n",
      "Epoch 889/2000, train Loss: 0.0039\n",
      "Epoch 889/2000, test Loss: 0.0064\n",
      "Epoch 890/2000, train Loss: 0.0039\n",
      "Epoch 890/2000, test Loss: 0.0062\n",
      "Epoch 891/2000, train Loss: 0.0039\n",
      "Epoch 891/2000, test Loss: 0.0063\n",
      "Epoch 892/2000, train Loss: 0.0040\n",
      "Epoch 892/2000, test Loss: 0.0063\n",
      "Epoch 893/2000, train Loss: 0.0039\n",
      "Epoch 893/2000, test Loss: 0.0061\n",
      "Epoch 894/2000, train Loss: 0.0039\n",
      "Epoch 894/2000, test Loss: 0.0062\n",
      "Epoch 895/2000, train Loss: 0.0039\n",
      "Epoch 895/2000, test Loss: 0.0062\n",
      "Epoch 896/2000, train Loss: 0.0039\n",
      "Epoch 896/2000, test Loss: 0.0062\n",
      "Epoch 897/2000, train Loss: 0.0038\n",
      "Epoch 897/2000, test Loss: 0.0061\n",
      "Epoch 898/2000, train Loss: 0.0039\n",
      "Epoch 898/2000, test Loss: 0.0064\n",
      "Epoch 899/2000, train Loss: 0.0039\n",
      "Epoch 899/2000, test Loss: 0.0062\n",
      "Epoch 900/2000, train Loss: 0.0038\n",
      "Epoch 900/2000, test Loss: 0.0062\n",
      "Epoch 901/2000, train Loss: 0.0038\n",
      "Epoch 901/2000, test Loss: 0.0062\n",
      "Epoch 902/2000, train Loss: 0.0039\n",
      "Epoch 902/2000, test Loss: 0.0063\n",
      "Epoch 903/2000, train Loss: 0.0038\n",
      "Epoch 903/2000, test Loss: 0.0062\n",
      "Epoch 904/2000, train Loss: 0.0038\n",
      "Epoch 904/2000, test Loss: 0.0062\n",
      "Epoch 905/2000, train Loss: 0.0038\n",
      "Epoch 905/2000, test Loss: 0.0061\n",
      "Epoch 906/2000, train Loss: 0.0038\n",
      "Epoch 906/2000, test Loss: 0.0061\n",
      "Epoch 907/2000, train Loss: 0.0038\n",
      "Epoch 907/2000, test Loss: 0.0062\n",
      "Epoch 908/2000, train Loss: 0.0038\n",
      "Epoch 908/2000, test Loss: 0.0062\n",
      "Epoch 909/2000, train Loss: 0.0038\n",
      "Epoch 909/2000, test Loss: 0.0061\n",
      "Epoch 910/2000, train Loss: 0.0038\n",
      "Epoch 910/2000, test Loss: 0.0061\n",
      "Epoch 911/2000, train Loss: 0.0038\n",
      "Epoch 911/2000, test Loss: 0.0061\n",
      "Epoch 912/2000, train Loss: 0.0038\n",
      "Epoch 912/2000, test Loss: 0.0063\n",
      "Epoch 913/2000, train Loss: 0.0038\n",
      "Epoch 913/2000, test Loss: 0.0061\n",
      "Epoch 914/2000, train Loss: 0.0038\n",
      "Epoch 914/2000, test Loss: 0.0060\n",
      "Epoch 915/2000, train Loss: 0.0037\n",
      "Epoch 915/2000, test Loss: 0.0060\n",
      "Epoch 916/2000, train Loss: 0.0038\n",
      "Epoch 916/2000, test Loss: 0.0061\n",
      "Epoch 917/2000, train Loss: 0.0038\n",
      "Epoch 917/2000, test Loss: 0.0061\n",
      "Epoch 918/2000, train Loss: 0.0037\n",
      "Epoch 918/2000, test Loss: 0.0060\n",
      "Epoch 919/2000, train Loss: 0.0037\n",
      "Epoch 919/2000, test Loss: 0.0061\n",
      "Epoch 920/2000, train Loss: 0.0037\n",
      "Epoch 920/2000, test Loss: 0.0060\n",
      "Epoch 921/2000, train Loss: 0.0037\n",
      "Epoch 921/2000, test Loss: 0.0061\n",
      "Epoch 922/2000, train Loss: 0.0037\n",
      "Epoch 922/2000, test Loss: 0.0059\n",
      "Epoch 923/2000, train Loss: 0.0037\n",
      "Epoch 923/2000, test Loss: 0.0061\n",
      "Epoch 924/2000, train Loss: 0.0037\n",
      "Epoch 924/2000, test Loss: 0.0060\n",
      "Epoch 925/2000, train Loss: 0.0037\n",
      "Epoch 925/2000, test Loss: 0.0060\n",
      "Epoch 926/2000, train Loss: 0.0038\n",
      "Epoch 926/2000, test Loss: 0.0059\n",
      "Epoch 927/2000, train Loss: 0.0037\n",
      "Epoch 927/2000, test Loss: 0.0060\n",
      "Epoch 928/2000, train Loss: 0.0037\n",
      "Epoch 928/2000, test Loss: 0.0059\n",
      "Epoch 929/2000, train Loss: 0.0037\n",
      "Epoch 929/2000, test Loss: 0.0059\n",
      "Epoch 930/2000, train Loss: 0.0037\n",
      "Epoch 930/2000, test Loss: 0.0059\n",
      "Epoch 931/2000, train Loss: 0.0037\n",
      "Epoch 931/2000, test Loss: 0.0059\n",
      "Epoch 932/2000, train Loss: 0.0038\n",
      "Epoch 932/2000, test Loss: 0.0061\n",
      "Epoch 933/2000, train Loss: 0.0037\n",
      "Epoch 933/2000, test Loss: 0.0060\n",
      "Epoch 934/2000, train Loss: 0.0036\n",
      "Epoch 934/2000, test Loss: 0.0058\n",
      "Epoch 935/2000, train Loss: 0.0037\n",
      "Epoch 935/2000, test Loss: 0.0059\n",
      "Epoch 936/2000, train Loss: 0.0037\n",
      "Epoch 936/2000, test Loss: 0.0060\n",
      "Epoch 937/2000, train Loss: 0.0037\n",
      "Epoch 937/2000, test Loss: 0.0059\n",
      "Epoch 938/2000, train Loss: 0.0037\n",
      "Epoch 938/2000, test Loss: 0.0059\n",
      "Epoch 939/2000, train Loss: 0.0037\n",
      "Epoch 939/2000, test Loss: 0.0059\n",
      "Epoch 940/2000, train Loss: 0.0036\n",
      "Epoch 940/2000, test Loss: 0.0058\n",
      "Epoch 941/2000, train Loss: 0.0037\n",
      "Epoch 941/2000, test Loss: 0.0058\n",
      "Epoch 942/2000, train Loss: 0.0037\n",
      "Epoch 942/2000, test Loss: 0.0059\n",
      "Epoch 943/2000, train Loss: 0.0036\n",
      "Epoch 943/2000, test Loss: 0.0058\n",
      "Epoch 944/2000, train Loss: 0.0036\n",
      "Epoch 944/2000, test Loss: 0.0058\n",
      "Epoch 945/2000, train Loss: 0.0036\n",
      "Epoch 945/2000, test Loss: 0.0057\n",
      "Epoch 946/2000, train Loss: 0.0036\n",
      "Epoch 946/2000, test Loss: 0.0058\n",
      "Epoch 947/2000, train Loss: 0.0036\n",
      "Epoch 947/2000, test Loss: 0.0058\n",
      "Epoch 948/2000, train Loss: 0.0036\n",
      "Epoch 948/2000, test Loss: 0.0059\n",
      "Epoch 949/2000, train Loss: 0.0036\n",
      "Epoch 949/2000, test Loss: 0.0058\n",
      "Epoch 950/2000, train Loss: 0.0036\n",
      "Epoch 950/2000, test Loss: 0.0058\n",
      "Epoch 951/2000, train Loss: 0.0036\n",
      "Epoch 951/2000, test Loss: 0.0057\n",
      "Epoch 952/2000, train Loss: 0.0036\n",
      "Epoch 952/2000, test Loss: 0.0057\n",
      "Epoch 953/2000, train Loss: 0.0036\n",
      "Epoch 953/2000, test Loss: 0.0057\n",
      "Epoch 954/2000, train Loss: 0.0036\n",
      "Epoch 954/2000, test Loss: 0.0058\n",
      "Epoch 955/2000, train Loss: 0.0036\n",
      "Epoch 955/2000, test Loss: 0.0057\n",
      "Epoch 956/2000, train Loss: 0.0036\n",
      "Epoch 956/2000, test Loss: 0.0057\n",
      "Epoch 957/2000, train Loss: 0.0036\n",
      "Epoch 957/2000, test Loss: 0.0058\n",
      "Epoch 958/2000, train Loss: 0.0035\n",
      "Epoch 958/2000, test Loss: 0.0058\n",
      "Epoch 959/2000, train Loss: 0.0035\n",
      "Epoch 959/2000, test Loss: 0.0057\n",
      "Epoch 960/2000, train Loss: 0.0035\n",
      "Epoch 960/2000, test Loss: 0.0059\n",
      "Epoch 961/2000, train Loss: 0.0035\n",
      "Epoch 961/2000, test Loss: 0.0058\n",
      "Epoch 962/2000, train Loss: 0.0036\n",
      "Epoch 962/2000, test Loss: 0.0057\n",
      "Epoch 963/2000, train Loss: 0.0035\n",
      "Epoch 963/2000, test Loss: 0.0057\n",
      "Epoch 964/2000, train Loss: 0.0035\n",
      "Epoch 964/2000, test Loss: 0.0056\n",
      "Epoch 965/2000, train Loss: 0.0035\n",
      "Epoch 965/2000, test Loss: 0.0056\n",
      "Epoch 966/2000, train Loss: 0.0035\n",
      "Epoch 966/2000, test Loss: 0.0058\n",
      "Epoch 967/2000, train Loss: 0.0035\n",
      "Epoch 967/2000, test Loss: 0.0056\n",
      "Epoch 968/2000, train Loss: 0.0035\n",
      "Epoch 968/2000, test Loss: 0.0057\n",
      "Epoch 969/2000, train Loss: 0.0035\n",
      "Epoch 969/2000, test Loss: 0.0056\n",
      "Epoch 970/2000, train Loss: 0.0035\n",
      "Epoch 970/2000, test Loss: 0.0057\n",
      "Epoch 971/2000, train Loss: 0.0035\n",
      "Epoch 971/2000, test Loss: 0.0057\n",
      "Epoch 972/2000, train Loss: 0.0035\n",
      "Epoch 972/2000, test Loss: 0.0056\n",
      "Epoch 973/2000, train Loss: 0.0035\n",
      "Epoch 973/2000, test Loss: 0.0056\n",
      "Epoch 974/2000, train Loss: 0.0035\n",
      "Epoch 974/2000, test Loss: 0.0056\n",
      "Epoch 975/2000, train Loss: 0.0035\n",
      "Epoch 975/2000, test Loss: 0.0056\n",
      "Epoch 976/2000, train Loss: 0.0035\n",
      "Epoch 976/2000, test Loss: 0.0056\n",
      "Epoch 977/2000, train Loss: 0.0035\n",
      "Epoch 977/2000, test Loss: 0.0057\n",
      "Epoch 978/2000, train Loss: 0.0035\n",
      "Epoch 978/2000, test Loss: 0.0057\n",
      "Epoch 979/2000, train Loss: 0.0035\n",
      "Epoch 979/2000, test Loss: 0.0057\n",
      "Epoch 980/2000, train Loss: 0.0034\n",
      "Epoch 980/2000, test Loss: 0.0056\n",
      "Epoch 981/2000, train Loss: 0.0034\n",
      "Epoch 981/2000, test Loss: 0.0055\n",
      "Epoch 982/2000, train Loss: 0.0034\n",
      "Epoch 982/2000, test Loss: 0.0057\n",
      "Epoch 983/2000, train Loss: 0.0035\n",
      "Epoch 983/2000, test Loss: 0.0057\n",
      "Epoch 984/2000, train Loss: 0.0035\n",
      "Epoch 984/2000, test Loss: 0.0056\n",
      "Epoch 985/2000, train Loss: 0.0034\n",
      "Epoch 985/2000, test Loss: 0.0055\n",
      "Epoch 986/2000, train Loss: 0.0034\n",
      "Epoch 986/2000, test Loss: 0.0057\n",
      "Epoch 987/2000, train Loss: 0.0035\n",
      "Epoch 987/2000, test Loss: 0.0055\n",
      "Epoch 988/2000, train Loss: 0.0035\n",
      "Epoch 988/2000, test Loss: 0.0055\n",
      "Epoch 989/2000, train Loss: 0.0034\n",
      "Epoch 989/2000, test Loss: 0.0056\n",
      "Epoch 990/2000, train Loss: 0.0034\n",
      "Epoch 990/2000, test Loss: 0.0056\n",
      "Epoch 991/2000, train Loss: 0.0034\n",
      "Epoch 991/2000, test Loss: 0.0055\n",
      "Epoch 992/2000, train Loss: 0.0034\n",
      "Epoch 992/2000, test Loss: 0.0055\n",
      "Epoch 993/2000, train Loss: 0.0034\n",
      "Epoch 993/2000, test Loss: 0.0056\n",
      "Epoch 994/2000, train Loss: 0.0034\n",
      "Epoch 994/2000, test Loss: 0.0057\n",
      "Epoch 995/2000, train Loss: 0.0034\n",
      "Epoch 995/2000, test Loss: 0.0055\n",
      "Epoch 996/2000, train Loss: 0.0034\n",
      "Epoch 996/2000, test Loss: 0.0057\n",
      "Epoch 997/2000, train Loss: 0.0034\n",
      "Epoch 997/2000, test Loss: 0.0055\n",
      "Epoch 998/2000, train Loss: 0.0034\n",
      "Epoch 998/2000, test Loss: 0.0055\n",
      "Epoch 999/2000, train Loss: 0.0034\n",
      "Epoch 999/2000, test Loss: 0.0055\n",
      "Epoch 1000/2000, train Loss: 0.0034\n",
      "Epoch 1000/2000, test Loss: 0.0057\n",
      "Epoch 1001/2000, train Loss: 0.0033\n",
      "Epoch 1001/2000, test Loss: 0.0056\n",
      "Epoch 1002/2000, train Loss: 0.0034\n",
      "Epoch 1002/2000, test Loss: 0.0055\n",
      "Epoch 1003/2000, train Loss: 0.0034\n",
      "Epoch 1003/2000, test Loss: 0.0056\n",
      "Epoch 1004/2000, train Loss: 0.0034\n",
      "Epoch 1004/2000, test Loss: 0.0053\n",
      "Epoch 1005/2000, train Loss: 0.0034\n",
      "Epoch 1005/2000, test Loss: 0.0054\n",
      "Epoch 1006/2000, train Loss: 0.0033\n",
      "Epoch 1006/2000, test Loss: 0.0054\n",
      "Epoch 1007/2000, train Loss: 0.0034\n",
      "Epoch 1007/2000, test Loss: 0.0054\n",
      "Epoch 1008/2000, train Loss: 0.0034\n",
      "Epoch 1008/2000, test Loss: 0.0055\n",
      "Epoch 1009/2000, train Loss: 0.0034\n",
      "Epoch 1009/2000, test Loss: 0.0055\n",
      "Epoch 1010/2000, train Loss: 0.0033\n",
      "Epoch 1010/2000, test Loss: 0.0054\n",
      "Epoch 1011/2000, train Loss: 0.0033\n",
      "Epoch 1011/2000, test Loss: 0.0054\n",
      "Epoch 1012/2000, train Loss: 0.0033\n",
      "Epoch 1012/2000, test Loss: 0.0055\n",
      "Epoch 1013/2000, train Loss: 0.0033\n",
      "Epoch 1013/2000, test Loss: 0.0053\n",
      "Epoch 1014/2000, train Loss: 0.0033\n",
      "Epoch 1014/2000, test Loss: 0.0053\n",
      "Epoch 1015/2000, train Loss: 0.0033\n",
      "Epoch 1015/2000, test Loss: 0.0054\n",
      "Epoch 1016/2000, train Loss: 0.0034\n",
      "Epoch 1016/2000, test Loss: 0.0054\n",
      "Epoch 1017/2000, train Loss: 0.0033\n",
      "Epoch 1017/2000, test Loss: 0.0054\n",
      "Epoch 1018/2000, train Loss: 0.0033\n",
      "Epoch 1018/2000, test Loss: 0.0054\n",
      "Epoch 1019/2000, train Loss: 0.0034\n",
      "Epoch 1019/2000, test Loss: 0.0052\n",
      "Epoch 1020/2000, train Loss: 0.0033\n",
      "Epoch 1020/2000, test Loss: 0.0055\n",
      "Epoch 1021/2000, train Loss: 0.0033\n",
      "Epoch 1021/2000, test Loss: 0.0054\n",
      "Epoch 1022/2000, train Loss: 0.0033\n",
      "Epoch 1022/2000, test Loss: 0.0054\n",
      "Epoch 1023/2000, train Loss: 0.0033\n",
      "Epoch 1023/2000, test Loss: 0.0053\n",
      "Epoch 1024/2000, train Loss: 0.0033\n",
      "Epoch 1024/2000, test Loss: 0.0054\n",
      "Epoch 1025/2000, train Loss: 0.0033\n",
      "Epoch 1025/2000, test Loss: 0.0054\n",
      "Epoch 1026/2000, train Loss: 0.0033\n",
      "Epoch 1026/2000, test Loss: 0.0053\n",
      "Epoch 1027/2000, train Loss: 0.0033\n",
      "Epoch 1027/2000, test Loss: 0.0053\n",
      "Epoch 1028/2000, train Loss: 0.0033\n",
      "Epoch 1028/2000, test Loss: 0.0054\n",
      "Epoch 1029/2000, train Loss: 0.0032\n",
      "Epoch 1029/2000, test Loss: 0.0053\n",
      "Epoch 1030/2000, train Loss: 0.0033\n",
      "Epoch 1030/2000, test Loss: 0.0053\n",
      "Epoch 1031/2000, train Loss: 0.0033\n",
      "Epoch 1031/2000, test Loss: 0.0054\n",
      "Epoch 1032/2000, train Loss: 0.0032\n",
      "Epoch 1032/2000, test Loss: 0.0054\n",
      "Epoch 1033/2000, train Loss: 0.0033\n",
      "Epoch 1033/2000, test Loss: 0.0053\n",
      "Epoch 1034/2000, train Loss: 0.0033\n",
      "Epoch 1034/2000, test Loss: 0.0053\n",
      "Epoch 1035/2000, train Loss: 0.0033\n",
      "Epoch 1035/2000, test Loss: 0.0054\n",
      "Epoch 1036/2000, train Loss: 0.0032\n",
      "Epoch 1036/2000, test Loss: 0.0053\n",
      "Epoch 1037/2000, train Loss: 0.0033\n",
      "Epoch 1037/2000, test Loss: 0.0052\n",
      "Epoch 1038/2000, train Loss: 0.0033\n",
      "Epoch 1038/2000, test Loss: 0.0053\n",
      "Epoch 1039/2000, train Loss: 0.0032\n",
      "Epoch 1039/2000, test Loss: 0.0053\n",
      "Epoch 1040/2000, train Loss: 0.0032\n",
      "Epoch 1040/2000, test Loss: 0.0054\n",
      "Epoch 1041/2000, train Loss: 0.0032\n",
      "Epoch 1041/2000, test Loss: 0.0052\n",
      "Epoch 1042/2000, train Loss: 0.0033\n",
      "Epoch 1042/2000, test Loss: 0.0052\n",
      "Epoch 1043/2000, train Loss: 0.0032\n",
      "Epoch 1043/2000, test Loss: 0.0053\n",
      "Epoch 1044/2000, train Loss: 0.0032\n",
      "Epoch 1044/2000, test Loss: 0.0052\n",
      "Epoch 1045/2000, train Loss: 0.0032\n",
      "Epoch 1045/2000, test Loss: 0.0053\n",
      "Epoch 1046/2000, train Loss: 0.0032\n",
      "Epoch 1046/2000, test Loss: 0.0053\n",
      "Epoch 1047/2000, train Loss: 0.0032\n",
      "Epoch 1047/2000, test Loss: 0.0052\n",
      "Epoch 1048/2000, train Loss: 0.0032\n",
      "Epoch 1048/2000, test Loss: 0.0053\n",
      "Epoch 1049/2000, train Loss: 0.0032\n",
      "Epoch 1049/2000, test Loss: 0.0053\n",
      "Epoch 1050/2000, train Loss: 0.0032\n",
      "Epoch 1050/2000, test Loss: 0.0052\n",
      "Epoch 1051/2000, train Loss: 0.0032\n",
      "Epoch 1051/2000, test Loss: 0.0054\n",
      "Epoch 1052/2000, train Loss: 0.0032\n",
      "Epoch 1052/2000, test Loss: 0.0052\n",
      "Epoch 1053/2000, train Loss: 0.0032\n",
      "Epoch 1053/2000, test Loss: 0.0051\n",
      "Epoch 1054/2000, train Loss: 0.0032\n",
      "Epoch 1054/2000, test Loss: 0.0052\n",
      "Epoch 1055/2000, train Loss: 0.0032\n",
      "Epoch 1055/2000, test Loss: 0.0053\n",
      "Epoch 1056/2000, train Loss: 0.0032\n",
      "Epoch 1056/2000, test Loss: 0.0054\n",
      "Epoch 1057/2000, train Loss: 0.0032\n",
      "Epoch 1057/2000, test Loss: 0.0052\n",
      "Epoch 1058/2000, train Loss: 0.0032\n",
      "Epoch 1058/2000, test Loss: 0.0053\n",
      "Epoch 1059/2000, train Loss: 0.0032\n",
      "Epoch 1059/2000, test Loss: 0.0051\n",
      "Epoch 1060/2000, train Loss: 0.0032\n",
      "Epoch 1060/2000, test Loss: 0.0052\n",
      "Epoch 1061/2000, train Loss: 0.0032\n",
      "Epoch 1061/2000, test Loss: 0.0052\n",
      "Epoch 1062/2000, train Loss: 0.0031\n",
      "Epoch 1062/2000, test Loss: 0.0052\n",
      "Epoch 1063/2000, train Loss: 0.0032\n",
      "Epoch 1063/2000, test Loss: 0.0052\n",
      "Epoch 1064/2000, train Loss: 0.0032\n",
      "Epoch 1064/2000, test Loss: 0.0053\n",
      "Epoch 1065/2000, train Loss: 0.0032\n",
      "Epoch 1065/2000, test Loss: 0.0053\n",
      "Epoch 1066/2000, train Loss: 0.0032\n",
      "Epoch 1066/2000, test Loss: 0.0053\n",
      "Epoch 1067/2000, train Loss: 0.0032\n",
      "Epoch 1067/2000, test Loss: 0.0052\n",
      "Epoch 1068/2000, train Loss: 0.0031\n",
      "Epoch 1068/2000, test Loss: 0.0051\n",
      "Epoch 1069/2000, train Loss: 0.0031\n",
      "Epoch 1069/2000, test Loss: 0.0052\n",
      "Epoch 1070/2000, train Loss: 0.0032\n",
      "Epoch 1070/2000, test Loss: 0.0052\n",
      "Epoch 1071/2000, train Loss: 0.0031\n",
      "Epoch 1071/2000, test Loss: 0.0052\n",
      "Epoch 1072/2000, train Loss: 0.0032\n",
      "Epoch 1072/2000, test Loss: 0.0052\n",
      "Epoch 1073/2000, train Loss: 0.0031\n",
      "Epoch 1073/2000, test Loss: 0.0051\n",
      "Epoch 1074/2000, train Loss: 0.0032\n",
      "Epoch 1074/2000, test Loss: 0.0051\n",
      "Epoch 1075/2000, train Loss: 0.0032\n",
      "Epoch 1075/2000, test Loss: 0.0051\n",
      "Epoch 1076/2000, train Loss: 0.0031\n",
      "Epoch 1076/2000, test Loss: 0.0051\n",
      "Epoch 1077/2000, train Loss: 0.0032\n",
      "Epoch 1077/2000, test Loss: 0.0052\n",
      "Epoch 1078/2000, train Loss: 0.0031\n",
      "Epoch 1078/2000, test Loss: 0.0051\n",
      "Epoch 1079/2000, train Loss: 0.0031\n",
      "Epoch 1079/2000, test Loss: 0.0050\n",
      "Epoch 1080/2000, train Loss: 0.0031\n",
      "Epoch 1080/2000, test Loss: 0.0052\n",
      "Epoch 1081/2000, train Loss: 0.0031\n",
      "Epoch 1081/2000, test Loss: 0.0051\n",
      "Epoch 1082/2000, train Loss: 0.0031\n",
      "Epoch 1082/2000, test Loss: 0.0051\n",
      "Epoch 1083/2000, train Loss: 0.0031\n",
      "Epoch 1083/2000, test Loss: 0.0050\n",
      "Epoch 1084/2000, train Loss: 0.0031\n",
      "Epoch 1084/2000, test Loss: 0.0051\n",
      "Epoch 1085/2000, train Loss: 0.0031\n",
      "Epoch 1085/2000, test Loss: 0.0052\n",
      "Epoch 1086/2000, train Loss: 0.0031\n",
      "Epoch 1086/2000, test Loss: 0.0051\n",
      "Epoch 1087/2000, train Loss: 0.0031\n",
      "Epoch 1087/2000, test Loss: 0.0051\n",
      "Epoch 1088/2000, train Loss: 0.0031\n",
      "Epoch 1088/2000, test Loss: 0.0051\n",
      "Epoch 1089/2000, train Loss: 0.0031\n",
      "Epoch 1089/2000, test Loss: 0.0051\n",
      "Epoch 1090/2000, train Loss: 0.0031\n",
      "Epoch 1090/2000, test Loss: 0.0051\n",
      "Epoch 1091/2000, train Loss: 0.0031\n",
      "Epoch 1091/2000, test Loss: 0.0049\n",
      "Epoch 1092/2000, train Loss: 0.0031\n",
      "Epoch 1092/2000, test Loss: 0.0050\n",
      "Epoch 1093/2000, train Loss: 0.0031\n",
      "Epoch 1093/2000, test Loss: 0.0051\n",
      "Epoch 1094/2000, train Loss: 0.0031\n",
      "Epoch 1094/2000, test Loss: 0.0052\n",
      "Epoch 1095/2000, train Loss: 0.0031\n",
      "Epoch 1095/2000, test Loss: 0.0053\n",
      "Epoch 1096/2000, train Loss: 0.0031\n",
      "Epoch 1096/2000, test Loss: 0.0053\n",
      "Epoch 1097/2000, train Loss: 0.0031\n",
      "Epoch 1097/2000, test Loss: 0.0051\n",
      "Epoch 1098/2000, train Loss: 0.0031\n",
      "Epoch 1098/2000, test Loss: 0.0050\n",
      "Epoch 1099/2000, train Loss: 0.0031\n",
      "Epoch 1099/2000, test Loss: 0.0050\n",
      "Epoch 1100/2000, train Loss: 0.0031\n",
      "Epoch 1100/2000, test Loss: 0.0051\n",
      "Epoch 1101/2000, train Loss: 0.0031\n",
      "Epoch 1101/2000, test Loss: 0.0049\n",
      "Epoch 1102/2000, train Loss: 0.0031\n",
      "Epoch 1102/2000, test Loss: 0.0050\n",
      "Epoch 1103/2000, train Loss: 0.0030\n",
      "Epoch 1103/2000, test Loss: 0.0050\n",
      "Epoch 1104/2000, train Loss: 0.0031\n",
      "Epoch 1104/2000, test Loss: 0.0049\n",
      "Epoch 1105/2000, train Loss: 0.0030\n",
      "Epoch 1105/2000, test Loss: 0.0050\n",
      "Epoch 1106/2000, train Loss: 0.0031\n",
      "Epoch 1106/2000, test Loss: 0.0049\n",
      "Epoch 1107/2000, train Loss: 0.0030\n",
      "Epoch 1107/2000, test Loss: 0.0050\n",
      "Epoch 1108/2000, train Loss: 0.0031\n",
      "Epoch 1108/2000, test Loss: 0.0049\n",
      "Epoch 1109/2000, train Loss: 0.0030\n",
      "Epoch 1109/2000, test Loss: 0.0050\n",
      "Epoch 1110/2000, train Loss: 0.0030\n",
      "Epoch 1110/2000, test Loss: 0.0050\n",
      "Epoch 1111/2000, train Loss: 0.0030\n",
      "Epoch 1111/2000, test Loss: 0.0050\n",
      "Epoch 1112/2000, train Loss: 0.0030\n",
      "Epoch 1112/2000, test Loss: 0.0050\n",
      "Epoch 1113/2000, train Loss: 0.0030\n",
      "Epoch 1113/2000, test Loss: 0.0050\n",
      "Epoch 1114/2000, train Loss: 0.0030\n",
      "Epoch 1114/2000, test Loss: 0.0049\n",
      "Epoch 1115/2000, train Loss: 0.0030\n",
      "Epoch 1115/2000, test Loss: 0.0050\n",
      "Epoch 1116/2000, train Loss: 0.0030\n",
      "Epoch 1116/2000, test Loss: 0.0049\n",
      "Epoch 1117/2000, train Loss: 0.0030\n",
      "Epoch 1117/2000, test Loss: 0.0050\n",
      "Epoch 1118/2000, train Loss: 0.0030\n",
      "Epoch 1118/2000, test Loss: 0.0049\n",
      "Epoch 1119/2000, train Loss: 0.0030\n",
      "Epoch 1119/2000, test Loss: 0.0051\n",
      "Epoch 1120/2000, train Loss: 0.0030\n",
      "Epoch 1120/2000, test Loss: 0.0050\n",
      "Epoch 1121/2000, train Loss: 0.0030\n",
      "Epoch 1121/2000, test Loss: 0.0049\n",
      "Epoch 1122/2000, train Loss: 0.0030\n",
      "Epoch 1122/2000, test Loss: 0.0049\n",
      "Epoch 1123/2000, train Loss: 0.0030\n",
      "Epoch 1123/2000, test Loss: 0.0049\n",
      "Epoch 1124/2000, train Loss: 0.0030\n",
      "Epoch 1124/2000, test Loss: 0.0050\n",
      "Epoch 1125/2000, train Loss: 0.0030\n",
      "Epoch 1125/2000, test Loss: 0.0050\n",
      "Epoch 1126/2000, train Loss: 0.0030\n",
      "Epoch 1126/2000, test Loss: 0.0050\n",
      "Epoch 1127/2000, train Loss: 0.0030\n",
      "Epoch 1127/2000, test Loss: 0.0048\n",
      "Epoch 1128/2000, train Loss: 0.0030\n",
      "Epoch 1128/2000, test Loss: 0.0050\n",
      "Epoch 1129/2000, train Loss: 0.0030\n",
      "Epoch 1129/2000, test Loss: 0.0050\n",
      "Epoch 1130/2000, train Loss: 0.0030\n",
      "Epoch 1130/2000, test Loss: 0.0050\n",
      "Epoch 1131/2000, train Loss: 0.0030\n",
      "Epoch 1131/2000, test Loss: 0.0050\n",
      "Epoch 1132/2000, train Loss: 0.0031\n",
      "Epoch 1132/2000, test Loss: 0.0050\n",
      "Epoch 1133/2000, train Loss: 0.0030\n",
      "Epoch 1133/2000, test Loss: 0.0051\n",
      "Epoch 1134/2000, train Loss: 0.0030\n",
      "Epoch 1134/2000, test Loss: 0.0049\n",
      "Epoch 1135/2000, train Loss: 0.0030\n",
      "Epoch 1135/2000, test Loss: 0.0048\n",
      "Epoch 1136/2000, train Loss: 0.0030\n",
      "Epoch 1136/2000, test Loss: 0.0048\n",
      "Epoch 1137/2000, train Loss: 0.0030\n",
      "Epoch 1137/2000, test Loss: 0.0048\n",
      "Epoch 1138/2000, train Loss: 0.0030\n",
      "Epoch 1138/2000, test Loss: 0.0047\n",
      "Epoch 1139/2000, train Loss: 0.0030\n",
      "Epoch 1139/2000, test Loss: 0.0049\n",
      "Epoch 1140/2000, train Loss: 0.0030\n",
      "Epoch 1140/2000, test Loss: 0.0048\n",
      "Epoch 1141/2000, train Loss: 0.0029\n",
      "Epoch 1141/2000, test Loss: 0.0048\n",
      "Epoch 1142/2000, train Loss: 0.0030\n",
      "Epoch 1142/2000, test Loss: 0.0048\n",
      "Epoch 1143/2000, train Loss: 0.0029\n",
      "Epoch 1143/2000, test Loss: 0.0048\n",
      "Epoch 1144/2000, train Loss: 0.0030\n",
      "Epoch 1144/2000, test Loss: 0.0050\n",
      "Epoch 1145/2000, train Loss: 0.0030\n",
      "Epoch 1145/2000, test Loss: 0.0048\n",
      "Epoch 1146/2000, train Loss: 0.0030\n",
      "Epoch 1146/2000, test Loss: 0.0048\n",
      "Epoch 1147/2000, train Loss: 0.0030\n",
      "Epoch 1147/2000, test Loss: 0.0049\n",
      "Epoch 1148/2000, train Loss: 0.0029\n",
      "Epoch 1148/2000, test Loss: 0.0049\n",
      "Epoch 1149/2000, train Loss: 0.0030\n",
      "Epoch 1149/2000, test Loss: 0.0047\n",
      "Epoch 1150/2000, train Loss: 0.0030\n",
      "Epoch 1150/2000, test Loss: 0.0048\n",
      "Epoch 1151/2000, train Loss: 0.0029\n",
      "Epoch 1151/2000, test Loss: 0.0048\n",
      "Epoch 1152/2000, train Loss: 0.0029\n",
      "Epoch 1152/2000, test Loss: 0.0047\n",
      "Epoch 1153/2000, train Loss: 0.0029\n",
      "Epoch 1153/2000, test Loss: 0.0049\n",
      "Epoch 1154/2000, train Loss: 0.0029\n",
      "Epoch 1154/2000, test Loss: 0.0049\n",
      "Epoch 1155/2000, train Loss: 0.0029\n",
      "Epoch 1155/2000, test Loss: 0.0049\n",
      "Epoch 1156/2000, train Loss: 0.0029\n",
      "Epoch 1156/2000, test Loss: 0.0048\n",
      "Epoch 1157/2000, train Loss: 0.0029\n",
      "Epoch 1157/2000, test Loss: 0.0048\n",
      "Epoch 1158/2000, train Loss: 0.0029\n",
      "Epoch 1158/2000, test Loss: 0.0048\n",
      "Epoch 1159/2000, train Loss: 0.0029\n",
      "Epoch 1159/2000, test Loss: 0.0049\n",
      "Epoch 1160/2000, train Loss: 0.0029\n",
      "Epoch 1160/2000, test Loss: 0.0048\n",
      "Epoch 1161/2000, train Loss: 0.0030\n",
      "Epoch 1161/2000, test Loss: 0.0049\n",
      "Epoch 1162/2000, train Loss: 0.0030\n",
      "Epoch 1162/2000, test Loss: 0.0047\n",
      "Epoch 1163/2000, train Loss: 0.0029\n",
      "Epoch 1163/2000, test Loss: 0.0046\n",
      "Epoch 1164/2000, train Loss: 0.0029\n",
      "Epoch 1164/2000, test Loss: 0.0047\n",
      "Epoch 1165/2000, train Loss: 0.0029\n",
      "Epoch 1165/2000, test Loss: 0.0048\n",
      "Epoch 1166/2000, train Loss: 0.0029\n",
      "Epoch 1166/2000, test Loss: 0.0049\n",
      "Epoch 1167/2000, train Loss: 0.0029\n",
      "Epoch 1167/2000, test Loss: 0.0048\n",
      "Epoch 1168/2000, train Loss: 0.0029\n",
      "Epoch 1168/2000, test Loss: 0.0050\n",
      "Epoch 1169/2000, train Loss: 0.0029\n",
      "Epoch 1169/2000, test Loss: 0.0052\n",
      "Epoch 1170/2000, train Loss: 0.0030\n",
      "Epoch 1170/2000, test Loss: 0.0049\n",
      "Epoch 1171/2000, train Loss: 0.0029\n",
      "Epoch 1171/2000, test Loss: 0.0048\n",
      "Epoch 1172/2000, train Loss: 0.0029\n",
      "Epoch 1172/2000, test Loss: 0.0047\n",
      "Epoch 1173/2000, train Loss: 0.0029\n",
      "Epoch 1173/2000, test Loss: 0.0048\n",
      "Epoch 1174/2000, train Loss: 0.0029\n",
      "Epoch 1174/2000, test Loss: 0.0048\n",
      "Epoch 1175/2000, train Loss: 0.0029\n",
      "Epoch 1175/2000, test Loss: 0.0048\n",
      "Epoch 1176/2000, train Loss: 0.0029\n",
      "Epoch 1176/2000, test Loss: 0.0048\n",
      "Epoch 1177/2000, train Loss: 0.0029\n",
      "Epoch 1177/2000, test Loss: 0.0047\n",
      "Epoch 1178/2000, train Loss: 0.0029\n",
      "Epoch 1178/2000, test Loss: 0.0046\n",
      "Epoch 1179/2000, train Loss: 0.0029\n",
      "Epoch 1179/2000, test Loss: 0.0048\n",
      "Epoch 1180/2000, train Loss: 0.0029\n",
      "Epoch 1180/2000, test Loss: 0.0047\n",
      "Epoch 1181/2000, train Loss: 0.0029\n",
      "Epoch 1181/2000, test Loss: 0.0047\n",
      "Epoch 1182/2000, train Loss: 0.0029\n",
      "Epoch 1182/2000, test Loss: 0.0047\n",
      "Epoch 1183/2000, train Loss: 0.0029\n",
      "Epoch 1183/2000, test Loss: 0.0047\n",
      "Epoch 1184/2000, train Loss: 0.0029\n",
      "Epoch 1184/2000, test Loss: 0.0047\n",
      "Epoch 1185/2000, train Loss: 0.0029\n",
      "Epoch 1185/2000, test Loss: 0.0048\n",
      "Epoch 1186/2000, train Loss: 0.0029\n",
      "Epoch 1186/2000, test Loss: 0.0046\n",
      "Epoch 1187/2000, train Loss: 0.0029\n",
      "Epoch 1187/2000, test Loss: 0.0046\n",
      "Epoch 1188/2000, train Loss: 0.0029\n",
      "Epoch 1188/2000, test Loss: 0.0046\n",
      "Epoch 1189/2000, train Loss: 0.0028\n",
      "Epoch 1189/2000, test Loss: 0.0046\n",
      "Epoch 1190/2000, train Loss: 0.0028\n",
      "Epoch 1190/2000, test Loss: 0.0046\n",
      "Epoch 1191/2000, train Loss: 0.0028\n",
      "Epoch 1191/2000, test Loss: 0.0047\n",
      "Epoch 1192/2000, train Loss: 0.0028\n",
      "Epoch 1192/2000, test Loss: 0.0047\n",
      "Epoch 1193/2000, train Loss: 0.0030\n",
      "Epoch 1193/2000, test Loss: 0.0047\n",
      "Epoch 1194/2000, train Loss: 0.0029\n",
      "Epoch 1194/2000, test Loss: 0.0048\n",
      "Epoch 1195/2000, train Loss: 0.0029\n",
      "Epoch 1195/2000, test Loss: 0.0047\n",
      "Epoch 1196/2000, train Loss: 0.0028\n",
      "Epoch 1196/2000, test Loss: 0.0047\n",
      "Epoch 1197/2000, train Loss: 0.0028\n",
      "Epoch 1197/2000, test Loss: 0.0046\n",
      "Epoch 1198/2000, train Loss: 0.0028\n",
      "Epoch 1198/2000, test Loss: 0.0046\n",
      "Epoch 1199/2000, train Loss: 0.0028\n",
      "Epoch 1199/2000, test Loss: 0.0046\n",
      "Epoch 1200/2000, train Loss: 0.0029\n",
      "Epoch 1200/2000, test Loss: 0.0045\n",
      "Epoch 1201/2000, train Loss: 0.0028\n",
      "Epoch 1201/2000, test Loss: 0.0047\n",
      "Epoch 1202/2000, train Loss: 0.0028\n",
      "Epoch 1202/2000, test Loss: 0.0046\n",
      "Epoch 1203/2000, train Loss: 0.0028\n",
      "Epoch 1203/2000, test Loss: 0.0046\n",
      "Epoch 1204/2000, train Loss: 0.0028\n",
      "Epoch 1204/2000, test Loss: 0.0045\n",
      "Epoch 1205/2000, train Loss: 0.0028\n",
      "Epoch 1205/2000, test Loss: 0.0046\n",
      "Epoch 1206/2000, train Loss: 0.0028\n",
      "Epoch 1206/2000, test Loss: 0.0045\n",
      "Epoch 1207/2000, train Loss: 0.0029\n",
      "Epoch 1207/2000, test Loss: 0.0046\n",
      "Epoch 1208/2000, train Loss: 0.0029\n",
      "Epoch 1208/2000, test Loss: 0.0048\n",
      "Epoch 1209/2000, train Loss: 0.0029\n",
      "Epoch 1209/2000, test Loss: 0.0047\n",
      "Epoch 1210/2000, train Loss: 0.0028\n",
      "Epoch 1210/2000, test Loss: 0.0045\n",
      "Epoch 1211/2000, train Loss: 0.0028\n",
      "Epoch 1211/2000, test Loss: 0.0046\n",
      "Epoch 1212/2000, train Loss: 0.0028\n",
      "Epoch 1212/2000, test Loss: 0.0045\n",
      "Epoch 1213/2000, train Loss: 0.0028\n",
      "Epoch 1213/2000, test Loss: 0.0046\n",
      "Epoch 1214/2000, train Loss: 0.0028\n",
      "Epoch 1214/2000, test Loss: 0.0046\n",
      "Epoch 1215/2000, train Loss: 0.0028\n",
      "Epoch 1215/2000, test Loss: 0.0045\n",
      "Epoch 1216/2000, train Loss: 0.0028\n",
      "Epoch 1216/2000, test Loss: 0.0045\n",
      "Epoch 1217/2000, train Loss: 0.0028\n",
      "Epoch 1217/2000, test Loss: 0.0046\n",
      "Epoch 1218/2000, train Loss: 0.0028\n",
      "Epoch 1218/2000, test Loss: 0.0046\n",
      "Epoch 1219/2000, train Loss: 0.0028\n",
      "Epoch 1219/2000, test Loss: 0.0045\n",
      "Epoch 1220/2000, train Loss: 0.0028\n",
      "Epoch 1220/2000, test Loss: 0.0045\n",
      "Epoch 1221/2000, train Loss: 0.0028\n",
      "Epoch 1221/2000, test Loss: 0.0046\n",
      "Epoch 1222/2000, train Loss: 0.0028\n",
      "Epoch 1222/2000, test Loss: 0.0045\n",
      "Epoch 1223/2000, train Loss: 0.0028\n",
      "Epoch 1223/2000, test Loss: 0.0046\n",
      "Epoch 1224/2000, train Loss: 0.0028\n",
      "Epoch 1224/2000, test Loss: 0.0045\n",
      "Epoch 1225/2000, train Loss: 0.0028\n",
      "Epoch 1225/2000, test Loss: 0.0045\n",
      "Epoch 1226/2000, train Loss: 0.0028\n",
      "Epoch 1226/2000, test Loss: 0.0047\n",
      "Epoch 1227/2000, train Loss: 0.0028\n",
      "Epoch 1227/2000, test Loss: 0.0045\n",
      "Epoch 1228/2000, train Loss: 0.0027\n",
      "Epoch 1228/2000, test Loss: 0.0045\n",
      "Epoch 1229/2000, train Loss: 0.0028\n",
      "Epoch 1229/2000, test Loss: 0.0044\n",
      "Epoch 1230/2000, train Loss: 0.0028\n",
      "Epoch 1230/2000, test Loss: 0.0044\n",
      "Epoch 1231/2000, train Loss: 0.0028\n",
      "Epoch 1231/2000, test Loss: 0.0045\n",
      "Epoch 1232/2000, train Loss: 0.0028\n",
      "Epoch 1232/2000, test Loss: 0.0045\n",
      "Epoch 1233/2000, train Loss: 0.0027\n",
      "Epoch 1233/2000, test Loss: 0.0046\n",
      "Epoch 1234/2000, train Loss: 0.0027\n",
      "Epoch 1234/2000, test Loss: 0.0046\n",
      "Epoch 1235/2000, train Loss: 0.0028\n",
      "Epoch 1235/2000, test Loss: 0.0046\n",
      "Epoch 1236/2000, train Loss: 0.0028\n",
      "Epoch 1236/2000, test Loss: 0.0046\n",
      "Epoch 1237/2000, train Loss: 0.0028\n",
      "Epoch 1237/2000, test Loss: 0.0046\n",
      "Epoch 1238/2000, train Loss: 0.0028\n",
      "Epoch 1238/2000, test Loss: 0.0045\n",
      "Epoch 1239/2000, train Loss: 0.0028\n",
      "Epoch 1239/2000, test Loss: 0.0045\n",
      "Epoch 1240/2000, train Loss: 0.0028\n",
      "Epoch 1240/2000, test Loss: 0.0046\n",
      "Epoch 1241/2000, train Loss: 0.0028\n",
      "Epoch 1241/2000, test Loss: 0.0045\n",
      "Epoch 1242/2000, train Loss: 0.0028\n",
      "Epoch 1242/2000, test Loss: 0.0045\n",
      "Epoch 1243/2000, train Loss: 0.0028\n",
      "Epoch 1243/2000, test Loss: 0.0044\n",
      "Epoch 1244/2000, train Loss: 0.0027\n",
      "Epoch 1244/2000, test Loss: 0.0045\n",
      "Epoch 1245/2000, train Loss: 0.0027\n",
      "Epoch 1245/2000, test Loss: 0.0044\n",
      "Epoch 1246/2000, train Loss: 0.0028\n",
      "Epoch 1246/2000, test Loss: 0.0045\n",
      "Epoch 1247/2000, train Loss: 0.0027\n",
      "Epoch 1247/2000, test Loss: 0.0045\n",
      "Epoch 1248/2000, train Loss: 0.0027\n",
      "Epoch 1248/2000, test Loss: 0.0045\n",
      "Epoch 1249/2000, train Loss: 0.0027\n",
      "Epoch 1249/2000, test Loss: 0.0044\n",
      "Epoch 1250/2000, train Loss: 0.0027\n",
      "Epoch 1250/2000, test Loss: 0.0044\n",
      "Epoch 1251/2000, train Loss: 0.0027\n",
      "Epoch 1251/2000, test Loss: 0.0044\n",
      "Epoch 1252/2000, train Loss: 0.0028\n",
      "Epoch 1252/2000, test Loss: 0.0044\n",
      "Epoch 1253/2000, train Loss: 0.0027\n",
      "Epoch 1253/2000, test Loss: 0.0044\n",
      "Epoch 1254/2000, train Loss: 0.0027\n",
      "Epoch 1254/2000, test Loss: 0.0045\n",
      "Epoch 1255/2000, train Loss: 0.0028\n",
      "Epoch 1255/2000, test Loss: 0.0046\n",
      "Epoch 1256/2000, train Loss: 0.0027\n",
      "Epoch 1256/2000, test Loss: 0.0044\n",
      "Epoch 1257/2000, train Loss: 0.0027\n",
      "Epoch 1257/2000, test Loss: 0.0044\n",
      "Epoch 1258/2000, train Loss: 0.0027\n",
      "Epoch 1258/2000, test Loss: 0.0044\n",
      "Epoch 1259/2000, train Loss: 0.0027\n",
      "Epoch 1259/2000, test Loss: 0.0044\n",
      "Epoch 1260/2000, train Loss: 0.0027\n",
      "Epoch 1260/2000, test Loss: 0.0044\n",
      "Epoch 1261/2000, train Loss: 0.0027\n",
      "Epoch 1261/2000, test Loss: 0.0044\n",
      "Epoch 1262/2000, train Loss: 0.0027\n",
      "Epoch 1262/2000, test Loss: 0.0044\n",
      "Epoch 1263/2000, train Loss: 0.0027\n",
      "Epoch 1263/2000, test Loss: 0.0043\n",
      "Epoch 1264/2000, train Loss: 0.0027\n",
      "Epoch 1264/2000, test Loss: 0.0043\n",
      "Epoch 1265/2000, train Loss: 0.0028\n",
      "Epoch 1265/2000, test Loss: 0.0044\n",
      "Epoch 1266/2000, train Loss: 0.0027\n",
      "Epoch 1266/2000, test Loss: 0.0043\n",
      "Epoch 1267/2000, train Loss: 0.0027\n",
      "Epoch 1267/2000, test Loss: 0.0043\n",
      "Epoch 1268/2000, train Loss: 0.0027\n",
      "Epoch 1268/2000, test Loss: 0.0044\n",
      "Epoch 1269/2000, train Loss: 0.0027\n",
      "Epoch 1269/2000, test Loss: 0.0043\n",
      "Epoch 1270/2000, train Loss: 0.0027\n",
      "Epoch 1270/2000, test Loss: 0.0044\n",
      "Epoch 1271/2000, train Loss: 0.0027\n",
      "Epoch 1271/2000, test Loss: 0.0044\n",
      "Epoch 1272/2000, train Loss: 0.0027\n",
      "Epoch 1272/2000, test Loss: 0.0044\n",
      "Epoch 1273/2000, train Loss: 0.0027\n",
      "Epoch 1273/2000, test Loss: 0.0045\n",
      "Epoch 1274/2000, train Loss: 0.0027\n",
      "Epoch 1274/2000, test Loss: 0.0044\n",
      "Epoch 1275/2000, train Loss: 0.0027\n",
      "Epoch 1275/2000, test Loss: 0.0042\n",
      "Epoch 1276/2000, train Loss: 0.0027\n",
      "Epoch 1276/2000, test Loss: 0.0043\n",
      "Epoch 1277/2000, train Loss: 0.0027\n",
      "Epoch 1277/2000, test Loss: 0.0044\n",
      "Epoch 1278/2000, train Loss: 0.0027\n",
      "Epoch 1278/2000, test Loss: 0.0043\n",
      "Epoch 1279/2000, train Loss: 0.0027\n",
      "Epoch 1279/2000, test Loss: 0.0043\n",
      "Epoch 1280/2000, train Loss: 0.0027\n",
      "Epoch 1280/2000, test Loss: 0.0043\n",
      "Epoch 1281/2000, train Loss: 0.0027\n",
      "Epoch 1281/2000, test Loss: 0.0046\n",
      "Epoch 1282/2000, train Loss: 0.0027\n",
      "Epoch 1282/2000, test Loss: 0.0045\n",
      "Epoch 1283/2000, train Loss: 0.0027\n",
      "Epoch 1283/2000, test Loss: 0.0043\n",
      "Epoch 1284/2000, train Loss: 0.0027\n",
      "Epoch 1284/2000, test Loss: 0.0043\n",
      "Epoch 1285/2000, train Loss: 0.0027\n",
      "Epoch 1285/2000, test Loss: 0.0043\n",
      "Epoch 1286/2000, train Loss: 0.0027\n",
      "Epoch 1286/2000, test Loss: 0.0043\n",
      "Epoch 1287/2000, train Loss: 0.0027\n",
      "Epoch 1287/2000, test Loss: 0.0043\n",
      "Epoch 1288/2000, train Loss: 0.0027\n",
      "Epoch 1288/2000, test Loss: 0.0043\n",
      "Epoch 1289/2000, train Loss: 0.0027\n",
      "Epoch 1289/2000, test Loss: 0.0045\n",
      "Epoch 1290/2000, train Loss: 0.0027\n",
      "Epoch 1290/2000, test Loss: 0.0044\n",
      "Epoch 1291/2000, train Loss: 0.0027\n",
      "Epoch 1291/2000, test Loss: 0.0043\n",
      "Epoch 1292/2000, train Loss: 0.0027\n",
      "Epoch 1292/2000, test Loss: 0.0043\n",
      "Epoch 1293/2000, train Loss: 0.0027\n",
      "Epoch 1293/2000, test Loss: 0.0043\n",
      "Epoch 1294/2000, train Loss: 0.0027\n",
      "Epoch 1294/2000, test Loss: 0.0043\n",
      "Epoch 1295/2000, train Loss: 0.0027\n",
      "Epoch 1295/2000, test Loss: 0.0042\n",
      "Epoch 1296/2000, train Loss: 0.0027\n",
      "Epoch 1296/2000, test Loss: 0.0043\n",
      "Epoch 1297/2000, train Loss: 0.0027\n",
      "Epoch 1297/2000, test Loss: 0.0043\n",
      "Epoch 1298/2000, train Loss: 0.0026\n",
      "Epoch 1298/2000, test Loss: 0.0042\n",
      "Epoch 1299/2000, train Loss: 0.0027\n",
      "Epoch 1299/2000, test Loss: 0.0044\n",
      "Epoch 1300/2000, train Loss: 0.0027\n",
      "Epoch 1300/2000, test Loss: 0.0044\n",
      "Epoch 1301/2000, train Loss: 0.0027\n",
      "Epoch 1301/2000, test Loss: 0.0043\n",
      "Epoch 1302/2000, train Loss: 0.0027\n",
      "Epoch 1302/2000, test Loss: 0.0042\n",
      "Epoch 1303/2000, train Loss: 0.0027\n",
      "Epoch 1303/2000, test Loss: 0.0041\n",
      "Epoch 1304/2000, train Loss: 0.0027\n",
      "Epoch 1304/2000, test Loss: 0.0045\n",
      "Epoch 1305/2000, train Loss: 0.0027\n",
      "Epoch 1305/2000, test Loss: 0.0045\n",
      "Epoch 1306/2000, train Loss: 0.0027\n",
      "Epoch 1306/2000, test Loss: 0.0042\n",
      "Epoch 1307/2000, train Loss: 0.0027\n",
      "Epoch 1307/2000, test Loss: 0.0042\n",
      "Epoch 1308/2000, train Loss: 0.0026\n",
      "Epoch 1308/2000, test Loss: 0.0042\n",
      "Epoch 1309/2000, train Loss: 0.0026\n",
      "Epoch 1309/2000, test Loss: 0.0042\n",
      "Epoch 1310/2000, train Loss: 0.0026\n",
      "Epoch 1310/2000, test Loss: 0.0041\n",
      "Epoch 1311/2000, train Loss: 0.0026\n",
      "Epoch 1311/2000, test Loss: 0.0043\n",
      "Epoch 1312/2000, train Loss: 0.0026\n",
      "Epoch 1312/2000, test Loss: 0.0042\n",
      "Epoch 1313/2000, train Loss: 0.0026\n",
      "Epoch 1313/2000, test Loss: 0.0041\n",
      "Epoch 1314/2000, train Loss: 0.0026\n",
      "Epoch 1314/2000, test Loss: 0.0042\n",
      "Epoch 1315/2000, train Loss: 0.0026\n",
      "Epoch 1315/2000, test Loss: 0.0041\n",
      "Epoch 1316/2000, train Loss: 0.0026\n",
      "Epoch 1316/2000, test Loss: 0.0042\n",
      "Epoch 1317/2000, train Loss: 0.0026\n",
      "Epoch 1317/2000, test Loss: 0.0042\n",
      "Epoch 1318/2000, train Loss: 0.0027\n",
      "Epoch 1318/2000, test Loss: 0.0041\n",
      "Epoch 1319/2000, train Loss: 0.0026\n",
      "Epoch 1319/2000, test Loss: 0.0042\n",
      "Epoch 1320/2000, train Loss: 0.0026\n",
      "Epoch 1320/2000, test Loss: 0.0042\n",
      "Epoch 1321/2000, train Loss: 0.0026\n",
      "Epoch 1321/2000, test Loss: 0.0043\n",
      "Epoch 1322/2000, train Loss: 0.0026\n",
      "Epoch 1322/2000, test Loss: 0.0044\n",
      "Epoch 1323/2000, train Loss: 0.0026\n",
      "Epoch 1323/2000, test Loss: 0.0041\n",
      "Epoch 1324/2000, train Loss: 0.0026\n",
      "Epoch 1324/2000, test Loss: 0.0041\n",
      "Epoch 1325/2000, train Loss: 0.0026\n",
      "Epoch 1325/2000, test Loss: 0.0042\n",
      "Epoch 1326/2000, train Loss: 0.0026\n",
      "Epoch 1326/2000, test Loss: 0.0042\n",
      "Epoch 1327/2000, train Loss: 0.0026\n",
      "Epoch 1327/2000, test Loss: 0.0044\n",
      "Epoch 1328/2000, train Loss: 0.0026\n",
      "Epoch 1328/2000, test Loss: 0.0041\n",
      "Epoch 1329/2000, train Loss: 0.0026\n",
      "Epoch 1329/2000, test Loss: 0.0043\n",
      "Epoch 1330/2000, train Loss: 0.0026\n",
      "Epoch 1330/2000, test Loss: 0.0041\n",
      "Epoch 1331/2000, train Loss: 0.0026\n",
      "Epoch 1331/2000, test Loss: 0.0041\n",
      "Epoch 1332/2000, train Loss: 0.0027\n",
      "Epoch 1332/2000, test Loss: 0.0042\n",
      "Epoch 1333/2000, train Loss: 0.0026\n",
      "Epoch 1333/2000, test Loss: 0.0042\n",
      "Epoch 1334/2000, train Loss: 0.0026\n",
      "Epoch 1334/2000, test Loss: 0.0042\n",
      "Epoch 1335/2000, train Loss: 0.0026\n",
      "Epoch 1335/2000, test Loss: 0.0042\n",
      "Epoch 1336/2000, train Loss: 0.0026\n",
      "Epoch 1336/2000, test Loss: 0.0043\n",
      "Epoch 1337/2000, train Loss: 0.0026\n",
      "Epoch 1337/2000, test Loss: 0.0041\n",
      "Epoch 1338/2000, train Loss: 0.0026\n",
      "Epoch 1338/2000, test Loss: 0.0041\n",
      "Epoch 1339/2000, train Loss: 0.0026\n",
      "Epoch 1339/2000, test Loss: 0.0041\n",
      "Epoch 1340/2000, train Loss: 0.0026\n",
      "Epoch 1340/2000, test Loss: 0.0041\n",
      "Epoch 1341/2000, train Loss: 0.0026\n",
      "Epoch 1341/2000, test Loss: 0.0042\n",
      "Epoch 1342/2000, train Loss: 0.0026\n",
      "Epoch 1342/2000, test Loss: 0.0043\n",
      "Epoch 1343/2000, train Loss: 0.0026\n",
      "Epoch 1343/2000, test Loss: 0.0044\n",
      "Epoch 1344/2000, train Loss: 0.0027\n",
      "Epoch 1344/2000, test Loss: 0.0044\n",
      "Epoch 1345/2000, train Loss: 0.0026\n",
      "Epoch 1345/2000, test Loss: 0.0041\n",
      "Epoch 1346/2000, train Loss: 0.0026\n",
      "Epoch 1346/2000, test Loss: 0.0041\n",
      "Epoch 1347/2000, train Loss: 0.0026\n",
      "Epoch 1347/2000, test Loss: 0.0041\n",
      "Epoch 1348/2000, train Loss: 0.0026\n",
      "Epoch 1348/2000, test Loss: 0.0041\n",
      "Epoch 1349/2000, train Loss: 0.0026\n",
      "Epoch 1349/2000, test Loss: 0.0041\n",
      "Epoch 1350/2000, train Loss: 0.0026\n",
      "Epoch 1350/2000, test Loss: 0.0042\n",
      "Epoch 1351/2000, train Loss: 0.0026\n",
      "Epoch 1351/2000, test Loss: 0.0040\n",
      "Epoch 1352/2000, train Loss: 0.0026\n",
      "Epoch 1352/2000, test Loss: 0.0041\n",
      "Epoch 1353/2000, train Loss: 0.0026\n",
      "Epoch 1353/2000, test Loss: 0.0040\n",
      "Epoch 1354/2000, train Loss: 0.0026\n",
      "Epoch 1354/2000, test Loss: 0.0041\n",
      "Epoch 1355/2000, train Loss: 0.0026\n",
      "Epoch 1355/2000, test Loss: 0.0042\n",
      "Epoch 1356/2000, train Loss: 0.0026\n",
      "Epoch 1356/2000, test Loss: 0.0041\n",
      "Epoch 1357/2000, train Loss: 0.0025\n",
      "Epoch 1357/2000, test Loss: 0.0042\n",
      "Epoch 1358/2000, train Loss: 0.0026\n",
      "Epoch 1358/2000, test Loss: 0.0041\n",
      "Epoch 1359/2000, train Loss: 0.0026\n",
      "Epoch 1359/2000, test Loss: 0.0041\n",
      "Epoch 1360/2000, train Loss: 0.0025\n",
      "Epoch 1360/2000, test Loss: 0.0041\n",
      "Epoch 1361/2000, train Loss: 0.0026\n",
      "Epoch 1361/2000, test Loss: 0.0041\n",
      "Epoch 1362/2000, train Loss: 0.0026\n",
      "Epoch 1362/2000, test Loss: 0.0040\n",
      "Epoch 1363/2000, train Loss: 0.0026\n",
      "Epoch 1363/2000, test Loss: 0.0041\n",
      "Epoch 1364/2000, train Loss: 0.0025\n",
      "Epoch 1364/2000, test Loss: 0.0040\n",
      "Epoch 1365/2000, train Loss: 0.0026\n",
      "Epoch 1365/2000, test Loss: 0.0040\n",
      "Epoch 1366/2000, train Loss: 0.0026\n",
      "Epoch 1366/2000, test Loss: 0.0040\n",
      "Epoch 1367/2000, train Loss: 0.0026\n",
      "Epoch 1367/2000, test Loss: 0.0041\n",
      "Epoch 1368/2000, train Loss: 0.0026\n",
      "Epoch 1368/2000, test Loss: 0.0040\n",
      "Epoch 1369/2000, train Loss: 0.0025\n",
      "Epoch 1369/2000, test Loss: 0.0041\n",
      "Epoch 1370/2000, train Loss: 0.0025\n",
      "Epoch 1370/2000, test Loss: 0.0039\n",
      "Epoch 1371/2000, train Loss: 0.0026\n",
      "Epoch 1371/2000, test Loss: 0.0040\n",
      "Epoch 1372/2000, train Loss: 0.0025\n",
      "Epoch 1372/2000, test Loss: 0.0041\n",
      "Epoch 1373/2000, train Loss: 0.0025\n",
      "Epoch 1373/2000, test Loss: 0.0040\n",
      "Epoch 1374/2000, train Loss: 0.0025\n",
      "Epoch 1374/2000, test Loss: 0.0040\n",
      "Epoch 1375/2000, train Loss: 0.0026\n",
      "Epoch 1375/2000, test Loss: 0.0040\n",
      "Epoch 1376/2000, train Loss: 0.0026\n",
      "Epoch 1376/2000, test Loss: 0.0040\n",
      "Epoch 1377/2000, train Loss: 0.0025\n",
      "Epoch 1377/2000, test Loss: 0.0041\n",
      "Epoch 1378/2000, train Loss: 0.0025\n",
      "Epoch 1378/2000, test Loss: 0.0041\n",
      "Epoch 1379/2000, train Loss: 0.0026\n",
      "Epoch 1379/2000, test Loss: 0.0040\n",
      "Epoch 1380/2000, train Loss: 0.0025\n",
      "Epoch 1380/2000, test Loss: 0.0040\n",
      "Epoch 1381/2000, train Loss: 0.0025\n",
      "Epoch 1381/2000, test Loss: 0.0040\n",
      "Epoch 1382/2000, train Loss: 0.0025\n",
      "Epoch 1382/2000, test Loss: 0.0040\n",
      "Epoch 1383/2000, train Loss: 0.0025\n",
      "Epoch 1383/2000, test Loss: 0.0042\n",
      "Epoch 1384/2000, train Loss: 0.0025\n",
      "Epoch 1384/2000, test Loss: 0.0040\n",
      "Epoch 1385/2000, train Loss: 0.0026\n",
      "Epoch 1385/2000, test Loss: 0.0040\n",
      "Epoch 1386/2000, train Loss: 0.0025\n",
      "Epoch 1386/2000, test Loss: 0.0040\n",
      "Epoch 1387/2000, train Loss: 0.0025\n",
      "Epoch 1387/2000, test Loss: 0.0039\n",
      "Epoch 1388/2000, train Loss: 0.0025\n",
      "Epoch 1388/2000, test Loss: 0.0040\n",
      "Epoch 1389/2000, train Loss: 0.0025\n",
      "Epoch 1389/2000, test Loss: 0.0039\n",
      "Epoch 1390/2000, train Loss: 0.0025\n",
      "Epoch 1390/2000, test Loss: 0.0040\n",
      "Epoch 1391/2000, train Loss: 0.0026\n",
      "Epoch 1391/2000, test Loss: 0.0039\n",
      "Epoch 1392/2000, train Loss: 0.0025\n",
      "Epoch 1392/2000, test Loss: 0.0039\n",
      "Epoch 1393/2000, train Loss: 0.0025\n",
      "Epoch 1393/2000, test Loss: 0.0039\n",
      "Epoch 1394/2000, train Loss: 0.0025\n",
      "Epoch 1394/2000, test Loss: 0.0039\n",
      "Epoch 1395/2000, train Loss: 0.0025\n",
      "Epoch 1395/2000, test Loss: 0.0039\n",
      "Epoch 1396/2000, train Loss: 0.0025\n",
      "Epoch 1396/2000, test Loss: 0.0041\n",
      "Epoch 1397/2000, train Loss: 0.0025\n",
      "Epoch 1397/2000, test Loss: 0.0041\n",
      "Epoch 1398/2000, train Loss: 0.0025\n",
      "Epoch 1398/2000, test Loss: 0.0041\n",
      "Epoch 1399/2000, train Loss: 0.0025\n",
      "Epoch 1399/2000, test Loss: 0.0040\n",
      "Epoch 1400/2000, train Loss: 0.0025\n",
      "Epoch 1400/2000, test Loss: 0.0039\n",
      "Epoch 1401/2000, train Loss: 0.0025\n",
      "Epoch 1401/2000, test Loss: 0.0039\n",
      "Epoch 1402/2000, train Loss: 0.0025\n",
      "Epoch 1402/2000, test Loss: 0.0039\n",
      "Epoch 1403/2000, train Loss: 0.0025\n",
      "Epoch 1403/2000, test Loss: 0.0040\n",
      "Epoch 1404/2000, train Loss: 0.0025\n",
      "Epoch 1404/2000, test Loss: 0.0041\n",
      "Epoch 1405/2000, train Loss: 0.0025\n",
      "Epoch 1405/2000, test Loss: 0.0040\n",
      "Epoch 1406/2000, train Loss: 0.0025\n",
      "Epoch 1406/2000, test Loss: 0.0039\n",
      "Epoch 1407/2000, train Loss: 0.0025\n",
      "Epoch 1407/2000, test Loss: 0.0039\n",
      "Epoch 1408/2000, train Loss: 0.0025\n",
      "Epoch 1408/2000, test Loss: 0.0039\n",
      "Epoch 1409/2000, train Loss: 0.0025\n",
      "Epoch 1409/2000, test Loss: 0.0039\n",
      "Epoch 1410/2000, train Loss: 0.0025\n",
      "Epoch 1410/2000, test Loss: 0.0038\n",
      "Epoch 1411/2000, train Loss: 0.0025\n",
      "Epoch 1411/2000, test Loss: 0.0039\n",
      "Epoch 1412/2000, train Loss: 0.0025\n",
      "Epoch 1412/2000, test Loss: 0.0039\n",
      "Epoch 1413/2000, train Loss: 0.0025\n",
      "Epoch 1413/2000, test Loss: 0.0039\n",
      "Epoch 1414/2000, train Loss: 0.0025\n",
      "Epoch 1414/2000, test Loss: 0.0039\n",
      "Epoch 1415/2000, train Loss: 0.0025\n",
      "Epoch 1415/2000, test Loss: 0.0039\n",
      "Epoch 1416/2000, train Loss: 0.0025\n",
      "Epoch 1416/2000, test Loss: 0.0040\n",
      "Epoch 1417/2000, train Loss: 0.0025\n",
      "Epoch 1417/2000, test Loss: 0.0038\n",
      "Epoch 1418/2000, train Loss: 0.0025\n",
      "Epoch 1418/2000, test Loss: 0.0039\n",
      "Epoch 1419/2000, train Loss: 0.0025\n",
      "Epoch 1419/2000, test Loss: 0.0040\n",
      "Epoch 1420/2000, train Loss: 0.0025\n",
      "Epoch 1420/2000, test Loss: 0.0039\n",
      "Epoch 1421/2000, train Loss: 0.0025\n",
      "Epoch 1421/2000, test Loss: 0.0039\n",
      "Epoch 1422/2000, train Loss: 0.0025\n",
      "Epoch 1422/2000, test Loss: 0.0039\n",
      "Epoch 1423/2000, train Loss: 0.0025\n",
      "Epoch 1423/2000, test Loss: 0.0040\n",
      "Epoch 1424/2000, train Loss: 0.0025\n",
      "Epoch 1424/2000, test Loss: 0.0041\n",
      "Epoch 1425/2000, train Loss: 0.0025\n",
      "Epoch 1425/2000, test Loss: 0.0041\n",
      "Epoch 1426/2000, train Loss: 0.0025\n",
      "Epoch 1426/2000, test Loss: 0.0039\n",
      "Epoch 1427/2000, train Loss: 0.0025\n",
      "Epoch 1427/2000, test Loss: 0.0040\n",
      "Epoch 1428/2000, train Loss: 0.0025\n",
      "Epoch 1428/2000, test Loss: 0.0038\n",
      "Epoch 1429/2000, train Loss: 0.0025\n",
      "Epoch 1429/2000, test Loss: 0.0039\n",
      "Epoch 1430/2000, train Loss: 0.0025\n",
      "Epoch 1430/2000, test Loss: 0.0039\n",
      "Epoch 1431/2000, train Loss: 0.0025\n",
      "Epoch 1431/2000, test Loss: 0.0040\n",
      "Epoch 1432/2000, train Loss: 0.0025\n",
      "Epoch 1432/2000, test Loss: 0.0039\n",
      "Epoch 1433/2000, train Loss: 0.0025\n",
      "Epoch 1433/2000, test Loss: 0.0039\n",
      "Epoch 1434/2000, train Loss: 0.0025\n",
      "Epoch 1434/2000, test Loss: 0.0040\n",
      "Epoch 1435/2000, train Loss: 0.0025\n",
      "Epoch 1435/2000, test Loss: 0.0040\n",
      "Epoch 1436/2000, train Loss: 0.0025\n",
      "Epoch 1436/2000, test Loss: 0.0039\n",
      "Epoch 1437/2000, train Loss: 0.0025\n",
      "Epoch 1437/2000, test Loss: 0.0038\n",
      "Epoch 1438/2000, train Loss: 0.0025\n",
      "Epoch 1438/2000, test Loss: 0.0039\n",
      "Epoch 1439/2000, train Loss: 0.0024\n",
      "Epoch 1439/2000, test Loss: 0.0039\n",
      "Epoch 1440/2000, train Loss: 0.0025\n",
      "Epoch 1440/2000, test Loss: 0.0038\n",
      "Epoch 1441/2000, train Loss: 0.0025\n",
      "Epoch 1441/2000, test Loss: 0.0039\n",
      "Epoch 1442/2000, train Loss: 0.0024\n",
      "Epoch 1442/2000, test Loss: 0.0040\n",
      "Epoch 1443/2000, train Loss: 0.0025\n",
      "Epoch 1443/2000, test Loss: 0.0039\n",
      "Epoch 1444/2000, train Loss: 0.0024\n",
      "Epoch 1444/2000, test Loss: 0.0038\n",
      "Epoch 1445/2000, train Loss: 0.0024\n",
      "Epoch 1445/2000, test Loss: 0.0039\n",
      "Epoch 1446/2000, train Loss: 0.0024\n",
      "Epoch 1446/2000, test Loss: 0.0039\n",
      "Epoch 1447/2000, train Loss: 0.0025\n",
      "Epoch 1447/2000, test Loss: 0.0038\n",
      "Epoch 1448/2000, train Loss: 0.0024\n",
      "Epoch 1448/2000, test Loss: 0.0039\n",
      "Epoch 1449/2000, train Loss: 0.0024\n",
      "Epoch 1449/2000, test Loss: 0.0037\n",
      "Epoch 1450/2000, train Loss: 0.0024\n",
      "Epoch 1450/2000, test Loss: 0.0038\n",
      "Epoch 1451/2000, train Loss: 0.0024\n",
      "Epoch 1451/2000, test Loss: 0.0037\n",
      "Epoch 1452/2000, train Loss: 0.0024\n",
      "Epoch 1452/2000, test Loss: 0.0039\n",
      "Epoch 1453/2000, train Loss: 0.0025\n",
      "Epoch 1453/2000, test Loss: 0.0038\n",
      "Epoch 1454/2000, train Loss: 0.0024\n",
      "Epoch 1454/2000, test Loss: 0.0038\n",
      "Epoch 1455/2000, train Loss: 0.0025\n",
      "Epoch 1455/2000, test Loss: 0.0039\n",
      "Epoch 1456/2000, train Loss: 0.0024\n",
      "Epoch 1456/2000, test Loss: 0.0038\n",
      "Epoch 1457/2000, train Loss: 0.0025\n",
      "Epoch 1457/2000, test Loss: 0.0038\n",
      "Epoch 1458/2000, train Loss: 0.0025\n",
      "Epoch 1458/2000, test Loss: 0.0038\n",
      "Epoch 1459/2000, train Loss: 0.0025\n",
      "Epoch 1459/2000, test Loss: 0.0038\n",
      "Epoch 1460/2000, train Loss: 0.0024\n",
      "Epoch 1460/2000, test Loss: 0.0038\n",
      "Epoch 1461/2000, train Loss: 0.0024\n",
      "Epoch 1461/2000, test Loss: 0.0039\n",
      "Epoch 1462/2000, train Loss: 0.0024\n",
      "Epoch 1462/2000, test Loss: 0.0037\n",
      "Epoch 1463/2000, train Loss: 0.0025\n",
      "Epoch 1463/2000, test Loss: 0.0037\n",
      "Epoch 1464/2000, train Loss: 0.0024\n",
      "Epoch 1464/2000, test Loss: 0.0037\n",
      "Epoch 1465/2000, train Loss: 0.0024\n",
      "Epoch 1465/2000, test Loss: 0.0037\n",
      "Epoch 1466/2000, train Loss: 0.0024\n",
      "Epoch 1466/2000, test Loss: 0.0038\n",
      "Epoch 1467/2000, train Loss: 0.0024\n",
      "Epoch 1467/2000, test Loss: 0.0037\n",
      "Epoch 1468/2000, train Loss: 0.0024\n",
      "Epoch 1468/2000, test Loss: 0.0037\n",
      "Epoch 1469/2000, train Loss: 0.0024\n",
      "Epoch 1469/2000, test Loss: 0.0038\n",
      "Epoch 1470/2000, train Loss: 0.0025\n",
      "Epoch 1470/2000, test Loss: 0.0039\n",
      "Epoch 1471/2000, train Loss: 0.0024\n",
      "Epoch 1471/2000, test Loss: 0.0038\n",
      "Epoch 1472/2000, train Loss: 0.0024\n",
      "Epoch 1472/2000, test Loss: 0.0038\n",
      "Epoch 1473/2000, train Loss: 0.0025\n",
      "Epoch 1473/2000, test Loss: 0.0037\n",
      "Epoch 1474/2000, train Loss: 0.0025\n",
      "Epoch 1474/2000, test Loss: 0.0038\n",
      "Epoch 1475/2000, train Loss: 0.0024\n",
      "Epoch 1475/2000, test Loss: 0.0037\n",
      "Epoch 1476/2000, train Loss: 0.0025\n",
      "Epoch 1476/2000, test Loss: 0.0036\n",
      "Epoch 1477/2000, train Loss: 0.0025\n",
      "Epoch 1477/2000, test Loss: 0.0037\n",
      "Epoch 1478/2000, train Loss: 0.0025\n",
      "Epoch 1478/2000, test Loss: 0.0037\n",
      "Epoch 1479/2000, train Loss: 0.0024\n",
      "Epoch 1479/2000, test Loss: 0.0039\n",
      "Epoch 1480/2000, train Loss: 0.0025\n",
      "Epoch 1480/2000, test Loss: 0.0037\n",
      "Epoch 1481/2000, train Loss: 0.0024\n",
      "Epoch 1481/2000, test Loss: 0.0038\n",
      "Epoch 1482/2000, train Loss: 0.0025\n",
      "Epoch 1482/2000, test Loss: 0.0037\n",
      "Epoch 1483/2000, train Loss: 0.0024\n",
      "Epoch 1483/2000, test Loss: 0.0039\n",
      "Epoch 1484/2000, train Loss: 0.0024\n",
      "Epoch 1484/2000, test Loss: 0.0037\n",
      "Epoch 1485/2000, train Loss: 0.0024\n",
      "Epoch 1485/2000, test Loss: 0.0037\n",
      "Epoch 1486/2000, train Loss: 0.0024\n",
      "Epoch 1486/2000, test Loss: 0.0036\n",
      "Epoch 1487/2000, train Loss: 0.0024\n",
      "Epoch 1487/2000, test Loss: 0.0037\n",
      "Epoch 1488/2000, train Loss: 0.0024\n",
      "Epoch 1488/2000, test Loss: 0.0039\n",
      "Epoch 1489/2000, train Loss: 0.0024\n",
      "Epoch 1489/2000, test Loss: 0.0037\n",
      "Epoch 1490/2000, train Loss: 0.0024\n",
      "Epoch 1490/2000, test Loss: 0.0039\n",
      "Epoch 1491/2000, train Loss: 0.0024\n",
      "Epoch 1491/2000, test Loss: 0.0037\n",
      "Epoch 1492/2000, train Loss: 0.0024\n",
      "Epoch 1492/2000, test Loss: 0.0038\n",
      "Epoch 1493/2000, train Loss: 0.0024\n",
      "Epoch 1493/2000, test Loss: 0.0037\n",
      "Epoch 1494/2000, train Loss: 0.0024\n",
      "Epoch 1494/2000, test Loss: 0.0037\n",
      "Epoch 1495/2000, train Loss: 0.0024\n",
      "Epoch 1495/2000, test Loss: 0.0037\n",
      "Epoch 1496/2000, train Loss: 0.0024\n",
      "Epoch 1496/2000, test Loss: 0.0036\n",
      "Epoch 1497/2000, train Loss: 0.0024\n",
      "Epoch 1497/2000, test Loss: 0.0037\n",
      "Epoch 1498/2000, train Loss: 0.0024\n",
      "Epoch 1498/2000, test Loss: 0.0037\n",
      "Epoch 1499/2000, train Loss: 0.0024\n",
      "Epoch 1499/2000, test Loss: 0.0037\n",
      "Epoch 1500/2000, train Loss: 0.0024\n",
      "Epoch 1500/2000, test Loss: 0.0036\n",
      "Epoch 1501/2000, train Loss: 0.0024\n",
      "Epoch 1501/2000, test Loss: 0.0036\n",
      "Epoch 1502/2000, train Loss: 0.0024\n",
      "Epoch 1502/2000, test Loss: 0.0037\n",
      "Epoch 1503/2000, train Loss: 0.0024\n",
      "Epoch 1503/2000, test Loss: 0.0036\n",
      "Epoch 1504/2000, train Loss: 0.0024\n",
      "Epoch 1504/2000, test Loss: 0.0036\n",
      "Epoch 1505/2000, train Loss: 0.0024\n",
      "Epoch 1505/2000, test Loss: 0.0036\n",
      "Epoch 1506/2000, train Loss: 0.0024\n",
      "Epoch 1506/2000, test Loss: 0.0037\n",
      "Epoch 1507/2000, train Loss: 0.0024\n",
      "Epoch 1507/2000, test Loss: 0.0036\n",
      "Epoch 1508/2000, train Loss: 0.0024\n",
      "Epoch 1508/2000, test Loss: 0.0037\n",
      "Epoch 1509/2000, train Loss: 0.0024\n",
      "Epoch 1509/2000, test Loss: 0.0038\n",
      "Epoch 1510/2000, train Loss: 0.0024\n",
      "Epoch 1510/2000, test Loss: 0.0039\n",
      "Epoch 1511/2000, train Loss: 0.0024\n",
      "Epoch 1511/2000, test Loss: 0.0038\n",
      "Epoch 1512/2000, train Loss: 0.0024\n",
      "Epoch 1512/2000, test Loss: 0.0037\n",
      "Epoch 1513/2000, train Loss: 0.0024\n",
      "Epoch 1513/2000, test Loss: 0.0036\n",
      "Epoch 1514/2000, train Loss: 0.0024\n",
      "Epoch 1514/2000, test Loss: 0.0036\n",
      "Epoch 1515/2000, train Loss: 0.0023\n",
      "Epoch 1515/2000, test Loss: 0.0037\n",
      "Epoch 1516/2000, train Loss: 0.0024\n",
      "Epoch 1516/2000, test Loss: 0.0036\n",
      "Epoch 1517/2000, train Loss: 0.0024\n",
      "Epoch 1517/2000, test Loss: 0.0038\n",
      "Epoch 1518/2000, train Loss: 0.0024\n",
      "Epoch 1518/2000, test Loss: 0.0038\n",
      "Epoch 1519/2000, train Loss: 0.0024\n",
      "Epoch 1519/2000, test Loss: 0.0037\n",
      "Epoch 1520/2000, train Loss: 0.0024\n",
      "Epoch 1520/2000, test Loss: 0.0036\n",
      "Epoch 1521/2000, train Loss: 0.0024\n",
      "Epoch 1521/2000, test Loss: 0.0037\n",
      "Epoch 1522/2000, train Loss: 0.0024\n",
      "Epoch 1522/2000, test Loss: 0.0037\n",
      "Epoch 1523/2000, train Loss: 0.0023\n",
      "Epoch 1523/2000, test Loss: 0.0036\n",
      "Epoch 1524/2000, train Loss: 0.0024\n",
      "Epoch 1524/2000, test Loss: 0.0039\n",
      "Epoch 1525/2000, train Loss: 0.0024\n",
      "Epoch 1525/2000, test Loss: 0.0036\n",
      "Epoch 1526/2000, train Loss: 0.0024\n",
      "Epoch 1526/2000, test Loss: 0.0037\n",
      "Epoch 1527/2000, train Loss: 0.0024\n",
      "Epoch 1527/2000, test Loss: 0.0037\n",
      "Epoch 1528/2000, train Loss: 0.0024\n",
      "Epoch 1528/2000, test Loss: 0.0037\n",
      "Epoch 1529/2000, train Loss: 0.0024\n",
      "Epoch 1529/2000, test Loss: 0.0037\n",
      "Epoch 1530/2000, train Loss: 0.0024\n",
      "Epoch 1530/2000, test Loss: 0.0037\n",
      "Epoch 1531/2000, train Loss: 0.0023\n",
      "Epoch 1531/2000, test Loss: 0.0035\n",
      "Epoch 1532/2000, train Loss: 0.0024\n",
      "Epoch 1532/2000, test Loss: 0.0036\n",
      "Epoch 1533/2000, train Loss: 0.0023\n",
      "Epoch 1533/2000, test Loss: 0.0036\n",
      "Epoch 1534/2000, train Loss: 0.0023\n",
      "Epoch 1534/2000, test Loss: 0.0037\n",
      "Epoch 1535/2000, train Loss: 0.0023\n",
      "Epoch 1535/2000, test Loss: 0.0038\n",
      "Epoch 1536/2000, train Loss: 0.0024\n",
      "Epoch 1536/2000, test Loss: 0.0036\n",
      "Epoch 1537/2000, train Loss: 0.0024\n",
      "Epoch 1537/2000, test Loss: 0.0035\n",
      "Epoch 1538/2000, train Loss: 0.0024\n",
      "Epoch 1538/2000, test Loss: 0.0036\n",
      "Epoch 1539/2000, train Loss: 0.0023\n",
      "Epoch 1539/2000, test Loss: 0.0036\n",
      "Epoch 1540/2000, train Loss: 0.0024\n",
      "Epoch 1540/2000, test Loss: 0.0036\n",
      "Epoch 1541/2000, train Loss: 0.0023\n",
      "Epoch 1541/2000, test Loss: 0.0036\n",
      "Epoch 1542/2000, train Loss: 0.0023\n",
      "Epoch 1542/2000, test Loss: 0.0037\n",
      "Epoch 1543/2000, train Loss: 0.0024\n",
      "Epoch 1543/2000, test Loss: 0.0035\n",
      "Epoch 1544/2000, train Loss: 0.0023\n",
      "Epoch 1544/2000, test Loss: 0.0037\n",
      "Epoch 1545/2000, train Loss: 0.0024\n",
      "Epoch 1545/2000, test Loss: 0.0038\n",
      "Epoch 1546/2000, train Loss: 0.0023\n",
      "Epoch 1546/2000, test Loss: 0.0036\n",
      "Epoch 1547/2000, train Loss: 0.0023\n",
      "Epoch 1547/2000, test Loss: 0.0037\n",
      "Epoch 1548/2000, train Loss: 0.0023\n",
      "Epoch 1548/2000, test Loss: 0.0036\n",
      "Epoch 1549/2000, train Loss: 0.0023\n",
      "Epoch 1549/2000, test Loss: 0.0036\n",
      "Epoch 1550/2000, train Loss: 0.0023\n",
      "Epoch 1550/2000, test Loss: 0.0037\n",
      "Epoch 1551/2000, train Loss: 0.0024\n",
      "Epoch 1551/2000, test Loss: 0.0037\n",
      "Epoch 1552/2000, train Loss: 0.0024\n",
      "Epoch 1552/2000, test Loss: 0.0035\n",
      "Epoch 1553/2000, train Loss: 0.0024\n",
      "Epoch 1553/2000, test Loss: 0.0036\n",
      "Epoch 1554/2000, train Loss: 0.0023\n",
      "Epoch 1554/2000, test Loss: 0.0037\n",
      "Epoch 1555/2000, train Loss: 0.0024\n",
      "Epoch 1555/2000, test Loss: 0.0035\n",
      "Epoch 1556/2000, train Loss: 0.0023\n",
      "Epoch 1556/2000, test Loss: 0.0037\n",
      "Epoch 1557/2000, train Loss: 0.0024\n",
      "Epoch 1557/2000, test Loss: 0.0037\n",
      "Epoch 1558/2000, train Loss: 0.0023\n",
      "Epoch 1558/2000, test Loss: 0.0035\n",
      "Epoch 1559/2000, train Loss: 0.0023\n",
      "Epoch 1559/2000, test Loss: 0.0035\n",
      "Epoch 1560/2000, train Loss: 0.0023\n",
      "Epoch 1560/2000, test Loss: 0.0036\n",
      "Epoch 1561/2000, train Loss: 0.0023\n",
      "Epoch 1561/2000, test Loss: 0.0036\n",
      "Epoch 1562/2000, train Loss: 0.0023\n",
      "Epoch 1562/2000, test Loss: 0.0035\n",
      "Epoch 1563/2000, train Loss: 0.0023\n",
      "Epoch 1563/2000, test Loss: 0.0035\n",
      "Epoch 1564/2000, train Loss: 0.0023\n",
      "Epoch 1564/2000, test Loss: 0.0036\n",
      "Epoch 1565/2000, train Loss: 0.0023\n",
      "Epoch 1565/2000, test Loss: 0.0036\n",
      "Epoch 1566/2000, train Loss: 0.0024\n",
      "Epoch 1566/2000, test Loss: 0.0035\n",
      "Epoch 1567/2000, train Loss: 0.0023\n",
      "Epoch 1567/2000, test Loss: 0.0036\n",
      "Epoch 1568/2000, train Loss: 0.0023\n",
      "Epoch 1568/2000, test Loss: 0.0035\n",
      "Epoch 1569/2000, train Loss: 0.0023\n",
      "Epoch 1569/2000, test Loss: 0.0036\n",
      "Epoch 1570/2000, train Loss: 0.0023\n",
      "Epoch 1570/2000, test Loss: 0.0035\n",
      "Epoch 1571/2000, train Loss: 0.0024\n",
      "Epoch 1571/2000, test Loss: 0.0035\n",
      "Epoch 1572/2000, train Loss: 0.0023\n",
      "Epoch 1572/2000, test Loss: 0.0035\n",
      "Epoch 1573/2000, train Loss: 0.0023\n",
      "Epoch 1573/2000, test Loss: 0.0035\n",
      "Epoch 1574/2000, train Loss: 0.0023\n",
      "Epoch 1574/2000, test Loss: 0.0035\n",
      "Epoch 1575/2000, train Loss: 0.0024\n",
      "Epoch 1575/2000, test Loss: 0.0035\n",
      "Epoch 1576/2000, train Loss: 0.0023\n",
      "Epoch 1576/2000, test Loss: 0.0038\n",
      "Epoch 1577/2000, train Loss: 0.0023\n",
      "Epoch 1577/2000, test Loss: 0.0036\n",
      "Epoch 1578/2000, train Loss: 0.0023\n",
      "Epoch 1578/2000, test Loss: 0.0036\n",
      "Epoch 1579/2000, train Loss: 0.0023\n",
      "Epoch 1579/2000, test Loss: 0.0034\n",
      "Epoch 1580/2000, train Loss: 0.0023\n",
      "Epoch 1580/2000, test Loss: 0.0035\n",
      "Epoch 1581/2000, train Loss: 0.0023\n",
      "Epoch 1581/2000, test Loss: 0.0035\n",
      "Epoch 1582/2000, train Loss: 0.0023\n",
      "Epoch 1582/2000, test Loss: 0.0036\n",
      "Epoch 1583/2000, train Loss: 0.0023\n",
      "Epoch 1583/2000, test Loss: 0.0035\n",
      "Epoch 1584/2000, train Loss: 0.0023\n",
      "Epoch 1584/2000, test Loss: 0.0035\n",
      "Epoch 1585/2000, train Loss: 0.0023\n",
      "Epoch 1585/2000, test Loss: 0.0034\n",
      "Epoch 1586/2000, train Loss: 0.0023\n",
      "Epoch 1586/2000, test Loss: 0.0037\n",
      "Epoch 1587/2000, train Loss: 0.0023\n",
      "Epoch 1587/2000, test Loss: 0.0035\n",
      "Epoch 1588/2000, train Loss: 0.0023\n",
      "Epoch 1588/2000, test Loss: 0.0036\n",
      "Epoch 1589/2000, train Loss: 0.0023\n",
      "Epoch 1589/2000, test Loss: 0.0035\n",
      "Epoch 1590/2000, train Loss: 0.0023\n",
      "Epoch 1590/2000, test Loss: 0.0035\n",
      "Epoch 1591/2000, train Loss: 0.0023\n",
      "Epoch 1591/2000, test Loss: 0.0035\n",
      "Epoch 1592/2000, train Loss: 0.0023\n",
      "Epoch 1592/2000, test Loss: 0.0036\n",
      "Epoch 1593/2000, train Loss: 0.0023\n",
      "Epoch 1593/2000, test Loss: 0.0036\n",
      "Epoch 1594/2000, train Loss: 0.0023\n",
      "Epoch 1594/2000, test Loss: 0.0036\n",
      "Epoch 1595/2000, train Loss: 0.0023\n",
      "Epoch 1595/2000, test Loss: 0.0035\n",
      "Epoch 1596/2000, train Loss: 0.0023\n",
      "Epoch 1596/2000, test Loss: 0.0034\n",
      "Epoch 1597/2000, train Loss: 0.0023\n",
      "Epoch 1597/2000, test Loss: 0.0035\n",
      "Epoch 1598/2000, train Loss: 0.0023\n",
      "Epoch 1598/2000, test Loss: 0.0035\n",
      "Epoch 1599/2000, train Loss: 0.0023\n",
      "Epoch 1599/2000, test Loss: 0.0035\n",
      "Epoch 1600/2000, train Loss: 0.0023\n",
      "Epoch 1600/2000, test Loss: 0.0034\n",
      "Epoch 1601/2000, train Loss: 0.0023\n",
      "Epoch 1601/2000, test Loss: 0.0035\n",
      "Epoch 1602/2000, train Loss: 0.0023\n",
      "Epoch 1602/2000, test Loss: 0.0036\n",
      "Epoch 1603/2000, train Loss: 0.0023\n",
      "Epoch 1603/2000, test Loss: 0.0035\n",
      "Epoch 1604/2000, train Loss: 0.0023\n",
      "Epoch 1604/2000, test Loss: 0.0035\n",
      "Epoch 1605/2000, train Loss: 0.0023\n",
      "Epoch 1605/2000, test Loss: 0.0035\n",
      "Epoch 1606/2000, train Loss: 0.0023\n",
      "Epoch 1606/2000, test Loss: 0.0035\n",
      "Epoch 1607/2000, train Loss: 0.0023\n",
      "Epoch 1607/2000, test Loss: 0.0035\n",
      "Epoch 1608/2000, train Loss: 0.0023\n",
      "Epoch 1608/2000, test Loss: 0.0035\n",
      "Epoch 1609/2000, train Loss: 0.0023\n",
      "Epoch 1609/2000, test Loss: 0.0035\n",
      "Epoch 1610/2000, train Loss: 0.0023\n",
      "Epoch 1610/2000, test Loss: 0.0035\n",
      "Epoch 1611/2000, train Loss: 0.0023\n",
      "Epoch 1611/2000, test Loss: 0.0034\n",
      "Epoch 1612/2000, train Loss: 0.0023\n",
      "Epoch 1612/2000, test Loss: 0.0034\n",
      "Epoch 1613/2000, train Loss: 0.0023\n",
      "Epoch 1613/2000, test Loss: 0.0034\n",
      "Epoch 1614/2000, train Loss: 0.0023\n",
      "Epoch 1614/2000, test Loss: 0.0034\n",
      "Epoch 1615/2000, train Loss: 0.0023\n",
      "Epoch 1615/2000, test Loss: 0.0035\n",
      "Epoch 1616/2000, train Loss: 0.0023\n",
      "Epoch 1616/2000, test Loss: 0.0035\n",
      "Epoch 1617/2000, train Loss: 0.0023\n",
      "Epoch 1617/2000, test Loss: 0.0034\n",
      "Epoch 1618/2000, train Loss: 0.0023\n",
      "Epoch 1618/2000, test Loss: 0.0035\n",
      "Epoch 1619/2000, train Loss: 0.0023\n",
      "Epoch 1619/2000, test Loss: 0.0035\n",
      "Epoch 1620/2000, train Loss: 0.0023\n",
      "Epoch 1620/2000, test Loss: 0.0035\n",
      "Epoch 1621/2000, train Loss: 0.0023\n",
      "Epoch 1621/2000, test Loss: 0.0036\n",
      "Epoch 1622/2000, train Loss: 0.0023\n",
      "Epoch 1622/2000, test Loss: 0.0035\n",
      "Epoch 1623/2000, train Loss: 0.0023\n",
      "Epoch 1623/2000, test Loss: 0.0034\n",
      "Epoch 1624/2000, train Loss: 0.0023\n",
      "Epoch 1624/2000, test Loss: 0.0036\n",
      "Epoch 1625/2000, train Loss: 0.0022\n",
      "Epoch 1625/2000, test Loss: 0.0034\n",
      "Epoch 1626/2000, train Loss: 0.0023\n",
      "Epoch 1626/2000, test Loss: 0.0035\n",
      "Epoch 1627/2000, train Loss: 0.0023\n",
      "Epoch 1627/2000, test Loss: 0.0033\n",
      "Epoch 1628/2000, train Loss: 0.0023\n",
      "Epoch 1628/2000, test Loss: 0.0033\n",
      "Epoch 1629/2000, train Loss: 0.0023\n",
      "Epoch 1629/2000, test Loss: 0.0034\n",
      "Epoch 1630/2000, train Loss: 0.0023\n",
      "Epoch 1630/2000, test Loss: 0.0034\n",
      "Epoch 1631/2000, train Loss: 0.0023\n",
      "Epoch 1631/2000, test Loss: 0.0033\n",
      "Epoch 1632/2000, train Loss: 0.0023\n",
      "Epoch 1632/2000, test Loss: 0.0034\n",
      "Epoch 1633/2000, train Loss: 0.0023\n",
      "Epoch 1633/2000, test Loss: 0.0035\n",
      "Epoch 1634/2000, train Loss: 0.0022\n",
      "Epoch 1634/2000, test Loss: 0.0034\n",
      "Epoch 1635/2000, train Loss: 0.0022\n",
      "Epoch 1635/2000, test Loss: 0.0034\n",
      "Epoch 1636/2000, train Loss: 0.0022\n",
      "Epoch 1636/2000, test Loss: 0.0034\n",
      "Epoch 1637/2000, train Loss: 0.0022\n",
      "Epoch 1637/2000, test Loss: 0.0034\n",
      "Epoch 1638/2000, train Loss: 0.0022\n",
      "Epoch 1638/2000, test Loss: 0.0034\n",
      "Epoch 1639/2000, train Loss: 0.0022\n",
      "Epoch 1639/2000, test Loss: 0.0035\n",
      "Epoch 1640/2000, train Loss: 0.0023\n",
      "Epoch 1640/2000, test Loss: 0.0034\n",
      "Epoch 1641/2000, train Loss: 0.0022\n",
      "Epoch 1641/2000, test Loss: 0.0034\n",
      "Epoch 1642/2000, train Loss: 0.0023\n",
      "Epoch 1642/2000, test Loss: 0.0033\n",
      "Epoch 1643/2000, train Loss: 0.0023\n",
      "Epoch 1643/2000, test Loss: 0.0033\n",
      "Epoch 1644/2000, train Loss: 0.0023\n",
      "Epoch 1644/2000, test Loss: 0.0035\n",
      "Epoch 1645/2000, train Loss: 0.0022\n",
      "Epoch 1645/2000, test Loss: 0.0034\n",
      "Epoch 1646/2000, train Loss: 0.0022\n",
      "Epoch 1646/2000, test Loss: 0.0034\n",
      "Epoch 1647/2000, train Loss: 0.0023\n",
      "Epoch 1647/2000, test Loss: 0.0035\n",
      "Epoch 1648/2000, train Loss: 0.0023\n",
      "Epoch 1648/2000, test Loss: 0.0034\n",
      "Epoch 1649/2000, train Loss: 0.0023\n",
      "Epoch 1649/2000, test Loss: 0.0033\n",
      "Epoch 1650/2000, train Loss: 0.0023\n",
      "Epoch 1650/2000, test Loss: 0.0033\n",
      "Epoch 1651/2000, train Loss: 0.0022\n",
      "Epoch 1651/2000, test Loss: 0.0035\n",
      "Epoch 1652/2000, train Loss: 0.0022\n",
      "Epoch 1652/2000, test Loss: 0.0034\n",
      "Epoch 1653/2000, train Loss: 0.0022\n",
      "Epoch 1653/2000, test Loss: 0.0035\n",
      "Epoch 1654/2000, train Loss: 0.0022\n",
      "Epoch 1654/2000, test Loss: 0.0034\n",
      "Epoch 1655/2000, train Loss: 0.0022\n",
      "Epoch 1655/2000, test Loss: 0.0034\n",
      "Epoch 1656/2000, train Loss: 0.0023\n",
      "Epoch 1656/2000, test Loss: 0.0034\n",
      "Epoch 1657/2000, train Loss: 0.0022\n",
      "Epoch 1657/2000, test Loss: 0.0034\n",
      "Epoch 1658/2000, train Loss: 0.0022\n",
      "Epoch 1658/2000, test Loss: 0.0033\n",
      "Epoch 1659/2000, train Loss: 0.0022\n",
      "Epoch 1659/2000, test Loss: 0.0033\n",
      "Epoch 1660/2000, train Loss: 0.0022\n",
      "Epoch 1660/2000, test Loss: 0.0034\n",
      "Epoch 1661/2000, train Loss: 0.0022\n",
      "Epoch 1661/2000, test Loss: 0.0034\n",
      "Epoch 1662/2000, train Loss: 0.0022\n",
      "Epoch 1662/2000, test Loss: 0.0034\n",
      "Epoch 1663/2000, train Loss: 0.0022\n",
      "Epoch 1663/2000, test Loss: 0.0034\n",
      "Epoch 1664/2000, train Loss: 0.0022\n",
      "Epoch 1664/2000, test Loss: 0.0034\n",
      "Epoch 1665/2000, train Loss: 0.0022\n",
      "Epoch 1665/2000, test Loss: 0.0034\n",
      "Epoch 1666/2000, train Loss: 0.0023\n",
      "Epoch 1666/2000, test Loss: 0.0033\n",
      "Epoch 1667/2000, train Loss: 0.0022\n",
      "Epoch 1667/2000, test Loss: 0.0034\n",
      "Epoch 1668/2000, train Loss: 0.0022\n",
      "Epoch 1668/2000, test Loss: 0.0034\n",
      "Epoch 1669/2000, train Loss: 0.0022\n",
      "Epoch 1669/2000, test Loss: 0.0034\n",
      "Epoch 1670/2000, train Loss: 0.0022\n",
      "Epoch 1670/2000, test Loss: 0.0033\n",
      "Epoch 1671/2000, train Loss: 0.0022\n",
      "Epoch 1671/2000, test Loss: 0.0033\n",
      "Epoch 1672/2000, train Loss: 0.0022\n",
      "Epoch 1672/2000, test Loss: 0.0033\n",
      "Epoch 1673/2000, train Loss: 0.0023\n",
      "Epoch 1673/2000, test Loss: 0.0033\n",
      "Epoch 1674/2000, train Loss: 0.0022\n",
      "Epoch 1674/2000, test Loss: 0.0033\n",
      "Epoch 1675/2000, train Loss: 0.0022\n",
      "Epoch 1675/2000, test Loss: 0.0034\n",
      "Epoch 1676/2000, train Loss: 0.0022\n",
      "Epoch 1676/2000, test Loss: 0.0034\n",
      "Epoch 1677/2000, train Loss: 0.0023\n",
      "Epoch 1677/2000, test Loss: 0.0034\n",
      "Epoch 1678/2000, train Loss: 0.0022\n",
      "Epoch 1678/2000, test Loss: 0.0034\n",
      "Epoch 1679/2000, train Loss: 0.0022\n",
      "Epoch 1679/2000, test Loss: 0.0033\n",
      "Epoch 1680/2000, train Loss: 0.0022\n",
      "Epoch 1680/2000, test Loss: 0.0033\n",
      "Epoch 1681/2000, train Loss: 0.0022\n",
      "Epoch 1681/2000, test Loss: 0.0033\n",
      "Epoch 1682/2000, train Loss: 0.0022\n",
      "Epoch 1682/2000, test Loss: 0.0034\n",
      "Epoch 1683/2000, train Loss: 0.0022\n",
      "Epoch 1683/2000, test Loss: 0.0034\n",
      "Epoch 1684/2000, train Loss: 0.0022\n",
      "Epoch 1684/2000, test Loss: 0.0034\n",
      "Epoch 1685/2000, train Loss: 0.0022\n",
      "Epoch 1685/2000, test Loss: 0.0033\n",
      "Epoch 1686/2000, train Loss: 0.0022\n",
      "Epoch 1686/2000, test Loss: 0.0033\n",
      "Epoch 1687/2000, train Loss: 0.0022\n",
      "Epoch 1687/2000, test Loss: 0.0032\n",
      "Epoch 1688/2000, train Loss: 0.0022\n",
      "Epoch 1688/2000, test Loss: 0.0033\n",
      "Epoch 1689/2000, train Loss: 0.0022\n",
      "Epoch 1689/2000, test Loss: 0.0033\n",
      "Epoch 1690/2000, train Loss: 0.0022\n",
      "Epoch 1690/2000, test Loss: 0.0033\n",
      "Epoch 1691/2000, train Loss: 0.0022\n",
      "Epoch 1691/2000, test Loss: 0.0033\n",
      "Epoch 1692/2000, train Loss: 0.0022\n",
      "Epoch 1692/2000, test Loss: 0.0033\n",
      "Epoch 1693/2000, train Loss: 0.0022\n",
      "Epoch 1693/2000, test Loss: 0.0033\n",
      "Epoch 1694/2000, train Loss: 0.0022\n",
      "Epoch 1694/2000, test Loss: 0.0032\n",
      "Epoch 1695/2000, train Loss: 0.0022\n",
      "Epoch 1695/2000, test Loss: 0.0032\n",
      "Epoch 1696/2000, train Loss: 0.0022\n",
      "Epoch 1696/2000, test Loss: 0.0032\n",
      "Epoch 1697/2000, train Loss: 0.0022\n",
      "Epoch 1697/2000, test Loss: 0.0033\n",
      "Epoch 1698/2000, train Loss: 0.0022\n",
      "Epoch 1698/2000, test Loss: 0.0032\n",
      "Epoch 1699/2000, train Loss: 0.0022\n",
      "Epoch 1699/2000, test Loss: 0.0033\n",
      "Epoch 1700/2000, train Loss: 0.0022\n",
      "Epoch 1700/2000, test Loss: 0.0033\n",
      "Epoch 1701/2000, train Loss: 0.0022\n",
      "Epoch 1701/2000, test Loss: 0.0032\n",
      "Epoch 1702/2000, train Loss: 0.0022\n",
      "Epoch 1702/2000, test Loss: 0.0032\n",
      "Epoch 1703/2000, train Loss: 0.0022\n",
      "Epoch 1703/2000, test Loss: 0.0032\n",
      "Epoch 1704/2000, train Loss: 0.0022\n",
      "Epoch 1704/2000, test Loss: 0.0033\n",
      "Epoch 1705/2000, train Loss: 0.0022\n",
      "Epoch 1705/2000, test Loss: 0.0033\n",
      "Epoch 1706/2000, train Loss: 0.0022\n",
      "Epoch 1706/2000, test Loss: 0.0033\n",
      "Epoch 1707/2000, train Loss: 0.0022\n",
      "Epoch 1707/2000, test Loss: 0.0033\n",
      "Epoch 1708/2000, train Loss: 0.0022\n",
      "Epoch 1708/2000, test Loss: 0.0032\n",
      "Epoch 1709/2000, train Loss: 0.0022\n",
      "Epoch 1709/2000, test Loss: 0.0033\n",
      "Epoch 1710/2000, train Loss: 0.0022\n",
      "Epoch 1710/2000, test Loss: 0.0033\n",
      "Epoch 1711/2000, train Loss: 0.0022\n",
      "Epoch 1711/2000, test Loss: 0.0032\n",
      "Epoch 1712/2000, train Loss: 0.0022\n",
      "Epoch 1712/2000, test Loss: 0.0032\n",
      "Epoch 1713/2000, train Loss: 0.0022\n",
      "Epoch 1713/2000, test Loss: 0.0032\n",
      "Epoch 1714/2000, train Loss: 0.0022\n",
      "Epoch 1714/2000, test Loss: 0.0034\n",
      "Epoch 1715/2000, train Loss: 0.0022\n",
      "Epoch 1715/2000, test Loss: 0.0034\n",
      "Epoch 1716/2000, train Loss: 0.0022\n",
      "Epoch 1716/2000, test Loss: 0.0032\n",
      "Epoch 1717/2000, train Loss: 0.0022\n",
      "Epoch 1717/2000, test Loss: 0.0033\n",
      "Epoch 1718/2000, train Loss: 0.0022\n",
      "Epoch 1718/2000, test Loss: 0.0032\n",
      "Epoch 1719/2000, train Loss: 0.0022\n",
      "Epoch 1719/2000, test Loss: 0.0032\n",
      "Epoch 1720/2000, train Loss: 0.0022\n",
      "Epoch 1720/2000, test Loss: 0.0032\n",
      "Epoch 1721/2000, train Loss: 0.0022\n",
      "Epoch 1721/2000, test Loss: 0.0033\n",
      "Epoch 1722/2000, train Loss: 0.0022\n",
      "Epoch 1722/2000, test Loss: 0.0032\n",
      "Epoch 1723/2000, train Loss: 0.0022\n",
      "Epoch 1723/2000, test Loss: 0.0033\n",
      "Epoch 1724/2000, train Loss: 0.0022\n",
      "Epoch 1724/2000, test Loss: 0.0032\n",
      "Epoch 1725/2000, train Loss: 0.0022\n",
      "Epoch 1725/2000, test Loss: 0.0032\n",
      "Epoch 1726/2000, train Loss: 0.0022\n",
      "Epoch 1726/2000, test Loss: 0.0033\n",
      "Epoch 1727/2000, train Loss: 0.0022\n",
      "Epoch 1727/2000, test Loss: 0.0034\n",
      "Epoch 1728/2000, train Loss: 0.0022\n",
      "Epoch 1728/2000, test Loss: 0.0032\n",
      "Epoch 1729/2000, train Loss: 0.0022\n",
      "Epoch 1729/2000, test Loss: 0.0032\n",
      "Epoch 1730/2000, train Loss: 0.0022\n",
      "Epoch 1730/2000, test Loss: 0.0034\n",
      "Epoch 1731/2000, train Loss: 0.0022\n",
      "Epoch 1731/2000, test Loss: 0.0032\n",
      "Epoch 1732/2000, train Loss: 0.0022\n",
      "Epoch 1732/2000, test Loss: 0.0034\n",
      "Epoch 1733/2000, train Loss: 0.0021\n",
      "Epoch 1733/2000, test Loss: 0.0033\n",
      "Epoch 1734/2000, train Loss: 0.0022\n",
      "Epoch 1734/2000, test Loss: 0.0034\n",
      "Epoch 1735/2000, train Loss: 0.0022\n",
      "Epoch 1735/2000, test Loss: 0.0032\n",
      "Epoch 1736/2000, train Loss: 0.0022\n",
      "Epoch 1736/2000, test Loss: 0.0033\n",
      "Epoch 1737/2000, train Loss: 0.0022\n",
      "Epoch 1737/2000, test Loss: 0.0033\n",
      "Epoch 1738/2000, train Loss: 0.0021\n",
      "Epoch 1738/2000, test Loss: 0.0033\n",
      "Epoch 1739/2000, train Loss: 0.0022\n",
      "Epoch 1739/2000, test Loss: 0.0034\n",
      "Epoch 1740/2000, train Loss: 0.0022\n",
      "Epoch 1740/2000, test Loss: 0.0033\n",
      "Epoch 1741/2000, train Loss: 0.0022\n",
      "Epoch 1741/2000, test Loss: 0.0032\n",
      "Epoch 1742/2000, train Loss: 0.0022\n",
      "Epoch 1742/2000, test Loss: 0.0031\n",
      "Epoch 1743/2000, train Loss: 0.0022\n",
      "Epoch 1743/2000, test Loss: 0.0033\n",
      "Epoch 1744/2000, train Loss: 0.0021\n",
      "Epoch 1744/2000, test Loss: 0.0032\n",
      "Epoch 1745/2000, train Loss: 0.0021\n",
      "Epoch 1745/2000, test Loss: 0.0033\n",
      "Epoch 1746/2000, train Loss: 0.0022\n",
      "Epoch 1746/2000, test Loss: 0.0033\n",
      "Epoch 1747/2000, train Loss: 0.0022\n",
      "Epoch 1747/2000, test Loss: 0.0032\n",
      "Epoch 1748/2000, train Loss: 0.0022\n",
      "Epoch 1748/2000, test Loss: 0.0033\n",
      "Epoch 1749/2000, train Loss: 0.0022\n",
      "Epoch 1749/2000, test Loss: 0.0032\n",
      "Epoch 1750/2000, train Loss: 0.0022\n",
      "Epoch 1750/2000, test Loss: 0.0031\n",
      "Epoch 1751/2000, train Loss: 0.0022\n",
      "Epoch 1751/2000, test Loss: 0.0033\n",
      "Epoch 1752/2000, train Loss: 0.0022\n",
      "Epoch 1752/2000, test Loss: 0.0031\n",
      "Epoch 1753/2000, train Loss: 0.0021\n",
      "Epoch 1753/2000, test Loss: 0.0032\n",
      "Epoch 1754/2000, train Loss: 0.0021\n",
      "Epoch 1754/2000, test Loss: 0.0032\n",
      "Epoch 1755/2000, train Loss: 0.0021\n",
      "Epoch 1755/2000, test Loss: 0.0032\n",
      "Epoch 1756/2000, train Loss: 0.0022\n",
      "Epoch 1756/2000, test Loss: 0.0033\n",
      "Epoch 1757/2000, train Loss: 0.0021\n",
      "Epoch 1757/2000, test Loss: 0.0031\n",
      "Epoch 1758/2000, train Loss: 0.0022\n",
      "Epoch 1758/2000, test Loss: 0.0031\n",
      "Epoch 1759/2000, train Loss: 0.0021\n",
      "Epoch 1759/2000, test Loss: 0.0032\n",
      "Epoch 1760/2000, train Loss: 0.0021\n",
      "Epoch 1760/2000, test Loss: 0.0032\n",
      "Epoch 1761/2000, train Loss: 0.0022\n",
      "Epoch 1761/2000, test Loss: 0.0032\n",
      "Epoch 1762/2000, train Loss: 0.0021\n",
      "Epoch 1762/2000, test Loss: 0.0032\n",
      "Epoch 1763/2000, train Loss: 0.0022\n",
      "Epoch 1763/2000, test Loss: 0.0032\n",
      "Epoch 1764/2000, train Loss: 0.0022\n",
      "Epoch 1764/2000, test Loss: 0.0032\n",
      "Epoch 1765/2000, train Loss: 0.0021\n",
      "Epoch 1765/2000, test Loss: 0.0031\n",
      "Epoch 1766/2000, train Loss: 0.0022\n",
      "Epoch 1766/2000, test Loss: 0.0031\n",
      "Epoch 1767/2000, train Loss: 0.0022\n",
      "Epoch 1767/2000, test Loss: 0.0032\n",
      "Epoch 1768/2000, train Loss: 0.0021\n",
      "Epoch 1768/2000, test Loss: 0.0031\n",
      "Epoch 1769/2000, train Loss: 0.0021\n",
      "Epoch 1769/2000, test Loss: 0.0032\n",
      "Epoch 1770/2000, train Loss: 0.0022\n",
      "Epoch 1770/2000, test Loss: 0.0031\n",
      "Epoch 1771/2000, train Loss: 0.0022\n",
      "Epoch 1771/2000, test Loss: 0.0032\n",
      "Epoch 1772/2000, train Loss: 0.0021\n",
      "Epoch 1772/2000, test Loss: 0.0032\n",
      "Epoch 1773/2000, train Loss: 0.0021\n",
      "Epoch 1773/2000, test Loss: 0.0031\n",
      "Epoch 1774/2000, train Loss: 0.0021\n",
      "Epoch 1774/2000, test Loss: 0.0032\n",
      "Epoch 1775/2000, train Loss: 0.0021\n",
      "Epoch 1775/2000, test Loss: 0.0031\n",
      "Epoch 1776/2000, train Loss: 0.0021\n",
      "Epoch 1776/2000, test Loss: 0.0033\n",
      "Epoch 1777/2000, train Loss: 0.0021\n",
      "Epoch 1777/2000, test Loss: 0.0032\n",
      "Epoch 1778/2000, train Loss: 0.0021\n",
      "Epoch 1778/2000, test Loss: 0.0031\n",
      "Epoch 1779/2000, train Loss: 0.0021\n",
      "Epoch 1779/2000, test Loss: 0.0032\n",
      "Epoch 1780/2000, train Loss: 0.0022\n",
      "Epoch 1780/2000, test Loss: 0.0031\n",
      "Epoch 1781/2000, train Loss: 0.0021\n",
      "Epoch 1781/2000, test Loss: 0.0032\n",
      "Epoch 1782/2000, train Loss: 0.0021\n",
      "Epoch 1782/2000, test Loss: 0.0031\n",
      "Epoch 1783/2000, train Loss: 0.0021\n",
      "Epoch 1783/2000, test Loss: 0.0031\n",
      "Epoch 1784/2000, train Loss: 0.0021\n",
      "Epoch 1784/2000, test Loss: 0.0031\n",
      "Epoch 1785/2000, train Loss: 0.0022\n",
      "Epoch 1785/2000, test Loss: 0.0031\n",
      "Epoch 1786/2000, train Loss: 0.0021\n",
      "Epoch 1786/2000, test Loss: 0.0031\n",
      "Epoch 1787/2000, train Loss: 0.0021\n",
      "Epoch 1787/2000, test Loss: 0.0032\n",
      "Epoch 1788/2000, train Loss: 0.0021\n",
      "Epoch 1788/2000, test Loss: 0.0032\n",
      "Epoch 1789/2000, train Loss: 0.0021\n",
      "Epoch 1789/2000, test Loss: 0.0033\n",
      "Epoch 1790/2000, train Loss: 0.0021\n",
      "Epoch 1790/2000, test Loss: 0.0031\n",
      "Epoch 1791/2000, train Loss: 0.0021\n",
      "Epoch 1791/2000, test Loss: 0.0031\n",
      "Epoch 1792/2000, train Loss: 0.0021\n",
      "Epoch 1792/2000, test Loss: 0.0032\n",
      "Epoch 1793/2000, train Loss: 0.0021\n",
      "Epoch 1793/2000, test Loss: 0.0032\n",
      "Epoch 1794/2000, train Loss: 0.0021\n",
      "Epoch 1794/2000, test Loss: 0.0032\n",
      "Epoch 1795/2000, train Loss: 0.0022\n",
      "Epoch 1795/2000, test Loss: 0.0031\n",
      "Epoch 1796/2000, train Loss: 0.0021\n",
      "Epoch 1796/2000, test Loss: 0.0031\n",
      "Epoch 1797/2000, train Loss: 0.0021\n",
      "Epoch 1797/2000, test Loss: 0.0030\n",
      "Epoch 1798/2000, train Loss: 0.0021\n",
      "Epoch 1798/2000, test Loss: 0.0031\n",
      "Epoch 1799/2000, train Loss: 0.0021\n",
      "Epoch 1799/2000, test Loss: 0.0031\n",
      "Epoch 1800/2000, train Loss: 0.0021\n",
      "Epoch 1800/2000, test Loss: 0.0033\n",
      "Epoch 1801/2000, train Loss: 0.0021\n",
      "Epoch 1801/2000, test Loss: 0.0031\n",
      "Epoch 1802/2000, train Loss: 0.0022\n",
      "Epoch 1802/2000, test Loss: 0.0031\n",
      "Epoch 1803/2000, train Loss: 0.0021\n",
      "Epoch 1803/2000, test Loss: 0.0031\n",
      "Epoch 1804/2000, train Loss: 0.0021\n",
      "Epoch 1804/2000, test Loss: 0.0031\n",
      "Epoch 1805/2000, train Loss: 0.0021\n",
      "Epoch 1805/2000, test Loss: 0.0032\n",
      "Epoch 1806/2000, train Loss: 0.0021\n",
      "Epoch 1806/2000, test Loss: 0.0032\n",
      "Epoch 1807/2000, train Loss: 0.0021\n",
      "Epoch 1807/2000, test Loss: 0.0032\n",
      "Epoch 1808/2000, train Loss: 0.0022\n",
      "Epoch 1808/2000, test Loss: 0.0030\n",
      "Epoch 1809/2000, train Loss: 0.0021\n",
      "Epoch 1809/2000, test Loss: 0.0032\n",
      "Epoch 1810/2000, train Loss: 0.0021\n",
      "Epoch 1810/2000, test Loss: 0.0031\n",
      "Epoch 1811/2000, train Loss: 0.0021\n",
      "Epoch 1811/2000, test Loss: 0.0031\n",
      "Epoch 1812/2000, train Loss: 0.0021\n",
      "Epoch 1812/2000, test Loss: 0.0030\n",
      "Epoch 1813/2000, train Loss: 0.0021\n",
      "Epoch 1813/2000, test Loss: 0.0030\n",
      "Epoch 1814/2000, train Loss: 0.0021\n",
      "Epoch 1814/2000, test Loss: 0.0031\n",
      "Epoch 1815/2000, train Loss: 0.0021\n",
      "Epoch 1815/2000, test Loss: 0.0032\n",
      "Epoch 1816/2000, train Loss: 0.0021\n",
      "Epoch 1816/2000, test Loss: 0.0032\n",
      "Epoch 1817/2000, train Loss: 0.0021\n",
      "Epoch 1817/2000, test Loss: 0.0031\n",
      "Epoch 1818/2000, train Loss: 0.0021\n",
      "Epoch 1818/2000, test Loss: 0.0032\n",
      "Epoch 1819/2000, train Loss: 0.0022\n",
      "Epoch 1819/2000, test Loss: 0.0031\n",
      "Epoch 1820/2000, train Loss: 0.0021\n",
      "Epoch 1820/2000, test Loss: 0.0032\n",
      "Epoch 1821/2000, train Loss: 0.0021\n",
      "Epoch 1821/2000, test Loss: 0.0031\n",
      "Epoch 1822/2000, train Loss: 0.0021\n",
      "Epoch 1822/2000, test Loss: 0.0031\n",
      "Epoch 1823/2000, train Loss: 0.0021\n",
      "Epoch 1823/2000, test Loss: 0.0030\n",
      "Epoch 1824/2000, train Loss: 0.0021\n",
      "Epoch 1824/2000, test Loss: 0.0032\n",
      "Epoch 1825/2000, train Loss: 0.0021\n",
      "Epoch 1825/2000, test Loss: 0.0031\n",
      "Epoch 1826/2000, train Loss: 0.0021\n",
      "Epoch 1826/2000, test Loss: 0.0031\n",
      "Epoch 1827/2000, train Loss: 0.0021\n",
      "Epoch 1827/2000, test Loss: 0.0031\n",
      "Epoch 1828/2000, train Loss: 0.0021\n",
      "Epoch 1828/2000, test Loss: 0.0030\n",
      "Epoch 1829/2000, train Loss: 0.0021\n",
      "Epoch 1829/2000, test Loss: 0.0030\n",
      "Epoch 1830/2000, train Loss: 0.0021\n",
      "Epoch 1830/2000, test Loss: 0.0032\n",
      "Epoch 1831/2000, train Loss: 0.0021\n",
      "Epoch 1831/2000, test Loss: 0.0030\n",
      "Epoch 1832/2000, train Loss: 0.0021\n",
      "Epoch 1832/2000, test Loss: 0.0031\n",
      "Epoch 1833/2000, train Loss: 0.0021\n",
      "Epoch 1833/2000, test Loss: 0.0031\n",
      "Epoch 1834/2000, train Loss: 0.0021\n",
      "Epoch 1834/2000, test Loss: 0.0031\n",
      "Epoch 1835/2000, train Loss: 0.0021\n",
      "Epoch 1835/2000, test Loss: 0.0030\n",
      "Epoch 1836/2000, train Loss: 0.0021\n",
      "Epoch 1836/2000, test Loss: 0.0031\n",
      "Epoch 1837/2000, train Loss: 0.0021\n",
      "Epoch 1837/2000, test Loss: 0.0030\n",
      "Epoch 1838/2000, train Loss: 0.0021\n",
      "Epoch 1838/2000, test Loss: 0.0031\n",
      "Epoch 1839/2000, train Loss: 0.0021\n",
      "Epoch 1839/2000, test Loss: 0.0030\n",
      "Epoch 1840/2000, train Loss: 0.0021\n",
      "Epoch 1840/2000, test Loss: 0.0030\n",
      "Epoch 1841/2000, train Loss: 0.0021\n",
      "Epoch 1841/2000, test Loss: 0.0030\n",
      "Epoch 1842/2000, train Loss: 0.0021\n",
      "Epoch 1842/2000, test Loss: 0.0031\n",
      "Epoch 1843/2000, train Loss: 0.0021\n",
      "Epoch 1843/2000, test Loss: 0.0031\n",
      "Epoch 1844/2000, train Loss: 0.0021\n",
      "Epoch 1844/2000, test Loss: 0.0030\n",
      "Epoch 1845/2000, train Loss: 0.0021\n",
      "Epoch 1845/2000, test Loss: 0.0030\n",
      "Epoch 1846/2000, train Loss: 0.0021\n",
      "Epoch 1846/2000, test Loss: 0.0030\n",
      "Epoch 1847/2000, train Loss: 0.0021\n",
      "Epoch 1847/2000, test Loss: 0.0031\n",
      "Epoch 1848/2000, train Loss: 0.0021\n",
      "Epoch 1848/2000, test Loss: 0.0031\n",
      "Epoch 1849/2000, train Loss: 0.0021\n",
      "Epoch 1849/2000, test Loss: 0.0030\n",
      "Epoch 1850/2000, train Loss: 0.0021\n",
      "Epoch 1850/2000, test Loss: 0.0030\n",
      "Epoch 1851/2000, train Loss: 0.0021\n",
      "Epoch 1851/2000, test Loss: 0.0030\n",
      "Epoch 1852/2000, train Loss: 0.0021\n",
      "Epoch 1852/2000, test Loss: 0.0030\n",
      "Epoch 1853/2000, train Loss: 0.0021\n",
      "Epoch 1853/2000, test Loss: 0.0030\n",
      "Epoch 1854/2000, train Loss: 0.0021\n",
      "Epoch 1854/2000, test Loss: 0.0030\n",
      "Epoch 1855/2000, train Loss: 0.0021\n",
      "Epoch 1855/2000, test Loss: 0.0030\n",
      "Epoch 1856/2000, train Loss: 0.0021\n",
      "Epoch 1856/2000, test Loss: 0.0031\n",
      "Epoch 1857/2000, train Loss: 0.0021\n",
      "Epoch 1857/2000, test Loss: 0.0030\n",
      "Epoch 1858/2000, train Loss: 0.0021\n",
      "Epoch 1858/2000, test Loss: 0.0031\n",
      "Epoch 1859/2000, train Loss: 0.0021\n",
      "Epoch 1859/2000, test Loss: 0.0030\n",
      "Epoch 1860/2000, train Loss: 0.0021\n",
      "Epoch 1860/2000, test Loss: 0.0031\n",
      "Epoch 1861/2000, train Loss: 0.0020\n",
      "Epoch 1861/2000, test Loss: 0.0031\n",
      "Epoch 1862/2000, train Loss: 0.0021\n",
      "Epoch 1862/2000, test Loss: 0.0031\n",
      "Epoch 1863/2000, train Loss: 0.0022\n",
      "Epoch 1863/2000, test Loss: 0.0030\n",
      "Epoch 1864/2000, train Loss: 0.0021\n",
      "Epoch 1864/2000, test Loss: 0.0031\n",
      "Epoch 1865/2000, train Loss: 0.0021\n",
      "Epoch 1865/2000, test Loss: 0.0031\n",
      "Epoch 1866/2000, train Loss: 0.0021\n",
      "Epoch 1866/2000, test Loss: 0.0030\n",
      "Epoch 1867/2000, train Loss: 0.0021\n",
      "Epoch 1867/2000, test Loss: 0.0031\n",
      "Epoch 1868/2000, train Loss: 0.0021\n",
      "Epoch 1868/2000, test Loss: 0.0030\n",
      "Epoch 1869/2000, train Loss: 0.0021\n",
      "Epoch 1869/2000, test Loss: 0.0032\n",
      "Epoch 1870/2000, train Loss: 0.0021\n",
      "Epoch 1870/2000, test Loss: 0.0031\n",
      "Epoch 1871/2000, train Loss: 0.0021\n",
      "Epoch 1871/2000, test Loss: 0.0030\n",
      "Epoch 1872/2000, train Loss: 0.0021\n",
      "Epoch 1872/2000, test Loss: 0.0030\n",
      "Epoch 1873/2000, train Loss: 0.0021\n",
      "Epoch 1873/2000, test Loss: 0.0030\n",
      "Epoch 1874/2000, train Loss: 0.0021\n",
      "Epoch 1874/2000, test Loss: 0.0031\n",
      "Epoch 1875/2000, train Loss: 0.0021\n",
      "Epoch 1875/2000, test Loss: 0.0030\n",
      "Epoch 1876/2000, train Loss: 0.0021\n",
      "Epoch 1876/2000, test Loss: 0.0030\n",
      "Epoch 1877/2000, train Loss: 0.0021\n",
      "Epoch 1877/2000, test Loss: 0.0030\n",
      "Epoch 1878/2000, train Loss: 0.0021\n",
      "Epoch 1878/2000, test Loss: 0.0031\n",
      "Epoch 1879/2000, train Loss: 0.0021\n",
      "Epoch 1879/2000, test Loss: 0.0030\n",
      "Epoch 1880/2000, train Loss: 0.0021\n",
      "Epoch 1880/2000, test Loss: 0.0029\n",
      "Epoch 1881/2000, train Loss: 0.0021\n",
      "Epoch 1881/2000, test Loss: 0.0031\n",
      "Epoch 1882/2000, train Loss: 0.0022\n",
      "Epoch 1882/2000, test Loss: 0.0030\n",
      "Epoch 1883/2000, train Loss: 0.0021\n",
      "Epoch 1883/2000, test Loss: 0.0031\n",
      "Epoch 1884/2000, train Loss: 0.0021\n",
      "Epoch 1884/2000, test Loss: 0.0030\n",
      "Epoch 1885/2000, train Loss: 0.0021\n",
      "Epoch 1885/2000, test Loss: 0.0030\n",
      "Epoch 1886/2000, train Loss: 0.0020\n",
      "Epoch 1886/2000, test Loss: 0.0030\n",
      "Epoch 1887/2000, train Loss: 0.0020\n",
      "Epoch 1887/2000, test Loss: 0.0030\n",
      "Epoch 1888/2000, train Loss: 0.0020\n",
      "Epoch 1888/2000, test Loss: 0.0030\n",
      "Epoch 1889/2000, train Loss: 0.0020\n",
      "Epoch 1889/2000, test Loss: 0.0031\n",
      "Epoch 1890/2000, train Loss: 0.0020\n",
      "Epoch 1890/2000, test Loss: 0.0030\n",
      "Epoch 1891/2000, train Loss: 0.0020\n",
      "Epoch 1891/2000, test Loss: 0.0030\n",
      "Epoch 1892/2000, train Loss: 0.0021\n",
      "Epoch 1892/2000, test Loss: 0.0030\n",
      "Epoch 1893/2000, train Loss: 0.0021\n",
      "Epoch 1893/2000, test Loss: 0.0030\n",
      "Epoch 1894/2000, train Loss: 0.0021\n",
      "Epoch 1894/2000, test Loss: 0.0029\n",
      "Epoch 1895/2000, train Loss: 0.0020\n",
      "Epoch 1895/2000, test Loss: 0.0031\n",
      "Epoch 1896/2000, train Loss: 0.0021\n",
      "Epoch 1896/2000, test Loss: 0.0031\n",
      "Epoch 1897/2000, train Loss: 0.0021\n",
      "Epoch 1897/2000, test Loss: 0.0030\n",
      "Epoch 1898/2000, train Loss: 0.0021\n",
      "Epoch 1898/2000, test Loss: 0.0030\n",
      "Epoch 1899/2000, train Loss: 0.0020\n",
      "Epoch 1899/2000, test Loss: 0.0031\n",
      "Epoch 1900/2000, train Loss: 0.0020\n",
      "Epoch 1900/2000, test Loss: 0.0030\n",
      "Epoch 1901/2000, train Loss: 0.0021\n",
      "Epoch 1901/2000, test Loss: 0.0029\n",
      "Epoch 1902/2000, train Loss: 0.0021\n",
      "Epoch 1902/2000, test Loss: 0.0030\n",
      "Epoch 1903/2000, train Loss: 0.0021\n",
      "Epoch 1903/2000, test Loss: 0.0030\n",
      "Epoch 1904/2000, train Loss: 0.0020\n",
      "Epoch 1904/2000, test Loss: 0.0030\n",
      "Epoch 1905/2000, train Loss: 0.0021\n",
      "Epoch 1905/2000, test Loss: 0.0030\n",
      "Epoch 1906/2000, train Loss: 0.0020\n",
      "Epoch 1906/2000, test Loss: 0.0029\n",
      "Epoch 1907/2000, train Loss: 0.0020\n",
      "Epoch 1907/2000, test Loss: 0.0030\n",
      "Epoch 1908/2000, train Loss: 0.0020\n",
      "Epoch 1908/2000, test Loss: 0.0030\n",
      "Epoch 1909/2000, train Loss: 0.0020\n",
      "Epoch 1909/2000, test Loss: 0.0031\n",
      "Epoch 1910/2000, train Loss: 0.0021\n",
      "Epoch 1910/2000, test Loss: 0.0029\n",
      "Epoch 1911/2000, train Loss: 0.0021\n",
      "Epoch 1911/2000, test Loss: 0.0029\n",
      "Epoch 1912/2000, train Loss: 0.0021\n",
      "Epoch 1912/2000, test Loss: 0.0030\n",
      "Epoch 1913/2000, train Loss: 0.0021\n",
      "Epoch 1913/2000, test Loss: 0.0030\n",
      "Epoch 1914/2000, train Loss: 0.0020\n",
      "Epoch 1914/2000, test Loss: 0.0029\n",
      "Epoch 1915/2000, train Loss: 0.0020\n",
      "Epoch 1915/2000, test Loss: 0.0029\n",
      "Epoch 1916/2000, train Loss: 0.0021\n",
      "Epoch 1916/2000, test Loss: 0.0030\n",
      "Epoch 1917/2000, train Loss: 0.0021\n",
      "Epoch 1917/2000, test Loss: 0.0030\n",
      "Epoch 1918/2000, train Loss: 0.0020\n",
      "Epoch 1918/2000, test Loss: 0.0029\n",
      "Epoch 1919/2000, train Loss: 0.0020\n",
      "Epoch 1919/2000, test Loss: 0.0029\n",
      "Epoch 1920/2000, train Loss: 0.0021\n",
      "Epoch 1920/2000, test Loss: 0.0031\n",
      "Epoch 1921/2000, train Loss: 0.0020\n",
      "Epoch 1921/2000, test Loss: 0.0029\n",
      "Epoch 1922/2000, train Loss: 0.0020\n",
      "Epoch 1922/2000, test Loss: 0.0029\n",
      "Epoch 1923/2000, train Loss: 0.0021\n",
      "Epoch 1923/2000, test Loss: 0.0029\n",
      "Epoch 1924/2000, train Loss: 0.0020\n",
      "Epoch 1924/2000, test Loss: 0.0030\n",
      "Epoch 1925/2000, train Loss: 0.0020\n",
      "Epoch 1925/2000, test Loss: 0.0029\n",
      "Epoch 1926/2000, train Loss: 0.0020\n",
      "Epoch 1926/2000, test Loss: 0.0029\n",
      "Epoch 1927/2000, train Loss: 0.0020\n",
      "Epoch 1927/2000, test Loss: 0.0030\n",
      "Epoch 1928/2000, train Loss: 0.0021\n",
      "Epoch 1928/2000, test Loss: 0.0029\n",
      "Epoch 1929/2000, train Loss: 0.0020\n",
      "Epoch 1929/2000, test Loss: 0.0031\n",
      "Epoch 1930/2000, train Loss: 0.0020\n",
      "Epoch 1930/2000, test Loss: 0.0029\n",
      "Epoch 1931/2000, train Loss: 0.0020\n",
      "Epoch 1931/2000, test Loss: 0.0029\n",
      "Epoch 1932/2000, train Loss: 0.0021\n",
      "Epoch 1932/2000, test Loss: 0.0029\n",
      "Epoch 1933/2000, train Loss: 0.0020\n",
      "Epoch 1933/2000, test Loss: 0.0030\n",
      "Epoch 1934/2000, train Loss: 0.0020\n",
      "Epoch 1934/2000, test Loss: 0.0029\n",
      "Epoch 1935/2000, train Loss: 0.0020\n",
      "Epoch 1935/2000, test Loss: 0.0030\n",
      "Epoch 1936/2000, train Loss: 0.0020\n",
      "Epoch 1936/2000, test Loss: 0.0030\n",
      "Epoch 1937/2000, train Loss: 0.0020\n",
      "Epoch 1937/2000, test Loss: 0.0029\n",
      "Epoch 1938/2000, train Loss: 0.0020\n",
      "Epoch 1938/2000, test Loss: 0.0030\n",
      "Epoch 1939/2000, train Loss: 0.0021\n",
      "Epoch 1939/2000, test Loss: 0.0029\n",
      "Epoch 1940/2000, train Loss: 0.0020\n",
      "Epoch 1940/2000, test Loss: 0.0030\n",
      "Epoch 1941/2000, train Loss: 0.0020\n",
      "Epoch 1941/2000, test Loss: 0.0029\n",
      "Epoch 1942/2000, train Loss: 0.0020\n",
      "Epoch 1942/2000, test Loss: 0.0029\n",
      "Epoch 1943/2000, train Loss: 0.0020\n",
      "Epoch 1943/2000, test Loss: 0.0031\n",
      "Epoch 1944/2000, train Loss: 0.0020\n",
      "Epoch 1944/2000, test Loss: 0.0029\n",
      "Epoch 1945/2000, train Loss: 0.0020\n",
      "Epoch 1945/2000, test Loss: 0.0028\n",
      "Epoch 1946/2000, train Loss: 0.0020\n",
      "Epoch 1946/2000, test Loss: 0.0030\n",
      "Epoch 1947/2000, train Loss: 0.0020\n",
      "Epoch 1947/2000, test Loss: 0.0029\n",
      "Epoch 1948/2000, train Loss: 0.0020\n",
      "Epoch 1948/2000, test Loss: 0.0030\n",
      "Epoch 1949/2000, train Loss: 0.0021\n",
      "Epoch 1949/2000, test Loss: 0.0028\n",
      "Epoch 1950/2000, train Loss: 0.0020\n",
      "Epoch 1950/2000, test Loss: 0.0029\n",
      "Epoch 1951/2000, train Loss: 0.0020\n",
      "Epoch 1951/2000, test Loss: 0.0029\n",
      "Epoch 1952/2000, train Loss: 0.0020\n",
      "Epoch 1952/2000, test Loss: 0.0029\n",
      "Epoch 1953/2000, train Loss: 0.0020\n",
      "Epoch 1953/2000, test Loss: 0.0030\n",
      "Epoch 1954/2000, train Loss: 0.0020\n",
      "Epoch 1954/2000, test Loss: 0.0030\n",
      "Epoch 1955/2000, train Loss: 0.0020\n",
      "Epoch 1955/2000, test Loss: 0.0030\n",
      "Epoch 1956/2000, train Loss: 0.0020\n",
      "Epoch 1956/2000, test Loss: 0.0030\n",
      "Epoch 1957/2000, train Loss: 0.0020\n",
      "Epoch 1957/2000, test Loss: 0.0031\n",
      "Epoch 1958/2000, train Loss: 0.0020\n",
      "Epoch 1958/2000, test Loss: 0.0029\n",
      "Epoch 1959/2000, train Loss: 0.0020\n",
      "Epoch 1959/2000, test Loss: 0.0029\n",
      "Epoch 1960/2000, train Loss: 0.0020\n",
      "Epoch 1960/2000, test Loss: 0.0031\n",
      "Epoch 1961/2000, train Loss: 0.0020\n",
      "Epoch 1961/2000, test Loss: 0.0030\n",
      "Epoch 1962/2000, train Loss: 0.0020\n",
      "Epoch 1962/2000, test Loss: 0.0029\n",
      "Epoch 1963/2000, train Loss: 0.0020\n",
      "Epoch 1963/2000, test Loss: 0.0029\n",
      "Epoch 1964/2000, train Loss: 0.0020\n",
      "Epoch 1964/2000, test Loss: 0.0029\n",
      "Epoch 1965/2000, train Loss: 0.0020\n",
      "Epoch 1965/2000, test Loss: 0.0028\n",
      "Epoch 1966/2000, train Loss: 0.0020\n",
      "Epoch 1966/2000, test Loss: 0.0030\n",
      "Epoch 1967/2000, train Loss: 0.0020\n",
      "Epoch 1967/2000, test Loss: 0.0029\n",
      "Epoch 1968/2000, train Loss: 0.0020\n",
      "Epoch 1968/2000, test Loss: 0.0028\n",
      "Epoch 1969/2000, train Loss: 0.0020\n",
      "Epoch 1969/2000, test Loss: 0.0030\n",
      "Epoch 1970/2000, train Loss: 0.0020\n",
      "Epoch 1970/2000, test Loss: 0.0028\n",
      "Epoch 1971/2000, train Loss: 0.0020\n",
      "Epoch 1971/2000, test Loss: 0.0029\n",
      "Epoch 1972/2000, train Loss: 0.0020\n",
      "Epoch 1972/2000, test Loss: 0.0029\n",
      "Epoch 1973/2000, train Loss: 0.0020\n",
      "Epoch 1973/2000, test Loss: 0.0029\n",
      "Epoch 1974/2000, train Loss: 0.0021\n",
      "Epoch 1974/2000, test Loss: 0.0028\n",
      "Epoch 1975/2000, train Loss: 0.0020\n",
      "Epoch 1975/2000, test Loss: 0.0029\n",
      "Epoch 1976/2000, train Loss: 0.0020\n",
      "Epoch 1976/2000, test Loss: 0.0029\n",
      "Epoch 1977/2000, train Loss: 0.0020\n",
      "Epoch 1977/2000, test Loss: 0.0029\n",
      "Epoch 1978/2000, train Loss: 0.0020\n",
      "Epoch 1978/2000, test Loss: 0.0028\n",
      "Epoch 1979/2000, train Loss: 0.0020\n",
      "Epoch 1979/2000, test Loss: 0.0029\n",
      "Epoch 1980/2000, train Loss: 0.0020\n",
      "Epoch 1980/2000, test Loss: 0.0029\n",
      "Epoch 1981/2000, train Loss: 0.0020\n",
      "Epoch 1981/2000, test Loss: 0.0029\n",
      "Epoch 1982/2000, train Loss: 0.0020\n",
      "Epoch 1982/2000, test Loss: 0.0029\n",
      "Epoch 1983/2000, train Loss: 0.0020\n",
      "Epoch 1983/2000, test Loss: 0.0028\n",
      "Epoch 1984/2000, train Loss: 0.0020\n",
      "Epoch 1984/2000, test Loss: 0.0030\n",
      "Epoch 1985/2000, train Loss: 0.0020\n",
      "Epoch 1985/2000, test Loss: 0.0029\n",
      "Epoch 1986/2000, train Loss: 0.0020\n",
      "Epoch 1986/2000, test Loss: 0.0028\n",
      "Epoch 1987/2000, train Loss: 0.0020\n",
      "Epoch 1987/2000, test Loss: 0.0029\n",
      "Epoch 1988/2000, train Loss: 0.0020\n",
      "Epoch 1988/2000, test Loss: 0.0029\n",
      "Epoch 1989/2000, train Loss: 0.0020\n",
      "Epoch 1989/2000, test Loss: 0.0029\n",
      "Epoch 1990/2000, train Loss: 0.0020\n",
      "Epoch 1990/2000, test Loss: 0.0028\n",
      "Epoch 1991/2000, train Loss: 0.0020\n",
      "Epoch 1991/2000, test Loss: 0.0030\n",
      "Epoch 1992/2000, train Loss: 0.0020\n",
      "Epoch 1992/2000, test Loss: 0.0030\n",
      "Epoch 1993/2000, train Loss: 0.0020\n",
      "Epoch 1993/2000, test Loss: 0.0028\n",
      "Epoch 1994/2000, train Loss: 0.0020\n",
      "Epoch 1994/2000, test Loss: 0.0029\n",
      "Epoch 1995/2000, train Loss: 0.0020\n",
      "Epoch 1995/2000, test Loss: 0.0029\n",
      "Epoch 1996/2000, train Loss: 0.0020\n",
      "Epoch 1996/2000, test Loss: 0.0029\n",
      "Epoch 1997/2000, train Loss: 0.0021\n",
      "Epoch 1997/2000, test Loss: 0.0028\n",
      "Epoch 1998/2000, train Loss: 0.0020\n",
      "Epoch 1998/2000, test Loss: 0.0029\n",
      "Epoch 1999/2000, train Loss: 0.0020\n",
      "Epoch 1999/2000, test Loss: 0.0028\n",
      "Epoch 2000/2000, train Loss: 0.0020\n",
      "Epoch 2000/2000, test Loss: 0.0030\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "learning_rate = 1e-3\n",
    "\n",
    "history = []\n",
    "model = DNN(input_dim=9)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    total_samples = 0\n",
    "    for batch_data in train_loader:\n",
    "        x, y = batch_data \n",
    "        y_pred = model(x)\n",
    "        loss = dnn_loss(y_pred, y.view(-1, 1))\n",
    "        optimizer.zero_grad() \n",
    "        loss.backward() # Computes the gradient\n",
    "        train_loss += loss.detach().item() * x.size(0)\n",
    "        total_samples += x.size(0)  # Count samples in the batch\n",
    "        optimizer.step() # update the parameters \n",
    "    avg_loss = train_loss / total_samples  # Normalize by number of batches\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    total_val_samples = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_data in test_loader:\n",
    "            x_val,y_val = batch_data\n",
    "            yval_pred = model(x_val)\n",
    "            loss = dnn_loss(yval_pred, y_val.view(-1, 1))\n",
    "            val_loss += loss.detach().item() * x_val.size(0)\n",
    "            total_val_samples += x_val.size(0)  # Count samples in the batch\n",
    "    avg_val_loss = val_loss / total_val_samples  # Normalize by number of batches\n",
    "    history.append([epoch,avg_loss, avg_val_loss])\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, train Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, test Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "history = np.array(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd \n",
    "# train = pd.DataFrame(avg_loss)\n",
    "# test = pd.DataFrame(avg_val_loss)\n",
    "\n",
    "# train.to_csv(\"./bias_0_train.csv\", header=False, index=False)\n",
    "# test.to_csv(\"./bias_0_test.csv\", header=False, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(history[:,1], label=\"Training Loss\")\n",
    "plt.plot(history[:,2], label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training vs Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=train_size, shuffle=True)\n",
    "\n",
    "model.eval()\n",
    "# Loop over the test set\n",
    "with torch.no_grad():  # Disable gradient calculation for testing\n",
    "    for batch_data in train_loader:\n",
    "        x, y = batch_data\n",
    "        y_pred = model(x)\n",
    "        \n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        x_val,y_val = batch_data\n",
    "        yval_pred = model(x_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "line_x = [min(y),max(y)] # yml is the original dataset, the total\n",
    "plt.scatter(y.detach().numpy(), y_pred.detach().numpy(), label= 'train', alpha = 1, s=5)\n",
    "plt.scatter(y_val.detach().numpy(), yval_pred.detach().numpy(), label= 'test', alpha = 1, s=5)\n",
    "plt.plot(line_x,line_x, c='grey', lw = 1.1)#, zorder=0)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9979012277626291 0.9976956713245355\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score    \n",
    "test_r2 = r2_score(y_val, yval_pred)\n",
    "train_r2 = r2_score(y, y_pred)\n",
    "print(train_r2, test_r2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
